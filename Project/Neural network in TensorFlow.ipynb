{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    max_data = np.max(data, axis=0)\n",
    "    min_data = np.min(data, axis=0)\n",
    "    stats = ['away_wins', 'away_losses', 'away_ot',\n",
    "             'away_pts', 'away_ptPctg', 'away_goalsPerGame',\n",
    "             'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "             'away_powerPlayPercentage', 'away_powerPlayGoals',\n",
    "             'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities',\n",
    "             'away_penaltyKillPercentage', 'away_shotsPerGame', 'away_shotsAllowed',\n",
    "             'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "             'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "             'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "             'away_faceOffWinPercentage', 'away_shootingPctg', 'away_savePctg',\n",
    "             'home_wins', 'home_losses', 'home_ot', 'home_pts', 'home_ptPctg',\n",
    "             'home_goalsPerGame', 'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "             'home_powerPlayPercentage', 'home_powerPlayGoals',\n",
    "             'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities',\n",
    "             'home_penaltyKillPercentage', 'home_shotsPerGame', 'home_shotsAllowed',\n",
    "             'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "             'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "             'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "             'home_faceOffWinPercentage', 'home_shootingPctg', 'home_savePctg']\n",
    "    for stat in stats:\n",
    "        data[stat] = (data[stat] - min_data[stat])/(max_data[stat] - min_data[stat])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "#each one of these data sets needs to be normalized \n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)\n",
    "\n",
    "X,y = prepare(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "n_x = X.shape[1]\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2) )\n",
    "\n",
    "# Optimizer.\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(tf_J)\n",
    "\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 273.934\n",
      "50 13.0115\n",
      "100 10.008\n",
      "150 7.84663\n",
      "200 5.51714\n",
      "250 3.27687\n",
      "300 1.85464\n",
      "350 8.97654\n",
      "400 13.5138\n",
      "450 1.54992\n",
      "500 1.83088\n",
      "550 1.82742\n",
      "600 1.78191\n",
      "650 1.7487\n",
      "700 1.71961\n",
      "750 1.69404\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(800):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.56554412928\n",
      "Accuracy on the test set is  0.550180831826\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A2.eval()\n",
    "A_test = tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
