{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying handwritten digits using Tensorflow (binary classification).\n",
    "--\n",
    "In this notebook we will do binary classification of MNIST handwritten digits. More precisely, we will do classification of digit 9 vs the rest. \n",
    "\n",
    "You should have in the same directory as this notebook file the Python pickle file mnist_bin_classification_9vsRest.pickle. You can download this file from: \n",
    "\n",
    "http://webhome.cs.uvic.ca/~thomo/mnist_bin_classification_9vsRest.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load the dataset of handwritten digits from a Python pickle file. \n",
    "# For information on pickle files, see: https://pythontips.com/2013/08/02/what-is-pickle-in-python\n",
    "# The pickle file contains 55,000 training images and their labels as well as\n",
    "# 10,000 test images and their labels.\n",
    "\n",
    "fileObject = open(\"mnist_bin_classification_9vsRest.pickle\",'rb')  \n",
    "X,Y,X_test,Y_test = pickle.load(fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n",
      "[False]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKMIPBEX4gaAIPxDU/wOQv/IG3GepCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f10cdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's view some images\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X[0,:].reshape((28,28)), cmap='Greys')\n",
    "\n",
    "print(Y[0])\n",
    "print(Y_test[0])\n",
    "\n",
    "# Feel free to display other images by changing the index 0 above to some other index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (55000, 784) (55000, 1)\n",
      "Test dataset shape (10000, 784) (10000, 1)\n",
      "Y = [[False]\n",
      " [False]\n",
      " [False]\n",
      " ..., \n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy\n",
    "--\n",
    "Compute the accuracy of binary classification on the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the logistic regression binary classifier on Tensorflow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X)\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test)\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, 1)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 0.693259\n",
      "1 0.362494\n",
      "2 0.338161\n",
      "3 0.326896\n",
      "4 0.31924\n",
      "5 0.313017\n",
      "6 0.307529\n",
      "7 0.302505\n",
      "8 0.29783\n",
      "9 0.293443\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(10):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-1c9176b9e4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# just to get that one numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Note that it recomputes all its computation graph dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mA_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_A_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \"\"\"\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4069\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   4072\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "--\n",
    "Here we want to do batch stochastic gradient descent and approximate the gradient using batches of training examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "tf_X_test = tf.constant(X_test)\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, 1)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step  (0, 0.69314742)\n",
      "Minibatch accuracy:  0.92\n",
      "Test accuracy:  0.8991\n",
      "Minibatch loss at step  (500, 0.11886039)\n",
      "Minibatch accuracy:  0.96\n",
      "Test accuracy:  0.9529\n",
      "Minibatch loss at step  (1000, 0.095326357)\n",
      "Minibatch accuracy:  0.97\n",
      "Test accuracy:  0.9588\n",
      "Minibatch loss at step  (1500, 0.11317737)\n",
      "Minibatch accuracy:  0.95\n",
      "Test accuracy:  0.9598\n",
      "Minibatch loss at step  (2000, 0.049157977)\n",
      "Minibatch accuracy:  0.99\n",
      "Test accuracy:  0.9623\n",
      "Minibatch loss at step  (2500, 0.30240571)\n",
      "Minibatch accuracy:  0.9\n",
      "Test accuracy:  0.9632\n",
      "Minibatch loss at step  (3000, 0.061813116)\n",
      "Minibatch accuracy:  0.99\n",
      "Test accuracy:  0.9636\n",
      "Minibatch loss at step  (3500, 0.16863389)\n",
      "Minibatch accuracy:  0.92\n",
      "Test accuracy:  0.9639\n",
      "Minibatch loss at step  (4000, 0.18505821)\n",
      "Minibatch accuracy:  0.94\n",
      "Test accuracy:  0.962\n",
      "Minibatch loss at step  (4500, 0.16158779)\n",
      "Minibatch accuracy:  0.92\n",
      "Test accuracy:  0.9641\n",
      "Minibatch loss at step  (5000, 0.062602691)\n",
      "Minibatch accuracy:  0.99\n",
      "Test accuracy:  0.9641\n"
     ]
    }
   ],
   "source": [
    "num_steps = 5001\n",
    "batch_size = 100\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
