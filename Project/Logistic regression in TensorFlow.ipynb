{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    max_data = np.max(data, axis=0)\n",
    "    min_data = np.min(data, axis=0)\n",
    "    stats = ['away_wins', 'away_losses', 'away_ot',\n",
    "             'away_pts', 'away_ptPctg', 'away_goalsPerGame',\n",
    "             'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "             'away_powerPlayPercentage', 'away_powerPlayGoals',\n",
    "             'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities',\n",
    "             'away_penaltyKillPercentage', 'away_shotsPerGame', 'away_shotsAllowed',\n",
    "             'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "             'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "             'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "             'away_faceOffWinPercentage', 'away_shootingPctg', 'away_savePctg',\n",
    "             'home_wins', 'home_losses', 'home_ot', 'home_pts', 'home_ptPctg',\n",
    "             'home_goalsPerGame', 'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "             'home_powerPlayPercentage', 'home_powerPlayGoals',\n",
    "             'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities',\n",
    "             'home_penaltyKillPercentage', 'home_shotsPerGame', 'home_shotsAllowed',\n",
    "             'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "             'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "             'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "             'home_faceOffWinPercentage', 'home_shootingPctg', 'home_savePctg']\n",
    "    for stat in stats:\n",
    "        data[stat] = (data[stat] - min_data[stat])/(max_data[stat] - min_data[stat])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "#each one of these data sets needs to be normalized \n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)\n",
    "\n",
    "X,y = prepare(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (17698, 55) (17698, 1)\n",
      "Test dataset shape (4424, 55) (4424, 1)\n",
      "Y = [[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable(tf.zeros((n_x, 1)))\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 0.693131\n",
      "1 396.354\n",
      "2 6038.19\n",
      "3 1191.65\n",
      "4 5135.42\n",
      "5 1984.2\n",
      "6 4232.74\n",
      "7 2776.69\n",
      "8 3330.06\n",
      "9 3569.17\n",
      "10 2427.39\n",
      "11 4361.64\n",
      "12 1524.72\n",
      "13 5154.05\n",
      "14 622.19\n",
      "15 5944.59\n",
      "16 244.399\n",
      "17 6208.73\n",
      "18 1041.77\n",
      "19 5305.93\n",
      "20 1834.35\n",
      "21 4403.24\n",
      "22 2626.84\n",
      "23 3500.57\n",
      "24 3419.31\n",
      "25 2597.88\n",
      "26 4211.79\n",
      "27 1695.22\n",
      "28 5004.21\n",
      "29 792.653\n",
      "30 5795.32\n",
      "31 95.2466\n",
      "32 6375.49\n",
      "33 895.198\n",
      "34 5472.64\n",
      "35 1687.82\n",
      "36 4569.96\n",
      "37 2480.31\n",
      "38 3667.28\n",
      "39 3272.78\n",
      "40 2764.61\n",
      "41 4065.26\n",
      "42 1861.94\n",
      "43 4857.68\n",
      "44 959.371\n",
      "45 5648.97\n",
      "46 59.0815\n",
      "47 6422.54\n",
      "48 722.178\n",
      "49 5668.88\n",
      "50 1515.39\n",
      "51 4766.18\n",
      "52 2307.9\n",
      "53 3863.49\n",
      "54 3100.38\n",
      "55 2960.82\n",
      "56 3892.86\n",
      "57 2058.14\n",
      "58 4685.32\n",
      "59 1155.5\n",
      "60 5477.46\n",
      "61 253.577\n",
      "62 6261.34\n",
      "63 561.042\n",
      "64 5850.75\n",
      "65 1355.58\n",
      "66 4947.98\n",
      "67 2148.14\n",
      "68 4045.29\n",
      "69 2940.62\n",
      "70 3142.61\n",
      "71 3733.1\n",
      "72 2239.94\n",
      "73 4525.57\n",
      "74 1337.28\n",
      "75 5317.86\n",
      "76 435.008\n",
      "77 6105.74\n",
      "78 405.545\n",
      "79 6025.22\n",
      "80 1202.25\n",
      "81 5122.38\n",
      "82 1994.87\n",
      "83 4219.68\n",
      "84 2787.36\n",
      "85 3317.01\n",
      "86 3579.84\n",
      "87 2414.33\n",
      "88 4372.31\n",
      "89 1511.67\n",
      "90 5164.64\n",
      "91 609.291\n",
      "92 5953.98\n",
      "93 253.942\n",
      "94 6194.81\n",
      "95 1053.21\n",
      "96 5291.85\n",
      "97 1845.93\n",
      "98 4389.16\n",
      "99 2638.42\n",
      "100 3486.48\n",
      "101 3430.9\n",
      "102 2583.8\n",
      "103 4223.37\n",
      "104 1681.15\n",
      "105 5015.71\n",
      "106 778.747\n",
      "107 5805.59\n",
      "108 105.714\n",
      "109 6360.87\n",
      "110 907.274\n",
      "111 5457.76\n",
      "112 1700.12\n",
      "113 4555.06\n",
      "114 2492.62\n",
      "115 3652.38\n",
      "116 3285.1\n",
      "117 2749.71\n",
      "118 4077.56\n",
      "119 1847.05\n",
      "120 4869.87\n",
      "121 944.676\n",
      "122 5659.92\n",
      "123 45.9586\n",
      "124 6434.62\n",
      "125 734.32\n",
      "126 5653.21\n",
      "127 1528.39\n",
      "128 4750.43\n",
      "129 2320.96\n",
      "130 3847.74\n",
      "131 3113.44\n",
      "132 2945.06\n",
      "133 3905.92\n",
      "134 2042.4\n",
      "135 4698.33\n",
      "136 1139.84\n",
      "137 5489.85\n",
      "138 238.863\n",
      "139 6271.84\n",
      "140 571.668\n",
      "141 5835.85\n",
      "142 1367.91\n",
      "143 4932.9\n",
      "144 2160.62\n",
      "145 4030.2\n",
      "146 2953.12\n",
      "147 3127.52\n",
      "148 3745.59\n",
      "149 2224.85\n",
      "150 4538.03\n",
      "151 1322.25\n",
      "152 5329.91\n",
      "153 420.627\n",
      "154 6115.53\n",
      "155 415.544\n",
      "156 6010.69\n",
      "157 1214.28\n",
      "158 5107.47\n",
      "159 2007.21\n",
      "160 4204.76\n",
      "161 2799.72\n",
      "162 3302.07\n",
      "163 3592.2\n",
      "164 2399.41\n",
      "165 4384.64\n",
      "166 1496.8\n",
      "167 5176.64\n",
      "168 594.961\n",
      "169 5963.88\n",
      "170 264.106\n",
      "171 6180.5\n",
      "172 1065.06\n",
      "173 5276.97\n",
      "174 1858.26\n",
      "175 4374.23\n",
      "176 2650.79\n",
      "177 3471.54\n",
      "178 3443.27\n",
      "179 2568.87\n",
      "180 4235.71\n",
      "181 1666.27\n",
      "182 5027.71\n",
      "183 764.386\n",
      "184 5815.72\n",
      "185 116.136\n",
      "186 6346.96\n",
      "187 918.802\n",
      "188 5443.04\n",
      "189 1712.31\n",
      "190 4540.27\n",
      "191 2504.87\n",
      "192 3637.58\n",
      "193 3297.35\n",
      "194 2734.91\n",
      "195 4089.78\n",
      "196 1832.33\n",
      "197 4881.75\n",
      "198 930.468\n",
      "199 5670.09\n",
      "200 33.85\n",
      "201 6375.49\n",
      "202 675.344\n",
      "203 5717.38\n",
      "204 1471.35\n",
      "205 4814.27\n",
      "206 2264.19\n",
      "207 3911.55\n",
      "208 3056.7\n",
      "209 3008.88\n",
      "210 3849.16\n",
      "211 2106.24\n",
      "212 4641.43\n",
      "213 1203.9\n",
      "214 5432.09\n",
      "215 304.023\n",
      "216 6214.12\n",
      "217 514.203\n",
      "218 5897.98\n",
      "219 1312.68\n",
      "220 4994.45\n",
      "221 2105.87\n",
      "222 4091.68\n",
      "223 2898.42\n",
      "224 3188.99\n",
      "225 3690.89\n",
      "226 2286.35\n",
      "227 4483.21\n",
      "228 1383.93\n",
      "229 5274.39\n",
      "230 483.271\n",
      "231 6059.0\n",
      "232 359.325\n",
      "233 6071.78\n",
      "234 1159.99\n",
      "235 5167.72\n",
      "236 1953.61\n",
      "237 4264.88\n",
      "238 2746.23\n",
      "239 3362.19\n",
      "240 3538.7\n",
      "241 2459.54\n",
      "242 4331.04\n",
      "243 1557.09\n",
      "244 5122.4\n",
      "245 656.128\n",
      "246 5908.36\n",
      "247 208.92\n",
      "248 6241.04\n",
      "249 1011.28\n",
      "250 5336.41\n",
      "251 1805.37\n",
      "252 4433.49\n",
      "253 2598.05\n",
      "254 3530.79\n",
      "255 3390.53\n",
      "256 2628.15\n",
      "257 4182.86\n",
      "258 1725.72\n",
      "259 4974.28\n",
      "260 824.65\n",
      "261 5760.93\n",
      "262 61.7063\n",
      "263 6407.19\n",
      "264 865.314\n",
      "265 5501.99\n",
      "266 1659.87\n",
      "267 4598.98\n",
      "268 2452.62\n",
      "269 3696.27\n",
      "270 3245.11\n",
      "271 2793.63\n",
      "272 4037.41\n",
      "273 1891.23\n",
      "274 4828.81\n",
      "275 990.169\n",
      "276 5615.83\n",
      "277 95.0743\n",
      "278 6392.24\n",
      "279 692.242\n",
      "280 5696.39\n",
      "281 1489.12\n",
      "282 4792.84\n",
      "283 2282.31\n",
      "284 3890.04\n",
      "285 3074.88\n",
      "286 2987.37\n",
      "287 3867.27\n",
      "288 2084.85\n",
      "289 4659.13\n",
      "290 1183.1\n",
      "291 5448.27\n",
      "292 285.187\n",
      "293 6228.86\n",
      "294 529.147\n",
      "295 5879.38\n",
      "296 1328.38\n",
      "297 4975.14\n",
      "298 2122.14\n",
      "299 4072.21\n",
      "300 2914.82\n",
      "301 3169.51\n",
      "302 3707.26\n",
      "303 2266.95\n",
      "304 4499.28\n",
      "305 1364.96\n",
      "306 5289.3\n",
      "307 465.822\n",
      "308 6072.4\n",
      "309 372.97\n",
      "310 6054.93\n",
      "311 1174.18\n",
      "312 5149.94\n",
      "313 1968.56\n",
      "314 4246.86\n",
      "315 2761.36\n",
      "316 3344.14\n",
      "317 3553.82\n",
      "318 2441.56\n",
      "319 4345.9\n",
      "320 1539.49\n",
      "321 5136.31\n",
      "322 639.785\n",
      "323 5920.83\n",
      "324 221.674\n",
      "325 6225.48\n",
      "326 1024.38\n",
      "327 5319.76\n",
      "328 1819.35\n",
      "329 4416.51\n",
      "330 2612.28\n",
      "331 3513.77\n",
      "332 3404.76\n",
      "333 2611.18\n",
      "334 4196.85\n",
      "335 1709.09\n",
      "336 4987.44\n",
      "337 809.14\n",
      "338 5772.78\n",
      "339 73.8418\n",
      "340 6392.54\n",
      "341 877.636\n",
      "342 5486.15\n",
      "343 1673.14\n",
      "344 4582.75\n",
      "345 2466.2\n",
      "346 3679.97\n",
      "347 3258.7\n",
      "348 2777.4\n",
      "349 4050.78\n",
      "350 1875.33\n",
      "351 4841.42\n",
      "352 975.281\n",
      "353 5627.23\n",
      "354 81.6313\n",
      "355 6403.86\n",
      "356 704.071\n",
      "357 5681.09\n",
      "358 1501.94\n",
      "359 4776.89\n",
      "360 2295.66\n",
      "361 3873.91\n",
      "362 3088.35\n",
      "363 2971.26\n",
      "364 3880.62\n",
      "365 2068.94\n",
      "366 4671.96\n",
      "367 1167.92\n",
      "368 5459.95\n",
      "369 271.428\n",
      "370 6240.1\n",
      "371 540.638\n",
      "372 5864.72\n",
      "373 1340.68\n",
      "374 4959.6\n",
      "375 2135.15\n",
      "376 4056.4\n",
      "377 2928.03\n",
      "378 3153.67\n",
      "379 3720.4\n",
      "380 2251.26\n",
      "381 4512.02\n",
      "382 1349.85\n",
      "383 5301.03\n",
      "384 451.987\n",
      "385 6083.38\n",
      "386 384.238\n",
      "387 6040.84\n",
      "388 1186.03\n",
      "389 5134.82\n",
      "390 1981.21\n",
      "391 4231.39\n",
      "392 2774.28\n",
      "393 3328.59\n",
      "394 3566.72\n",
      "395 2426.14\n",
      "396 4358.47\n",
      "397 1524.56\n",
      "398 5147.99\n",
      "399 626.004\n",
      "400 5931.66\n",
      "401 232.793\n",
      "402 6211.81\n",
      "403 1035.88\n",
      "404 5305.0\n",
      "405 1831.7\n",
      "406 4401.32\n",
      "407 2624.96\n",
      "408 3498.47\n",
      "409 3417.45\n",
      "410 2596.01\n",
      "411 4209.24\n",
      "412 1694.37\n",
      "413 4999.01\n",
      "414 795.462\n",
      "415 5783.48\n",
      "416 84.8515\n",
      "417 6379.2\n",
      "418 888.868\n",
      "419 5471.7\n",
      "420 1685.23\n",
      "421 4567.81\n",
      "422 2478.67\n",
      "423 3664.9\n",
      "424 3271.2\n",
      "425 2762.44\n",
      "426 4063.0\n",
      "427 1860.79\n",
      "428 4852.88\n",
      "429 961.725\n",
      "430 5637.83\n",
      "431 69.1445\n",
      "432 6333.86\n",
      "433 634.384\n",
      "434 5758.26\n",
      "435 1433.67\n",
      "436 4853.05\n",
      "437 2228.18\n",
      "438 3949.76\n",
      "439 3021.09\n",
      "440 3047.08\n",
      "441 3813.27\n",
      "442 2144.98\n",
      "443 4604.21\n",
      "444 1244.5\n",
      "445 5391.74\n",
      "446 348.53\n",
      "447 6172.38\n",
      "448 473.265\n",
      "449 5939.63\n",
      "450 1274.43\n",
      "451 5033.39\n",
      "452 2069.77\n",
      "453 4129.78\n",
      "454 2862.95\n",
      "455 3226.97\n",
      "456 3655.29\n",
      "457 2324.74\n",
      "458 4446.55\n",
      "459 1423.83\n",
      "460 5234.98\n",
      "461 526.69\n",
      "462 6017.31\n",
      "463 318.525\n",
      "464 6114.23\n",
      "465 1121.13\n",
      "466 5207.08\n",
      "467 1917.2\n",
      "468 4303.14\n",
      "469 2710.64\n",
      "470 3400.24\n",
      "471 3503.07\n",
      "472 2497.94\n",
      "473 4294.49\n",
      "474 1596.84\n",
      "475 5083.38\n",
      "476 699.071\n",
      "477 5866.79\n",
      "478 168.267\n",
      "479 6284.17\n",
      "480 971.914\n",
      "481 5376.23\n",
      "482 1768.59\n",
      "483 4472.0\n",
      "484 2562.27\n",
      "485 3569.01\n",
      "486 3354.78\n",
      "487 2666.68\n",
      "488 4146.27\n",
      "489 1765.49\n",
      "490 4935.42\n",
      "491 867.384\n",
      "492 5719.49\n",
      "493 31.6976\n",
      "494 2704.7\n",
      "495 4113.08\n",
      "496 1803.3\n",
      "497 4902.61\n",
      "498 904.692\n",
      "499 5687.4\n",
      "500 28.8184\n",
      "501 1721.02\n",
      "502 4525.67\n",
      "503 2515.27\n",
      "504 3622.42\n",
      "505 3308.05\n",
      "506 2719.9\n",
      "507 4099.87\n",
      "508 1818.31\n",
      "509 4889.76\n",
      "510 919.228\n",
      "511 5675.3\n",
      "512 33.4367\n",
      "513 3657.43\n",
      "514 2322.04\n",
      "515 4448.92\n",
      "516 1420.89\n",
      "517 5237.94\n",
      "518 522.969\n",
      "519 6021.73\n",
      "520 323.449\n",
      "521 6106.88\n",
      "522 1127.65\n",
      "523 5198.3\n",
      "524 1924.83\n",
      "525 4293.72\n",
      "526 2718.79\n",
      "527 3390.59\n",
      "528 3511.41\n",
      "529 2488.21\n",
      "530 4302.93\n",
      "531 1587.03\n",
      "532 5092.08\n",
      "533 688.922\n",
      "534 5876.3\n",
      "535 178.184\n",
      "536 6271.71\n",
      "537 982.895\n",
      "538 5362.63\n",
      "539 1780.47\n",
      "540 4457.81\n",
      "541 2574.6\n",
      "542 3554.63\n",
      "543 3367.26\n",
      "544 2652.24\n",
      "545 4158.79\n",
      "546 1751.07\n",
      "547 4948.0\n",
      "548 852.885\n",
      "549 5732.47\n",
      "550 38.1874\n",
      "551 4609.14\n",
      "552 2441.75\n",
      "553 3705.75\n",
      "554 3234.6\n",
      "555 2803.24\n",
      "556 4026.33\n",
      "557 1901.83\n",
      "558 4816.0\n",
      "559 1003.05\n",
      "560 5601.4\n",
      "561 109.778\n",
      "562 6380.06\n",
      "563 681.024\n",
      "564 5703.23\n",
      "565 1481.52\n",
      "566 4796.68\n",
      "567 2277.06\n",
      "568 3892.82\n",
      "569 3070.34\n",
      "570 2990.05\n",
      "571 3862.43\n",
      "572 2088.27\n",
      "573 4652.79\n",
      "574 1188.59\n",
      "575 5439.61\n",
      "576 293.5\n",
      "577 6220.34\n",
      "578 521.675\n",
      "579 5883.01\n",
      "580 1323.72\n",
      "581 4975.42\n",
      "582 2120.08\n",
      "583 4071.11\n",
      "584 2913.74\n",
      "585 3168.16\n",
      "586 3706.05\n",
      "587 2266.17\n",
      "588 4496.79\n",
      "589 1366.03\n",
      "590 5284.42\n",
      "591 469.901\n",
      "592 6066.47\n",
      "593 368.122\n",
      "594 6056.63\n",
      "595 1171.31\n",
      "596 5148.18\n",
      "597 1968.35\n",
      "598 4243.47\n",
      "599 2762.33\n",
      "600 3340.37\n",
      "601 3554.79\n",
      "602 2438.27\n",
      "603 4345.74\n",
      "604 1537.88\n",
      "605 5133.82\n",
      "606 641.159\n",
      "607 5916.74\n",
      "608 218.65\n",
      "609 6225.94\n",
      "610 1022.65\n",
      "611 5316.78\n",
      "612 1820.24\n",
      "613 4411.74\n",
      "614 2614.49\n",
      "615 3508.53\n",
      "616 3407.05\n",
      "617 2606.36\n",
      "618 4198.12\n",
      "619 1705.85\n",
      "620 4986.48\n",
      "621 808.776\n",
      "622 5769.95\n",
      "623 72.2355\n",
      "624 6234.89\n",
      "625 1014.64\n",
      "626 5326.07\n",
      "627 1811.95\n",
      "628 4421.17\n",
      "629 2606.06\n",
      "630 3518.04\n",
      "631 3398.5\n",
      "632 2616.02\n",
      "633 4189.33\n",
      "634 1715.82\n",
      "635 4977.28\n",
      "636 819.268\n",
      "637 5760.21\n",
      "638 62.8564\n",
      "639 6049.7\n",
      "640 1176.96\n",
      "641 5142.05\n",
      "642 1973.32\n",
      "643 4237.67\n",
      "644 2766.96\n",
      "645 3334.83\n",
      "646 3559.03\n",
      "647 2433.2\n",
      "648 4349.23\n",
      "649 1533.8\n",
      "650 5136.04\n",
      "651 638.693\n",
      "652 5917.36\n",
      "653 219.075\n",
      "654 6226.27\n",
      "655 1021.99\n",
      "656 5317.63\n",
      "657 1819.14\n",
      "658 4412.76\n",
      "659 2613.19\n",
      "660 3509.69\n",
      "661 3405.5\n",
      "662 2607.86\n",
      "663 4196.03\n",
      "664 1708.06\n",
      "665 4983.49\n",
      "666 812.128\n",
      "667 5765.85\n",
      "668 68.3611\n",
      "669 6083.35\n",
      "670 1147.25\n",
      "671 5175.53\n",
      "672 1943.72\n",
      "673 4271.05\n",
      "674 2737.41\n",
      "675 3368.21\n",
      "676 3529.42\n",
      "677 2466.71\n",
      "678 4319.43\n",
      "679 1567.56\n",
      "680 5106.01\n",
      "681 672.756\n",
      "682 5887.18\n",
      "683 188.976\n",
      "684 6260.35\n",
      "685 991.905\n",
      "686 5351.52\n",
      "687 1789.18\n",
      "688 4446.53\n",
      "689 2583.3\n",
      "690 3543.44\n",
      "691 3375.59\n",
      "692 2641.7\n",
      "693 4165.97\n",
      "694 1742.11\n",
      "695 4953.22\n",
      "696 846.453\n",
      "697 5735.44\n",
      "698 43.7417\n",
      "699 4239.44\n",
      "700 2765.06\n",
      "701 3336.51\n",
      "702 3557.12\n",
      "703 2434.99\n",
      "704 4347.16\n",
      "705 1535.84\n",
      "706 5133.81\n",
      "707 640.951\n",
      "708 5915.19\n",
      "709 217.102\n",
      "710 6227.99\n",
      "711 1020.28\n",
      "712 5318.85\n",
      "713 1817.78\n",
      "714 4413.65\n",
      "715 2612.06\n",
      "716 3510.48\n",
      "717 3404.39\n",
      "718 2608.73\n",
      "719 4194.77\n",
      "720 1709.17\n",
      "721 4982.02\n",
      "722 813.533\n",
      "723 5764.33\n",
      "724 67.307\n",
      "725 6002.01\n",
      "726 1218.37\n",
      "727 5094.14\n",
      "728 2014.85\n",
      "729 4189.57\n",
      "730 2808.55\n",
      "731 3286.8\n",
      "732 3600.38\n",
      "733 2385.58\n",
      "734 4389.98\n",
      "735 1486.99\n",
      "736 5175.99\n",
      "737 592.917\n",
      "738 5956.65\n",
      "739 258.496\n",
      "740 6181.12\n",
      "741 1061.22\n",
      "742 5272.17\n",
      "743 1858.53\n",
      "744 4367.04\n",
      "745 2652.72\n",
      "746 3463.95\n",
      "747 3444.9\n",
      "748 2562.41\n",
      "749 4234.97\n",
      "750 1663.26\n",
      "751 5021.77\n",
      "752 768.2\n",
      "753 5803.56\n",
      "754 105.742\n",
      "755 6354.08\n",
      "756 909.418\n",
      "757 5444.28\n",
      "758 1707.4\n",
      "759 4538.69\n",
      "760 2501.97\n",
      "761 3635.36\n",
      "762 3294.39\n",
      "763 2733.62\n",
      "764 4084.75\n",
      "765 1834.14\n",
      "766 4872.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "767 938.484\n",
      "768 5654.57\n",
      "769 54.2569\n",
      "770 4599.98\n",
      "771 1247.68\n",
      "772 5385.59\n",
      "773 354.131\n",
      "774 6165.83\n",
      "775 467.701\n",
      "776 5942.91\n",
      "777 1270.2\n",
      "778 5034.0\n",
      "779 2067.48\n",
      "780 4128.81\n",
      "781 2861.68\n",
      "782 3225.72\n",
      "783 3653.78\n",
      "784 2324.3\n",
      "785 4443.66\n",
      "786 1425.42\n",
      "787 5230.13\n",
      "788 530.799\n",
      "789 6011.57\n",
      "790 313.772\n",
      "791 6117.2\n",
      "792 1117.25\n",
      "793 5207.43\n",
      "794 1915.2\n",
      "795 4301.77\n",
      "796 2709.79\n",
      "797 3398.41\n",
      "798 3502.18\n",
      "799 2496.76\n",
      "800 4292.37\n",
      "801 1597.5\n",
      "802 5079.38\n",
      "803 702.214\n",
      "804 5861.61\n",
      "805 164.08\n",
      "806 6286.94\n",
      "807 968.252\n",
      "808 5376.51\n",
      "809 1766.71\n",
      "810 4470.46\n",
      "811 2561.61\n",
      "812 3566.89\n",
      "813 3354.21\n",
      "814 2665.1\n",
      "815 4144.61\n",
      "816 1765.62\n",
      "817 4931.95\n",
      "818 869.907\n",
      "819 5714.72\n",
      "820 37.5308\n",
      "821 1523.38\n",
      "822 5144.22\n",
      "823 628.426\n",
      "824 5926.12\n",
      "825 228.537\n",
      "826 6213.76\n",
      "827 1032.46\n",
      "828 5303.49\n",
      "829 1830.79\n",
      "830 4397.51\n",
      "831 2625.62\n",
      "832 3493.99\n",
      "833 3418.14\n",
      "834 2592.28\n",
      "835 4208.41\n",
      "836 1692.96\n",
      "837 4995.56\n",
      "838 797.499\n",
      "839 5778.08\n",
      "840 81.1619\n",
      "841 6132.53\n",
      "842 1103.53\n",
      "843 5223.02\n",
      "844 1901.24\n",
      "845 4317.46\n",
      "846 2695.7\n",
      "847 3414.24\n",
      "848 3487.86\n",
      "849 2512.89\n",
      "850 4277.67\n",
      "851 1614.13\n",
      "852 5064.15\n",
      "853 719.522\n",
      "854 5845.81\n",
      "855 148.249\n",
      "856 6305.15\n",
      "857 952.051\n",
      "858 5394.84\n",
      "859 1750.37\n",
      "860 4488.82\n",
      "861 2545.21\n",
      "862 3585.32\n",
      "863 3337.66\n",
      "864 2683.74\n",
      "865 4127.77\n",
      "866 1784.65\n",
      "867 4914.71\n",
      "868 889.45\n",
      "869 5697.06\n",
      "870 36.7194\n",
      "871 424.42\n",
      "872 5992.61\n",
      "873 1226.1\n",
      "874 5084.04\n",
      "875 2023.04\n",
      "876 4179.02\n",
      "877 2816.99\n",
      "878 3276.19\n",
      "879 3608.65\n",
      "880 2375.39\n",
      "881 4397.77\n",
      "882 1477.49\n",
      "883 5183.25\n",
      "884 584.118\n",
      "885 5963.72\n",
      "886 265.904\n",
      "887 6171.91\n",
      "888 1068.81\n",
      "889 5262.29\n",
      "890 1866.57\n",
      "891 4356.64\n",
      "892 2661.06\n",
      "893 3453.41\n",
      "894 3453.17\n",
      "895 2552.2\n",
      "896 4242.81\n",
      "897 1653.69\n",
      "898 5029.08\n",
      "899 759.336\n",
      "900 5810.6\n",
      "901 113.119\n",
      "902 6345.03\n",
      "903 916.89\n",
      "904 5434.6\n",
      "905 1715.29\n",
      "906 4528.47\n",
      "907 2510.18\n",
      "908 3624.93\n",
      "909 3302.6\n",
      "910 2723.46\n",
      "911 4092.57\n",
      "912 1824.57\n",
      "913 4879.35\n",
      "914 929.596\n",
      "915 5661.58\n",
      "916 50.2932\n",
      "917 3802.38\n",
      "918 2154.55\n",
      "919 4591.37\n",
      "920 1256.83\n",
      "921 5376.69\n",
      "922 363.689\n",
      "923 6157.01\n",
      "924 459.257\n",
      "925 5951.62\n",
      "926 1262.1\n",
      "927 5041.93\n",
      "928 2059.91\n",
      "929 4136.19\n",
      "930 2854.44\n",
      "931 3232.91\n",
      "932 3646.53\n",
      "933 2331.77\n",
      "934 4436.06\n",
      "935 1433.42\n",
      "936 5222.16\n",
      "937 539.309\n",
      "938 6003.51\n",
      "939 306.088\n",
      "940 6125.2\n",
      "941 1109.78\n",
      "942 5214.74\n",
      "943 1908.19\n",
      "944 4308.51\n",
      "945 2703.13\n",
      "946 3404.92\n",
      "947 3495.54\n",
      "948 2503.51\n",
      "949 4285.41\n",
      "950 1604.77\n",
      "951 5072.01\n",
      "952 710.029\n",
      "953 5854.08\n",
      "954 156.912\n",
      "955 6294.48\n",
      "956 961.202\n",
      "957 5383.41\n",
      "958 1760.08\n",
      "959 4476.8\n",
      "960 2555.33\n",
      "961 3572.99\n",
      "962 3347.96\n",
      "963 2671.39\n",
      "964 4138.07\n",
      "965 1772.4\n",
      "966 4925.0\n",
      "967 877.249\n",
      "968 5707.55\n",
      "969 38.6652\n",
      "970 566.803\n",
      "971 5979.1\n",
      "972 281.658\n",
      "973 6153.12\n",
      "974 1085.17\n",
      "975 5242.73\n",
      "976 1883.5\n",
      "977 4336.53\n",
      "978 2678.39\n",
      "979 3433.0\n",
      "980 3470.7\n",
      "981 2531.72\n",
      "982 4260.42\n",
      "983 1633.18\n",
      "984 5046.83\n",
      "985 738.693\n",
      "986 5828.69\n",
      "987 131.531\n",
      "988 6323.43\n",
      "989 935.692\n",
      "990 5412.4\n",
      "991 1734.53\n",
      "992 4505.79\n",
      "993 2529.75\n",
      "994 3602.0\n",
      "995 3322.32\n",
      "996 2700.5\n",
      "997 4112.31\n",
      "998 1801.67\n",
      "999 4899.09\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(1000):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.57181602441\n",
      "Accuracy on the test set is  0.577983725136\n"
     ]
    }
   ],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
