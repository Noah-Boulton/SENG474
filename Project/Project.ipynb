{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--\n",
    "In our ever changing and evolving field of technology one of the foremost topics is data analytics. The growth in the amount of digital data that is being collected across many different fields is massive and . That is, taking data in whatever raw form it exists and using technology to transform it into information that has value and context. In most cases data analysis is performed in order to provide class descriptions of data, highlight behaviors, trends, associations in the data or predictive information that prove useful or even vital to key decision-makers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "--\n",
    "Using the NHL API following the documentation found at https://gitlab.com/dword4/nhlapi/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each player in the specified year range (years must be consecutive) collect all avalible stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_skaters(y1, y2):\n",
    "    team_rosters = requests.get('https://statsapi.web.nhl.com/api/v1/teams?expand=team.roster&season=' + y1 + y2)\n",
    "    team_rosters = team_rosters.json()\n",
    "    players= []\n",
    "    for i in range(0, len(team_rosters['teams'])):\n",
    "        for j in range(0, len(team_rosters['teams'][i]['roster']['roster'])):\n",
    "            player = [team_rosters['teams'][i]['roster']['roster'][j]['person']['id'], \n",
    "                      team_rosters['teams'][i]['roster']['roster'][j]['person']['fullName'],\n",
    "                     team_rosters['teams'][i]['name']]\n",
    "            if (team_rosters['teams'][i]['roster']['roster'][j]['position']['code'] != 'G'):\n",
    "                players.append(player)\n",
    "    players_stats = []\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'fullName', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "    for i in range(0, len(players)): \n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            players_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        players_stats.append(stats_array)\n",
    "        \n",
    "    skaters = []\n",
    "    skaters.append(header)\n",
    "    for i in range(0, len(players)):\n",
    "        skaters.append(players[i] + players_stats[i])\n",
    "    return skaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the skaters data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_skaters_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting skaters data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        skaters = get_csv_skaters(str(i), str(i+1))\n",
    "        np.savetxt('data/skaters_' + str(i) + '_' + str(i+1) + '.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_skaters_data(1917,1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each team in the specified year range (years must be consecutive) collect all avalible stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_team(y1, y2):\n",
    "    teams = requests.get('https://statsapi.web.nhl.com/api/v1/teams?season=' + str(y1) + str(y2))\n",
    "    teams = teams.json()\n",
    "    team_id_name = []\n",
    "    for i in range(0, len(teams['teams'])):\n",
    "        team_arr = [teams['teams'][i]['id'], teams['teams'][i]['name']]\n",
    "        team_id_name.append(team_arr)\n",
    "\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                           + str(team_id_name[0][0])\n",
    "                           + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "\n",
    "    team_stats = []\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                             + str(team_id_name[i][0]) \n",
    "                             + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            team_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        team_stats.append(stats_array)\n",
    "\n",
    "    \n",
    "    teams_stats_final = []\n",
    "    teams_stats_final.append(header)\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        teams_stats_final.append(team_id_name[i] + team_stats[i]) \n",
    "    return teams_stats_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the team data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting team data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_team(str(i), str(i+1))\n",
    "        np.savetxt('team_data/teams_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_team_data(1917, 1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the index of the team stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_stats(id, team_data):\n",
    "    for i in range(0, len(team_data)):\n",
    "        if team_data[i][0] == id:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each game in the specified year range (years must be consecutive) return the winner, away team ID, home team ID, and the away and home team stats for that season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_game(y1, y2):\n",
    "    team_data = get_csv_team(y1, y2)\n",
    "    header = team_data[0][3:]\n",
    "    away_header = []\n",
    "    home_header = []\n",
    "    for head in header:\n",
    "        away_header.append('away_'+head)\n",
    "        home_header.append('home_'+head)\n",
    "    games_data = [['winner', 'awayID', 'homeID'] + away_header + home_header]\n",
    "    games = requests.get('https://statsapi.web.nhl.com/api/v1/schedule?startDate=' \n",
    "                         + str(y1) + '-10-01&endDate=' + str(y2) + '-06-30')\n",
    "    games = games.json()\n",
    "    for date in games['dates']:\n",
    "        for game in date['games']:\n",
    "            away_ID = game['teams']['away']['team']['id']\n",
    "            home_ID = game['teams']['home']['team']['id']\n",
    "            if away_ID > 80 or home_ID > 80:\n",
    "                continue\n",
    "            \n",
    "            away_score = game['teams']['away']['score']\n",
    "            home_score = game['teams']['home']['score']\n",
    "            winner = 0\n",
    "            away_stats = team_data[get_team_stats(away_ID, team_data)][3:]\n",
    "            home_stats = team_data[get_team_stats(home_ID, team_data)][3:]\n",
    "            if home_score > away_score:\n",
    "                winner = 1\n",
    "            games_data.append([winner,\n",
    "                          away_ID, \n",
    "                          home_ID] +\n",
    "                          away_stats +\n",
    "                          home_stats)\n",
    "    return games_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the game data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_game_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting game data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_game(i, i+1)\n",
    "        np.savetxt('game_data/game_data_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_game_data(1917,1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aanaylsis\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header for each dataset is \n",
    "winner, awayID, homeID, wins, losses, ot, pts, ptPctg, goalsPerGame, goalsAgainstPerGame, evGGARatio, powerPlayPercentage, powerPlayGoals, powerPlayGoalsAgainst, powerPlayOpportunities, penaltyKillPercentage, shotsPerGame, shotsAllowed, winScoreFirst, winOppScoreFirst, winLeadFirstPer, winLeadSecondPer, winOutshootOpp, win OutshotByOpp, faceOffstaken, faceOffsWon, faceOffsLost, faceOffWinPercentage, shootingPctg, savePctg\n",
    "\n",
    "Index(['winner', 'awayID', 'homeID', 'wins', 'losses', 'ot', 'pts', 'ptPctg',\n",
    "       'goalsPerGame', 'goalsAgainstPerGame', 'evGGARatio',\n",
    "       'powerPlayPercentage', 'powerPlayGoals', 'powerPlayGoalsAgainst',\n",
    "       'powerPlayOpportunities', 'penaltyKillPercentage', 'shotsPerGame',\n",
    "       'shotsAllowed', 'winScoreFirst', 'winOppScoreFirst', 'winLeadFirstPer',\n",
    "       'winLeadSecondPer', 'winOutshootOpp', 'winOutshotByOpp',\n",
    "       'faceOffsTaken', 'faceOffsWon', 'faceOffsLost', 'faceOffWinPercentage',\n",
    "       'shootingPctg', 'savePctg', 'wins.1', 'losses.1', 'ot.1', 'pts.1',\n",
    "       'ptPctg.1', 'goalsPerGame.1', 'goalsAgainstPerGame.1', 'evGGARatio.1',\n",
    "       'powerPlayPercentage.1', 'powerPlayGoals.1', 'powerPlayGoalsAgainst.1',\n",
    "       'powerPlayOpportunities.1', 'penaltyKillPercentage.1', 'shotsPerGame.1',\n",
    "       'shotsAllowed.1', 'winScoreFirst.1', 'winOppScoreFirst.1',\n",
    "       'winLeadFirstPer.1', 'winLeadSecondPer.1', 'winOutshootOpp.1',\n",
    "       'winOutshotByOpp.1', 'faceOffsTaken.1', 'faceOffsWon.1',\n",
    "       'faceOffsLost.1', 'faceOffWinPercentage.1', 'shootingPctg.1',\n",
    "       'savePctg.1'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    max_data = np.max(data, axis=0)\n",
    "    min_data = np.min(data, axis=0)\n",
    "    stats = ['away_wins', 'away_losses', 'away_ot',\n",
    "             'away_pts', 'away_ptPctg', 'away_goalsPerGame',\n",
    "             'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "             'away_powerPlayPercentage', 'away_powerPlayGoals',\n",
    "             'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities',\n",
    "             'away_penaltyKillPercentage', 'away_shotsPerGame', 'away_shotsAllowed',\n",
    "             'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "             'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "             'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "             'away_faceOffWinPercentage', 'away_shootingPctg', 'away_savePctg',\n",
    "             'home_wins', 'home_losses', 'home_ot', 'home_pts', 'home_ptPctg',\n",
    "             'home_goalsPerGame', 'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "             'home_powerPlayPercentage', 'home_powerPlayGoals',\n",
    "             'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities',\n",
    "             'home_penaltyKillPercentage', 'home_shotsPerGame', 'home_shotsAllowed',\n",
    "             'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "             'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "             'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "             'home_faceOffWinPercentage', 'home_shootingPctg', 'home_savePctg']\n",
    "    for stat in stats:\n",
    "        data[stat] = (data[stat] - min_data[stat])/(max_data[stat] - min_data[stat])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>awayID</th>\n",
       "      <th>homeID</th>\n",
       "      <th>away_wins</th>\n",
       "      <th>away_losses</th>\n",
       "      <th>away_ot</th>\n",
       "      <th>away_pts</th>\n",
       "      <th>away_ptPctg</th>\n",
       "      <th>away_goalsPerGame</th>\n",
       "      <th>away_goalsAgainstPerGame</th>\n",
       "      <th>...</th>\n",
       "      <th>home_winLeadFirstPer</th>\n",
       "      <th>home_winLeadSecondPer</th>\n",
       "      <th>home_winOutshootOpp</th>\n",
       "      <th>home_winOutshotByOpp</th>\n",
       "      <th>home_faceOffsTaken</th>\n",
       "      <th>home_faceOffsWon</th>\n",
       "      <th>home_faceOffsLost</th>\n",
       "      <th>home_faceOffWinPercentage</th>\n",
       "      <th>home_shootingPctg</th>\n",
       "      <th>home_savePctg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.803099</td>\n",
       "      <td>0.075019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.922902</td>\n",
       "      <td>0.975930</td>\n",
       "      <td>0.974771</td>\n",
       "      <td>0.709957</td>\n",
       "      <td>0.150456</td>\n",
       "      <td>0.315615</td>\n",
       "      <td>0.191977</td>\n",
       "      <td>0.560440</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863524</td>\n",
       "      <td>0.834087</td>\n",
       "      <td>0.197989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768707</td>\n",
       "      <td>0.792123</td>\n",
       "      <td>0.571101</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.524316</td>\n",
       "      <td>0.352159</td>\n",
       "      <td>0.512894</td>\n",
       "      <td>0.340659</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.287879</td>\n",
       "      <td>0.287841</td>\n",
       "      <td>0.330536</td>\n",
       "      <td>0.584687</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978118</td>\n",
       "      <td>0.940367</td>\n",
       "      <td>0.601732</td>\n",
       "      <td>0.384498</td>\n",
       "      <td>0.343854</td>\n",
       "      <td>0.388252</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.575682</td>\n",
       "      <td>0.559070</td>\n",
       "      <td>0.508894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.691466</td>\n",
       "      <td>0.566514</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.594225</td>\n",
       "      <td>0.486711</td>\n",
       "      <td>0.462751</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>0.668819</td>\n",
       "      <td>0.169374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.509847</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.073593</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.588040</td>\n",
       "      <td>0.356734</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   winner  awayID  homeID  away_wins  away_losses  away_ot  away_pts  \\\n",
       "0       0      21      25   1.000000     0.000000    0.375  1.000000   \n",
       "1       0       9       6   0.870968     0.142857    0.375  0.863636   \n",
       "2       1      16       7   0.258065     0.685714    0.500  0.287879   \n",
       "3       1      23       4   0.483871     0.342857    0.750  0.575758   \n",
       "4       0      17      20   0.903226     0.114286    0.375  0.893939   \n",
       "\n",
       "   away_ptPctg  away_goalsPerGame  away_goalsAgainstPerGame      ...        \\\n",
       "0     1.000000           0.803099                  0.075019      ...         \n",
       "1     0.863524           0.834087                  0.197989      ...         \n",
       "2     0.287841           0.330536                  0.584687      ...         \n",
       "3     0.575682           0.559070                  0.508894      ...         \n",
       "4     0.893300           0.668819                  0.169374      ...         \n",
       "\n",
       "   home_winLeadFirstPer  home_winLeadSecondPer  home_winOutshootOpp  \\\n",
       "0              0.922902               0.975930             0.974771   \n",
       "1              0.768707               0.792123             0.571101   \n",
       "2              1.000000               0.978118             0.940367   \n",
       "3              0.557823               0.691466             0.566514   \n",
       "4              0.557823               0.509847             0.458716   \n",
       "\n",
       "   home_winOutshotByOpp  home_faceOffsTaken  home_faceOffsWon  \\\n",
       "0              0.709957            0.150456          0.315615   \n",
       "1              0.357143            0.524316          0.352159   \n",
       "2              0.601732            0.384498          0.343854   \n",
       "3              0.857143            0.594225          0.486711   \n",
       "4              0.073593            0.574468          0.588040   \n",
       "\n",
       "   home_faceOffsLost  home_faceOffWinPercentage  home_shootingPctg  \\\n",
       "0           0.191977                   0.560440           0.942857   \n",
       "1           0.512894                   0.340659           0.400000   \n",
       "2           0.388252                   0.428571           0.485714   \n",
       "3           0.462751                   0.461538           0.457143   \n",
       "4           0.356734                   0.604396           0.171429   \n",
       "\n",
       "   home_savePctg  \n",
       "0       0.733333  \n",
       "1       0.000000  \n",
       "2       1.000000  \n",
       "3       0.566667  \n",
       "4       0.266667  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "\n",
    "    # we normalize X\n",
    "#     maxX = np.max(X, axis=0)\n",
    "#     minX = np.min(X, axis=0)\n",
    "#     X = (X-minX)/(maxX-minX)\n",
    "\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    \n",
    "    # we normalize y\n",
    "#     maxy = np.max(y, axis=0)\n",
    "#     miny = np.min(y, axis=0)\n",
    "#     y = (y-miny)/(maxy-miny)\n",
    "\n",
    "    where_are_zeros = (y==0)\n",
    "    y[where_are_zeros] = -1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22122, 55)\n",
      "(22122, 1)\n",
      "[[ 1.          1.          0.         ...,  0.56043956  0.94285714\n",
      "   0.73333333]\n",
      " [ 1.          0.87096774  0.14285714 ...,  0.34065934  0.4         0.        ]\n",
      " [ 1.          0.25806452  0.68571429 ...,  0.42857143  0.48571429  1.        ]\n",
      " ..., \n",
      " [ 1.          0.89655172  0.22222222 ...,  0.62626263  1.          0.53333333]\n",
      " [ 1.          0.89655172  0.22222222 ...,  0.62626263  1.          0.53333333]\n",
      " [ 1.          0.82758621  0.2962963  ...,  0.47474747  0.74193548  0.6       ]] [[-1]\n",
      " [-1]\n",
      " [ 1]\n",
      " ..., \n",
      " [ 1]\n",
      " [ 1]\n",
      " [-1]]\n"
     ]
    }
   ],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def error(x,y,w):\n",
    "    #print(-y*x@w.T)\n",
    "    return np.log(1+np.exp(-y*x@w.T))\n",
    "\n",
    "#TODO\n",
    "def error_mean(X,y,w):\n",
    "    return sum(error(X,y,w))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def grad(x,y,w):\n",
    "    #print(y*x@w.T)\n",
    "    return (y*x)/(1+np.exp(y*x@w.T))\n",
    "\n",
    "#TODO\n",
    "def grad_mean(X,y,w):\n",
    "    return -1/len(y)*sum(grad(X,y,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X,y,kappa,iter):\n",
    "    w = np.zeros((1,X.shape[1]))\n",
    "    E = []\n",
    "\n",
    "    #TODO\n",
    "    for i in range(0,iter):\n",
    "        E.append(error_mean(X,y,w))\n",
    "        w = w - kappa*grad_mean(X,y,w)\n",
    "    return w,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.43620995e-06  -2.99744695e-06   5.91523929e-06   2.80639671e-06\n",
      "   -5.31380303e-06  -3.27616875e-06   7.53547118e-08   3.11775940e-06\n",
      "   -9.76453651e-08  -3.55971221e-09  -8.72447761e-07   4.43923865e-06\n",
      "    2.98254381e-06   3.47191796e-07   2.30110087e-07   2.94262359e-06\n",
      "    3.90446640e-07   1.41930371e-08   8.45607593e-07   1.01755158e-06\n",
      "    1.71409812e-07  -3.41067690e-09   1.49699132e-05   6.57870855e-06\n",
      "    1.02075709e-05   1.29237500e-06   5.13134783e-07   7.57858135e-07\n",
      "    7.25753000e-06  -3.25307326e-06   6.08331209e-07   1.01164202e-05\n",
      "    7.75809508e-06   3.20676643e-06  -1.51649139e-07   3.25956664e-06\n",
      "    3.24547885e-06   4.05277480e-06  -2.17199144e-07   2.32389904e-06\n",
      "    3.44437122e-06   2.76363378e-06   3.32218242e-07   3.73331838e-06\n",
      "    3.33894766e-06   3.59790454e-06   3.37172120e-06   3.85526593e-06\n",
      "    3.61295366e-06  -4.89926610e-08   9.42409866e-06  -7.91571645e-06\n",
      "    2.49724569e-06   3.21045409e-06   2.98821113e-06]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XuQXOWd3vHv06dHNzAIC63L3CK8\nSCHCFjIrwBttHLADkcwSbRUYpF3KmPJCQqLKAmt5YSvLBrZILVuOcYxVW6G4WMbhkrBrUMVaI5dx\nEYpbIexIRgIFrWBBxqxkEMiMkGZ69Msf5+3RmZ5zunuaEYLR86maUvfb77lNQz/zXs7bigjMzMzG\nqnawT8DMzD6cHCBmZtYTB4iZmfXEAWJmZj1xgJiZWU8cIGZm1hMHiJmZ9cQBYmZmPXGAmJlZT+oH\n+wQOpKOPPjpmzZp1sE/DzOxD5dlnn/1VRMzsVG9CB8isWbNYt27dwT4NM7MPFUn/0E09d2GZmVlP\nHCBmZtYTB4iZmfXEAWJmZj1xgJiZWU8cIGZm1hMHiJmZ9cQBUqJ/b4NvrN3Mz17ZebBPxczsA8sB\nUmJvYx/femQLG7a9fbBPxczsA8sBUiKrCYDGvjjIZ2Jm9sHlAClRbwbI0L6DfCZmZh9cDpAS9cwt\nEDOzThwgJeq1/Ncy5AAxM6vkACmRerDchWVm1oYDpIQk+jK5C8vMrA0HSIWsJndhmZm10VWASFok\nabOkLZKurahzkaRNkjZKuqdQfrOk59LPxYXyOyStl7RB0gOSDk/ln5X0U0kNSReWHOcISb+Q9O2x\nX2736rUag0MOEDOzKh0DRFIGrAQWA3OBZZLmttSZDVwHLIyIU4CrUvl5wGnAfOBMYIWkI9JmV0fE\nqRExD3gFWJ7KXwG+DNxDub8AHu32AntVz8TQPo+BmJlV6aYFcgawJSK2RsQAcB+wpKXO5cDKiNgJ\nEBHbU/lc4NGIaEREP7AeWJTq7AKQJGAqEKn85YjYAIz69Jb0W8DHgLVjusoe1GseAzEza6ebADkW\neLXwfFsqK5oDzJH0uKSnJC1K5euBxZKmSToaOBs4vrmRpLuA14GTgVvbnYSkGvBfgRVdnPN7ltVE\nw11YZmaVugkQlZS1frLWgdnAWcAy4HZJ0yNiLbAGeAK4F3gSaAzvJOIy4BjgeeBi2vv3wJqIeLVd\nJUlXSFonad2OHTs67LJavVZzC8TMrI1uAmQbhVYDcBzwWkmdhyJiMCJeAjaTBwoRcVNEzI+Ic8jD\n6MXihhExBNwPXNDhPH4bWC7pZeDrwJck/WVrpYi4LSIWRMSCmTNndnF55TwGYmbWXjcB8gwwW9KJ\nkiYBS4HVLXUeJO+eInVVzQG2SsokzUjl84B5wFrlTkrlAs4HXmh3EhHxBxFxQkTMAr4KfDciSmeE\njYesJgbdAjEzq1TvVCEiGpKWAw8DGXBnRGyUdCOwLiJWp9fOlbQJGAJWRMQbkqYAj+UZwS7gkrS/\nGrAqzcgS+VjJlQCSTge+DxwFnC/phjSz633VV6sx5DEQM7NKHQMEICLWkI9lFMuuLzwO4Jr0U6yz\nh3wmVuv+9gELK471DHk3Wbvz+Q7wnW7OvVeZZ2GZmbXlO9Er1DPR8BiImVklB0iFupcyMTNrywFS\noV6r+T4QM7M2HCAV8jEQd2GZmVVxgFSoezl3M7O2HCAVPAZiZtaeA6RC5uXczczacoBU6PNSJmZm\nbTlAKvhGQjOz9hwgFepezt3MrC0HSIV6VvMguplZGw6QCnXfB2Jm1pYDpIK/kdDMrD0HSIW+zN9I\naGbWjgOkQuYbCc3M2nKAVKjXxOCQx0DMzKo4QCrk34nuFoiZWRUHSIWslo+B5F+2aGZmrRwgFeo1\nAbgVYmZWwQFSoZ7lAeKZWGZm5RwgFdwCMTNrzwFSIavlvxrfTGhmVs4BUqFvuAvLU3nNzMo4QCpk\n7sIyM2urqwCRtEjSZklbJF1bUeciSZskbZR0T6H8ZknPpZ+LC+V3SFovaYOkByQdnso/K+mnkhqS\nLizUny/pybT/DcV9HQjNMZBBB4iZWamOASIpA1YCi4G5wDJJc1vqzAauAxZGxCnAVan8POA0YD5w\nJrBC0hFps6sj4tSImAe8AixP5a8AXwbuYaTdwJfS/hcB35Q0fWyX2716GgMZ8hiImVmpblogZwBb\nImJrRAwA9wFLWupcDqyMiJ0AEbE9lc8FHo2IRkT0A+vJP/yJiF0AkgRMBSKVvxwRG4ARgw8R8f8i\n4sX0+DVgOzBzjNfbtbrHQMzM2uomQI4FXi0835bKiuYAcyQ9LukpSYtS+XpgsaRpko4GzgaOb24k\n6S7gdeBk4NZuT1rSGcAk4O9LXrtC0jpJ63bs2NHtLkdpjoH4PhAzs3LdBIhKylo/VevAbOAsYBlw\nu6TpEbEWWAM8AdwLPAk0hncScRlwDPA80NWYhqSPA3cDl0XEqOZBRNwWEQsiYsHMmb03UOqexmtm\n1lY3AbKNQqsBOA54raTOQxExGBEvAZvJA4WIuCki5kfEOeRh9GJxw4gYAu4HLuh0Imn85AfAf4qI\np7o49575RkIzs/a6CZBngNmSTpQ0CVgKrG6p8yB59xSpq2oOsFVSJmlGKp8HzAPWKndSKhdwPvBC\nu5NIx/4+8N2I+F/dXmCvsqw5C8tjIGZmZeqdKkREQ9Jy4GEgA+6MiI2SbgTWRcTq9Nq5kjYBQ8CK\niHhD0hTgsTwj2AVckvZXA1alFoXIx0quBJB0OnlQHAWcL+mGNPPqIuCzwAxJX06n9+WI+L/j86sY\nqa85C8stEDOzUh0DBCAi1pCPZRTLri88DuCa9FOss4d8Jlbr/vYBCyuO9Qx5N1lr+feA73VzvuNh\neBDdYyBmZqV8J3oFT+M1M2vPAVKh7mm8ZmZtOUAq+E50M7P2HCAV9t9I6C4sM7MyDpAKff5GQjOz\nthwgFbycu5lZew6QCs0xkEGPgZiZlXKAVGhO4x3yGIiZWSkHSAVP4zUza88BUsF3opuZtecAqVDP\n0nLuboGYmZVygFTYv5y7x0DMzMo4QCo0u7A8C8vMrJwDpEJf5uXczczacYBUSA0Qj4GYmVVwgFSQ\nRL0mGkMeAzEzK+MAaaOeyV1YZmYVHCBt1Gs1d2GZmVVwgLSRuQvLzKySA6SNvkxugZiZVXCAtJHV\nPAZiZlbFAdJGvVbzjYRmZhUcIG3ks7A8BmJmVqarAJG0SNJmSVskXVtR5yJJmyRtlHRPofxmSc+l\nn4sL5XdIWi9pg6QHJB2eyj8r6aeSGpIubDnGpZJeTD+X9nbJ3ctqHgMxM6tS71RBUgasBM4BtgHP\nSFodEZsKdWYD1wELI2KnpN9I5ecBpwHzgcnAo5L+LiJ2AVenf5H0DWA58JfAK8CXga+2nMdHgT8H\nFgABPJvOY+d7uP628hsJHSBmZmW6aYGcAWyJiK0RMQDcByxpqXM5sLL5YR4R21P5XODRiGhERD+w\nHliU6jTDQ8BU8lAgIl6OiA1Aa9/RvwZ+FBFvpuP8qLmvA8X3gZiZVesmQI4FXi0835bKiuYAcyQ9\nLukpSc0P9vXAYknTJB0NnA0c39xI0l3A68DJwK3jcB7jymMgZmbVugkQlZS1/lleB2YDZwHLgNsl\nTY+ItcAa4AngXuBJoDG8k4jLgGOA54GLaa+b80DSFZLWSVq3Y8eODrtsz2MgZmbVugmQbRRaDcBx\nwGsldR6KiMGIeAnYTB4oRMRNETE/Is4hD4EXixtGxBBwP3DBOJwHEXFbRCyIiAUzZ87seHHt9NVq\nHgMxM6vQTYA8A8yWdKKkScBSYHVLnQfJu6dIXVVzgK2SMkkzUvk8YB6wVrmTUrmA84EXOpzHw8C5\nko6SdBRwbio7YHwjoZlZtY6zsCKiIWk5+Yd1BtwZERsl3Qisi4jV7P9w3wQMASsi4g1JU4DH8oxg\nF3BJ2l8NWCXpCPJWyXrgSgBJpwPfB44Czpd0Q0ScEhFvSvoL8kADuDEi3hy330SJeib2NIYO5CHM\nzD60OgYIQESsIR/LKJZdX3gcwDXpp1hnD/lMrNb97QMWVhzrGfLuqbLX7gTu7Oacx0PdLRAzs0q+\nE72NzGMgZmaVHCBt1Gui4Wm8ZmalHCBt1L2cu5lZJQdIGx4DMTOr5gBpw2MgZmbVHCBt5N9I6DEQ\nM7MyDpA2fCOhmVk1B0gb9Zr8jYRmZhUcIG3Us5pbIGZmFRwgbfg+EDOzag6QNjJ/I6GZWSUHSBv1\nLP9GwnypLzMzK3KAtFGv5d9h5WEQM7PRHCBtZClAPA5iZjaaA6SNviwFiMdBzMxGcYC0kdXyX48X\nVDQzG80B0kZzDMT3gpiZjeYAaaM+3IXlMRAzs1YOkDbqw4PoboGYmbVygLTRHANxF5aZ2WgOkDaa\ns7AG3YVlZjaKA6SNzIPoZmaVHCBteAzEzKyaA6SNevM+EN9IaGY2SlcBImmRpM2Stki6tqLORZI2\nSdoo6Z5C+c2Snks/FxfK75C0XtIGSQ9IOjyVT5Z0fzrW05JmpfI+Sask/VzS85Kuey8X3o0s81Im\nZmZVOgaIpAxYCSwG5gLLJM1tqTMbuA5YGBGnAFel8vOA04D5wJnACklHpM2ujohTI2Ie8AqwPJV/\nBdgZEScBtwA3p/IvApMj4lPAbwH/thkuB4pvJDQzq9ZNC+QMYEtEbI2IAeA+YElLncuBlRGxEyAi\ntqfyucCjEdGIiH5gPbAo1dkFIEnAVKD5Kb0EWJUePwB8PtUJ4DBJ9VR/ANg1xusdk2YXlr/W1sxs\ntG4C5Fjg1cLzbamsaA4wR9Ljkp6StCiVrwcWS5om6WjgbOD45kaS7gJeB04Gbm09XkQ0gLeBGeRh\n0g/8krzF8vWIeLP1ZCVdIWmdpHU7duzo4vKqNe9EdwvEzGy0bgJEJWWtn6h1YDZwFrAMuF3S9IhY\nC6wBngDuBZ4EGsM7ibgMOAZ4HmiOj1Qd7wxgKNU/EfhjSZ8YVTHitohYEBELZs6c2cXlVfNy7mZm\n1boJkG0UWg3AccBrJXUeiojBiHgJ2EweKETETRExPyLOIQ+HF4sbRsQQcD9wQevxUnfVkcCbwO8D\nP0zH2A48Dizo9kJ70edZWGZmlboJkGeA2ZJOlDQJWAqsbqnzIHn3FKmrag6wVVImaUYqnwfMA9Yq\nd1IqF3A+8ELa12rg0vT4QuCRyL9T9hXgc2nbw4DPFLY5IDLfB2JmVqneqUJENCQtBx4GMuDOiNgo\n6UZgXUSsTq+dK2kTeTfTioh4Q9IU4LE8I9gFXJL2VwNWpRlZIh8ruTId8g7gbklbyFseS1P5SuAu\n4Lm0zV0RsWEcfgeVPAZiZlatY4AARMQa8rGMYtn1hccBXJN+inX2kM/Eat3fPmBhxbH2kE/ZbS1/\np6z8QKp7DMTMrJLvRG/Dd6KbmVVzgLSRuQvLzKySA6SNvtSFNeguLDOzURwgbXg5dzOzag6QNjwG\nYmZWzQHSRt2r8ZqZVXKAtOEbCc3MqjlA2hhezt1dWGZmozhA2siGZ2E5QMzMWjlA2pBEvSaGPAZi\nZjaKA6SDrCaPgZiZlXCAdNCX1TyN18yshAOkg6wm30hoZlbCAdJBvSbfB2JmVsIB0kE9k7uwzMxK\nOEA6qNdqHkQ3MyvhAOnAYyBmZuUcIB3UMzE45DEQM7NWDpAO6m6BmJmVcoB0kHkMxMyslAOkg75M\nNNyFZWY2igOkAy9lYmZWzgHSgcdAzMzKdRUgkhZJ2ixpi6RrK+pcJGmTpI2S7imU3yzpufRzcaH8\nDknrJW2Q9ICkw1P5ZEn3p2M9LWlWYZt5kp5Mx/i5pCm9Xni36jWvhWVmVqZjgEjKgJXAYmAusEzS\n3JY6s4HrgIURcQpwVSo/DzgNmA+cCayQdETa7OqIODUi5gGvAMtT+VeAnRFxEnALcHPaVx34HvDv\n0jHOAgZ7vO6u1TMvZWJmVqabFsgZwJaI2BoRA8B9wJKWOpcDKyNiJ0BEbE/lc4FHI6IREf3AemBR\nqrMLQJKAqUDzz/wlwKr0+AHg86nOucCGiFiftn8jIobGesFj5RsJzczKdRMgxwKvFp5vS2VFc4A5\nkh6X9JSkRal8PbBY0jRJRwNnA8c3N5J0F/A6cDJwa+vxIqIBvA3MSMcISQ9L+qmkr43hOntWr9UY\ndBeWmdko9S7qqKSs9RO1Dswm71Y6DnhM0icjYq2k04EngB3Ak0BjeCcRl6UusluBi4G72hyvDvwO\ncDqwG/ixpGcj4scjTla6ArgC4IQTTuji8trzILqZWbluWiDbKLQayAPitZI6D0XEYES8BGwmDxQi\n4qaImB8R55CHw4vFDVM31P3ABa3HS+MeRwJvpvJHI+JXEbEbWEM+vkLL/m6LiAURsWDmzJldXF57\nmcdAzMxKdRMgzwCzJZ0oaRKwFFjdUudB8u4pUlfVHGCrpEzSjFQ+D5gHrFXupFQu4HzghbSv1cCl\n6fGFwCMREcDDwLzUHVYH/iWwqZeLHos+3wdiZlaqYxdWRDQkLSf/AM+AOyNio6QbgXURsTq9dq6k\nTcAQsCIi3kjTbB/LM4JdwCVpfzVgVZqRJfKxkivTIe8A7pa0hbzlsTSdx05J3yAPtADWRMQPxun3\nUCnzNF4zs1LdjIEQEWvIu4yKZdcXHgdwTfop1tlDPhOrdX/7gIUVx9oDfLHite+RT+V933gMxMys\nnO9E78D3gZiZlXOAdFD3GIiZWSkHSAdZrcaQx0DMzEZxgHTQl4lBd2GZmY3iAOnAS5mYmZVzgHTg\nMRAzs3IOkA7qWY0I3AoxM2vhAOkgq+VLc3kqr5nZSA6QDuopQNwCMTMbyQHSQT3Lf0Ve0t3MbCQH\nSAdugZiZlXOAdOAxEDOzcg6QDvqyFCDuwjIzG8EB0kFWy39F7sIyMxvJAdJBfbgLywFiZlbkAOmg\nPtyF5TEQM7MiB0gHboGYmZVzgHTgMRAzs3IOkA6aXViD7sIyMxvBAdKBbyQ0MyvnAOkg8xiImVkp\nB0gHfWktLN9IaGY2kgOkAy9lYmZWrqsAkbRI0mZJWyRdW1HnIkmbJG2UdE+h/GZJz6Wfiwvld0ha\nL2mDpAckHZ7KJ0u6Px3raUmzWo5zgqR3JH21lwseK4+BmJmV6xggkjJgJbAYmAsskzS3pc5s4Dpg\nYUScAlyVys8DTgPmA2cCKyQdkTa7OiJOjYh5wCvA8lT+FWBnRJwE3ALc3HJKtwB/N9YL7VW95uXc\nzczKdNMCOQPYEhFbI2IAuA9Y0lLncmBlROwEiIjtqXwu8GhENCKiH1gPLEp1dgFIEjAVaH5CLwFW\npccPAJ9PdZD0e8BWYONYL7RXzWm8boGYmY3UTYAcC7xaeL4tlRXNAeZIelzSU5IWpfL1wGJJ0yQd\nDZwNHN/cSNJdwOvAycCtrceLiAbwNjBD0mHAnwA3jOH63jOPgZiZlesmQFRS1vrneB2YDZwFLANu\nlzQ9ItYCa4AngHuBJ4HG8E4iLgOOAZ4HmuMjVce7AbglIt5pe7LSFZLWSVq3Y8eODpfWWV/Ns7DM\nzMp0EyDbKLQagOOA10rqPBQRgxHxErCZPFCIiJsiYn5EnEMeDi8WN4yIIeB+4ILW40mqA0cCb5KP\nofyVpJfJx1j+VNJyWkTEbRGxICIWzJw5s4vLay9zF5aZWaluAuQZYLakEyVNApYCq1vqPEjePUXq\nqpoDbJWUSZqRyucB84C1yp2UygWcD7yQ9rUauDQ9vhB4JHL/IiJmRcQs4JvAf4mIb/d01WPgxRTN\nzMrVO1WIiEb6S/9hIAPujIiNkm4E1kXE6vTauZI2AUPAioh4Q9IU4LE0Br4LuCTtrwasSjOyRD5W\ncmU65B3A3ZK2kLc8lo7nBY9V3WMgZmalOgYIQESsIR/LKJZdX3gcwDXpp1hnD/lMrNb97QMWVhxr\nD/DFDufzn7s57/FQ9xiImVkp34negcdAzMzKOUA6aHZhDboLy8xsBAdIB80A2fDq2ww0HCJmZk0O\nkA7qWY0rz/pNfrjxdb7435/k1Td3H+xTMjP7QOhqEP1Q9yeLTmbesUfytb/ZwBe+9Rhf+OTH+ciU\nOodPqXP45DrTJtU5bHLGYZPqTEv/HjY5y8tTWXNZeDOzicIB0qXFn/o4pxxzJH/6/Z/zk83beWdv\ng90DQ11v35cpBUrGtMnp32LQDAdO9etT+1qfZ6Qp0mZm7zsHyBicMGMa3/vDM4efD+0Ldg/kQfLO\n3gbvDgzRv3f/890DDfr3prLBIXbvbfDO3iHeHdxf/tpbg+weSOUDeb3ocsKXBNP6Mg6bXOewyXWm\nTRrZCpo2Kct/2gbW/vpTJ2UcNimj7taSmXXBAfIeZDXxkSl9fGRKHx8bp31GBO8ODrF7YIjde1Mw\nFQJn98DQiMDpHxhZ3r93iJ39A2zb+e5wef/expjupJ9Ur/XUQpo2HFqFLr1JecBNrtfcWjKbYBwg\nHzCS0gdxHQ4fv/3ubQzlLaRi4DQDZmB/0PTvHWL3YIPdhcBqvv5m/7sppJqB1X0XXk2UtHZGtpaa\nrahpLeF0+OT9rxe3m9bn1pLZweQAOURMrmdMrmdMnzZ++9y3L/KuuYFGobVUCKKW7r1mSO3vzmvw\nZv8Ar765eziQxtpamlyvVQZPsbU0tU3rqbXrb0qfW0tm3XCAWM9qNXH45LyFwEfGb7/F1lIzaIaf\nd9Fa6t/b4I13BobDbKytJTVbS2Wh1NpVVwyl4bDyTDw7NDhA7APn/Wgt9Q80RnfppTGksrGld/Y2\neGv3AL94a3/5uwNDDAx1f3PppKw2PFGhNWyqWkbFrr0REyXSPqb1ZdRqbi3ZweEAsUPCgWotDTT2\n5d1zpRMdqrru9s/E2z3Q4PVde4brvJtaUWNZem1KX63NeFJh/Kgvq5yFVwwmT3qwbjlAzN6DSfUa\nk+o1jqRv3PYZEewZ3Le/u26gOemhfFbeu4Ojp5H3722w49d7C91/DfYMdt9aqokRraCqGXnFsaXh\nFtNwKBVe68uYNjlvWdrE4QAx+4CRxNTUvTVjHPfbvG+p2HVXHCcqjiPlYbR/zKnYjffaW4UW1cDQ\nmNaIa95QW+yem9o3sntu6qSsdILD/nGnkc+nTfL40sHiADE7RBTvWxpPg0P7CmNIzTAaGT79e4da\nJjWMHGfa8eu9o6aNj+UrFCZltZE30BbGlKYVAmm41VQxzlRsNU3ty8g8vtSWA8TM3pO+rMaRU2sc\nOXV8u/H2pvGl/oH940XvDuyfLt7aatq9d2hEOL070OC1t94tBFler9uVHmDk+NK0vtGrNuzvnhvZ\nXVd2Y23z+dQJNPHBAWJmHziSmNKXMaUv46jDJo3bfpvBVJze3V9oOQ0HVOG14sy95jZv9A+MaE2N\nZZo4MGJsqfXx/htmC69XtKiKAXUw1sZzgJjZIaMYTOM5vrRvX7CnUd5119oqGlmnGFj5xIfdg/tb\nWmOZ+CDB1L79gTLvuOncuuzT43iVozlAzMzeo1qtsAQRk8dtv0P70tp4LeNDzaWIhkNpxNJEedmx\nR00dt/Oo4gAxM/uAyor3L30Aee6bmZn1xAFiZmY9cYCYmVlPugoQSYskbZa0RdK1FXUukrRJ0kZJ\n9xTKb5b0XPq5uFB+h6T1kjZIekDS4al8sqT707GeljQrlZ8j6VlJP0//fu69XLiZmb03HQNEUgas\nBBYDc4Flkua21JkNXAcsjIhTgKtS+XnAacB84ExghaQj0mZXR8SpETEPeAVYnsq/AuyMiJOAW4Cb\nU/mvgPMj4lPApcDdvV2ymZmNh25aIGcAWyJia0QMAPcBS1rqXA6sjIidABGxPZXPBR6NiEZE9APr\ngUWpzi4A5Xe+TAWa94cuAValxw8An5ekiPhZRLyWyjcCUySN33w5MzMbk24C5Fjg1cLzbamsaA4w\nR9Ljkp6StCiVrwcWS5om6WjgbOD45kaS7gJeB04Gbm09XkQ0gLdh1D0/FwA/i4i9XZy/mZkdAN1M\nLi67N751NZk6MBs4CzgOeEzSJyNiraTTgSeAHcCTQGN4JxGXpS6yW4GLgbs6HU/SKeTdWueWnqx0\nBXAFwAknnNDF5ZmZWS+6CZBtFFoN5AHxWkmdpyJiEHhJ0mbyQHkmIm4CbgJIg+svFjeMiCFJ9wMr\nyAOkebxtkurAkcCbafvjgO8DX4qIvy872Yi4Dbgt1d8h6R+6uMYqR5OPvRxKDsVrhkPzug/Fa4ZD\n87rHes3/pJtK3QTIM8BsSScCvwCWAr/fUudBYBnwndRVNQfYmloX0yPiDUnzgHnA2jTu8ZsRsSU9\nPh94Ie1rNfkg+ZPAhcAjERGSpgM/AK6LiMe7ubiImNlNvSqS1kXEgveyjw+bQ/Ga4dC87kPxmuHQ\nvO4Ddc0dAyQiGpKWAw8DGXBnRGyUdCOwLiJWp9fOlbQJGAJWpNCYQt6dBbALuCTtrwasSjOyRD5W\ncmU65B3A3ZK2kLc8lqby5cBJwJ9J+rNUdm5hwN7MzN5HirEsjn+I8V8qh45D8boPxWuGQ/O6D9Q1\n+0709m472CdwEByK1wyH5nUfitcMh+Z1H5BrdgvEzMx64haImZn1xAFSopu1vyYCScdL+omk59Ma\nZn+Uyj8q6UeSXkz/HnWwz3W8Scok/UzS/07PT0xrr72Y1mIbv+9R/YCQND2tO/dCes9/e6K/15Ku\nTv9tPyfpXklTJuJ7LelOSdslPVcoK31vlftW+nzbIOm0Xo/rAGnRzdpfE0gD+OOI+GfAZ4D/kK71\nWuDHETEb+HF6PtH8EfB84fnNwC3pmneSr8k20fw34IcRcTJwKvn1T9j3WtKxwH8EFkTEJ8lnkS5l\nYr7X3yEtE1VQ9d4uJr9Pbzb5Tdd/3etBHSCjdbP214QQEb+MiJ+mx78m/0A5lpHrka0Cfu/gnOGB\nkW5IPQ+4PT0X8DnytddgYl7zEcBnyafJExEDEfEWE/y9Jr9VYWq6KXka8Esm4HsdEf+HdMN1QdV7\nuwT4buSeAqZL+ngvx3WAjNbN2l8TjvJl8z8NPA18LCJ+CXnIAL9x8M7sgPgm8DVgX3o+A3grrb0G\nE/M9/wT5ckJ3pa672yUdxgR+ryPiF8DXyVf7/iX5unrPMvHf66aq93bcPuMcIKN1s/bXhKL8u1j+\nBriquUryRCXpd4HtEfFssbj3/6LxAAABuElEQVSk6kR7z+vkX63w1xHxaaCfCdRdVSb1+S8BTgSO\nAQ4j775pNdHe607G7b93B8ho3az9NWFI6iMPj/8REX+biv+x2aRN/06ku/0XAv9G0svk3ZOfI2+R\nTE/dHDAx3/NtwLaIeDo9f4A8UCbye/2vgJciYkdap+9vgX/OxH+vm6re23H7jHOAjDa89leanbGU\nfH2uCSf1/d8BPB8R3yi81FyPjPTvQ+/3uR0oEXFdRBwXEbPI39tHIuIPgJ+Qr70GE+yaASLideBV\nSf80FX0e2MQEfq/Ju64+o/zrJMT+a57Q73VB1Xu7GvhSmo31GeDtZlfXWPlGwhKSvkD+V2lz7a+b\nDvIpHRCSfgd4DPg5+8cD/pR8HOR/AieQ/0/4xYhoHaD70JN0FvDViPhdSZ8gb5F8FPgZ+bptE+r7\nZiTNJ584MAnYClxG/kfkhH2vJd1A/lURDfL39Q/J+/sn1Hst6V7yr9M4GvhH4M/JF7kd9d6mMP02\n+ayt3cBlEbGup+M6QMzMrBfuwjIzs544QMzMrCcOEDMz64kDxMzMeuIAMTOznjhAzMysJw4QMzPr\niQPEzMx68v8BRY+HRDYNub8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dab0ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w,E = fit(X,y,0.000001,100)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5362082994304312\n"
     ]
    }
   ],
   "source": [
    "def predict(w, X):\n",
    "    pred = 1/(1+np.exp(X@-w.T))\n",
    "    for n in range(0, len(pred)):\n",
    "        if(pred[n] < 0.5):\n",
    "            pred[n] = -1\n",
    "        else:\n",
    "            pred[n] = 1\n",
    "    return pred\n",
    "#TODO\n",
    "def accuracy(y,y_pred):\n",
    "    acc = 0\n",
    "    for n in range(0,len(y)):\n",
    "        if(y[n] + y_pred[n] == 0):\n",
    "            acc = acc +1\n",
    "    return 1-((acc)/len(y))\n",
    "\n",
    "y_pred = predict(w,X)\n",
    "#print(y_pred)\n",
    "print( accuracy(y,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.08325453e-05  -8.88955529e-06   1.78910091e-05   8.40346325e-06\n",
      "   -1.62501792e-05  -9.89541606e-06   5.46296989e-07   9.70971773e-06\n",
      "   -9.28664803e-08   4.22752910e-07  -2.05697264e-06   1.36739573e-05\n",
      "    6.24689235e-06   6.37578619e-07   6.95196134e-07   8.93221332e-06\n",
      "    1.45324629e-06   2.51477008e-07   2.86349962e-06   3.40186436e-06\n",
      "    7.88880646e-07   2.29459168e-07   5.42622416e-06  -4.87089378e-07\n",
      "    1.16828520e-05   3.80841435e-06   1.86262818e-06   2.49323726e-06\n",
      "    2.33279504e-05  -1.12822588e-05   2.16130657e-06   3.32305942e-05\n",
      "    2.51110591e-05   9.89904583e-06  -1.82532787e-07   9.88554025e-06\n",
      "    1.02341673e-05   1.30878335e-05   7.35069682e-07   6.79732738e-06\n",
      "    9.91368923e-06   8.32979791e-06   1.00760160e-06   1.13824448e-05\n",
      "    1.01802928e-05   1.10246671e-05   1.02750147e-05   1.17727426e-05\n",
      "    1.10120940e-05   4.68778901e-06   2.64818742e-05  -1.68727296e-05\n",
      "    7.49140747e-06   1.00590772e-05   9.23912993e-06]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e1f4c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5298372513562387\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(X,y,pct=80)\n",
    "w,E = fit(X_train,y_train,0.000001,300)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()\n",
    "y_pred = predict(w,X_test)\n",
    "print( accuracy(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "predictions.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        ..., \n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression binary classifier in Tensorflow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_tensorFlow(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "#each one of these data sets needs to be normalized \n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "# #np.savetxt('test.csv', data_2000_2001, fmt='%s', delimiter=',')\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)\n",
    "\n",
    "X,y = prepare_tensorFlow(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (17698, 55) (17698, 1)\n",
      "Test dataset shape (4424, 55) (4424, 1)\n",
      "Y = [[0]\n",
      " [0]\n",
      " [0]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable(tf.zeros((n_x, 1)))\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 0.693131\n",
      "1 449.845\n",
      "2 6335.58\n",
      "3 1351.7\n",
      "4 5300.16\n",
      "5 2251.25\n",
      "6 4264.79\n",
      "7 3150.75\n",
      "8 3229.43\n",
      "9 4050.26\n",
      "10 2194.06\n",
      "11 4949.74\n",
      "12 1158.71\n",
      "13 5848.98\n",
      "14 124.151\n",
      "15 6736.46\n",
      "16 779.91\n",
      "17 5957.29\n",
      "18 1680.18\n",
      "19 4921.9\n",
      "20 2579.71\n",
      "21 3886.54\n",
      "22 3479.21\n",
      "23 2851.17\n",
      "24 4378.72\n",
      "25 1815.8\n",
      "26 5278.2\n",
      "27 780.487\n",
      "28 6176.93\n",
      "29 220.693\n",
      "30 6592.45\n",
      "31 1128.21\n",
      "32 5556.94\n",
      "33 2027.84\n",
      "34 4521.57\n",
      "35 2927.34\n",
      "36 3486.2\n",
      "37 3826.85\n",
      "38 2450.83\n",
      "39 4726.35\n",
      "40 1415.48\n",
      "41 5625.71\n",
      "42 380.495\n",
      "43 6519.53\n",
      "44 563.025\n",
      "45 6205.46\n",
      "46 1464.25\n",
      "47 5170.05\n",
      "48 2363.8\n",
      "49 4134.68\n",
      "50 3263.31\n",
      "51 3099.31\n",
      "52 4162.8\n",
      "53 2063.95\n",
      "54 5062.29\n",
      "55 1028.62\n",
      "56 5961.26\n",
      "57 5.81742\n",
      "58 4985.04\n",
      "59 2524.47\n",
      "60 3949.67\n",
      "61 3423.97\n",
      "62 2914.3\n",
      "63 4323.48\n",
      "64 1878.94\n",
      "65 5222.92\n",
      "66 843.712\n",
      "67 6120.65\n",
      "68 164.364\n",
      "69 6658.65\n",
      "70 1070.31\n",
      "71 5623.15\n",
      "72 1969.93\n",
      "73 4587.78\n",
      "74 2869.44\n",
      "75 3552.41\n",
      "76 3768.94\n",
      "77 2517.04\n",
      "78 4668.42\n",
      "79 1481.71\n",
      "80 5567.61\n",
      "81 447.104\n",
      "82 6458.66\n",
      "83 502.166\n",
      "84 6274.84\n",
      "85 1403.61\n",
      "86 5239.4\n",
      "87 2303.16\n",
      "88 4204.04\n",
      "89 3202.67\n",
      "90 3168.67\n",
      "91 4102.17\n",
      "92 2133.31\n",
      "93 5001.62\n",
      "94 1098.04\n",
      "95 5900.06\n",
      "96 64.9244\n",
      "97 6782.6\n",
      "98 826.074\n",
      "99 5902.85\n",
      "100 1726.63\n",
      "101 4867.43\n",
      "102 2626.17\n",
      "103 3832.06\n",
      "104 3525.68\n",
      "105 2796.69\n",
      "106 4425.18\n",
      "107 1761.34\n",
      "108 5324.59\n",
      "109 726.182\n",
      "110 6221.61\n",
      "111 265.459\n",
      "112 6540.24\n",
      "113 1172.73\n",
      "114 5504.57\n",
      "115 2072.49\n",
      "116 4469.18\n",
      "117 2972.0\n",
      "118 3433.82\n",
      "119 3871.51\n",
      "120 2398.45\n",
      "121 4770.98\n",
      "122 1363.14\n",
      "123 5669.96\n",
      "124 328.893\n",
      "125 6559.93\n",
      "126 603.511\n",
      "127 6156.45\n",
      "128 1506.0\n",
      "129 5120.93\n",
      "130 2405.63\n",
      "131 4085.56\n",
      "132 3305.14\n",
      "133 3050.19\n",
      "134 4204.64\n",
      "135 2014.83\n",
      "136 5104.06\n",
      "137 979.639\n",
      "138 6001.86\n",
      "139 46.1451\n",
      "140 6787.14\n",
      "141 957.934\n",
      "142 5751.01\n",
      "143 1858.08\n",
      "144 4715.6\n",
      "145 2757.61\n",
      "146 3680.24\n",
      "147 3657.11\n",
      "148 2644.88\n",
      "149 4556.58\n",
      "150 1609.57\n",
      "151 5455.56\n",
      "152 575.232\n",
      "153 6347.67\n",
      "154 391.391\n",
      "155 6397.9\n",
      "156 1295.93\n",
      "157 5362.23\n",
      "158 2195.69\n",
      "159 4326.85\n",
      "160 3095.2\n",
      "161 3291.48\n",
      "162 3994.7\n",
      "163 2256.13\n",
      "164 4894.09\n",
      "165 1220.99\n",
      "166 5791.86\n",
      "167 188.599\n",
      "168 6677.1\n",
      "169 720.683\n",
      "170 6021.35\n",
      "171 1622.92\n",
      "172 4985.78\n",
      "173 2522.6\n",
      "174 3950.39\n",
      "175 3422.11\n",
      "176 2915.03\n",
      "177 4321.6\n",
      "178 1879.7\n",
      "179 5220.83\n",
      "180 844.841\n",
      "181 6116.67\n",
      "182 160.899\n",
      "183 6656.91\n",
      "184 1070.65\n",
      "185 5620.52\n",
      "186 1971.0\n",
      "187 4585.08\n",
      "188 2870.56\n",
      "189 3549.71\n",
      "190 3770.06\n",
      "191 2514.36\n",
      "192 4669.46\n",
      "193 1479.16\n",
      "194 5567.8\n",
      "195 445.813\n",
      "196 6457.36\n",
      "197 501.15\n",
      "198 6270.96\n",
      "199 1405.78\n",
      "200 5235.09\n",
      "201 2305.7\n",
      "202 4199.68\n",
      "203 3205.24\n",
      "204 3164.31\n",
      "205 4104.72\n",
      "206 2128.99\n",
      "207 5003.96\n",
      "208 1094.11\n",
      "209 5900.52\n",
      "210 63.3482\n",
      "211 6784.48\n",
      "212 828.135\n",
      "213 5896.62\n",
      "214 1730.85\n",
      "215 4860.88\n",
      "216 2630.66\n",
      "217 3825.49\n",
      "218 3530.18\n",
      "219 2790.12\n",
      "220 4429.64\n",
      "221 1754.84\n",
      "222 5328.6\n",
      "223 720.434\n",
      "224 6222.94\n",
      "225 267.269\n",
      "226 6534.32\n",
      "227 1176.75\n",
      "228 5497.46\n",
      "229 2077.47\n",
      "230 4461.95\n",
      "231 2977.09\n",
      "232 3426.57\n",
      "233 3876.59\n",
      "234 2391.24\n",
      "235 4775.9\n",
      "236 1356.21\n",
      "237 5673.51\n",
      "238 323.861\n",
      "239 6561.88\n",
      "240 605.776\n",
      "241 6149.46\n",
      "242 1510.93\n",
      "243 5113.26\n",
      "244 2411.11\n",
      "245 4077.82\n",
      "246 3310.68\n",
      "247 3042.44\n",
      "248 4210.14\n",
      "249 2007.16\n",
      "250 5109.15\n",
      "251 972.626\n",
      "252 6004.66\n",
      "253 49.4651\n",
      "254 6781.65\n",
      "255 961.661\n",
      "256 5743.68\n",
      "257 1863.28\n",
      "258 4708.02\n",
      "259 2763.02\n",
      "260 3672.63\n",
      "261 3662.53\n",
      "262 2637.3\n",
      "263 4561.81\n",
      "264 1602.31\n",
      "265 5459.56\n",
      "266 569.676\n",
      "267 6349.63\n",
      "268 393.76\n",
      "269 6391.2\n",
      "270 1300.66\n",
      "271 5354.5\n",
      "272 2201.25\n",
      "273 4318.97\n",
      "274 3100.87\n",
      "275 3283.6\n",
      "276 4000.32\n",
      "277 2248.35\n",
      "278 4899.26\n",
      "279 1213.89\n",
      "280 5794.98\n",
      "281 184.113\n",
      "282 6680.18\n",
      "283 724.06\n",
      "284 6013.67\n",
      "285 1628.48\n",
      "286 4977.39\n",
      "287 2528.73\n",
      "288 3941.91\n",
      "289 3428.31\n",
      "290 2906.56\n",
      "291 4327.69\n",
      "292 1871.42\n",
      "293 5226.15\n",
      "294 837.709\n",
      "295 6119.6\n",
      "296 164.329\n",
      "297 6650.76\n",
      "298 1074.98\n",
      "299 5612.65\n",
      "300 1976.71\n",
      "301 4576.88\n",
      "302 2876.53\n",
      "303 3541.47\n",
      "304 3776.01\n",
      "305 2506.2\n",
      "306 4675.07\n",
      "307 1471.56\n",
      "308 5571.77\n",
      "309 440.362\n",
      "310 6460.07\n",
      "311 504.241\n",
      "312 6263.93\n",
      "313 1410.83\n",
      "314 5226.94\n",
      "315 2311.65\n",
      "316 4191.34\n",
      "317 3211.33\n",
      "318 3155.97\n",
      "319 4110.71\n",
      "320 2120.84\n",
      "321 5009.27\n",
      "322 1086.95\n",
      "323 5903.74\n",
      "324 58.894\n",
      "325 6681.78\n",
      "326 725.835\n",
      "327 6010.04\n",
      "328 1631.28\n",
      "329 4973.26\n",
      "330 2531.92\n",
      "331 3937.66\n",
      "332 3431.58\n",
      "333 2902.31\n",
      "334 4330.9\n",
      "335 1867.28\n",
      "336 5229.08\n",
      "337 833.952\n",
      "338 6122.23\n",
      "339 167.221\n",
      "340 6646.4\n",
      "341 1078.46\n",
      "342 5607.41\n",
      "343 1980.87\n",
      "344 4571.39\n",
      "345 2880.89\n",
      "346 3535.94\n",
      "347 3780.38\n",
      "348 2500.72\n",
      "349 4679.28\n",
      "350 1466.32\n",
      "351 5575.54\n",
      "352 435.675\n",
      "353 6463.85\n",
      "354 508.22\n",
      "355 6258.13\n",
      "356 1415.53\n",
      "357 5220.57\n",
      "358 2316.8\n",
      "359 4184.8\n",
      "360 3216.6\n",
      "361 3149.42\n",
      "362 4115.93\n",
      "363 2114.4\n",
      "364 5014.19\n",
      "365 1080.92\n",
      "366 5908.19\n",
      "367 53.8331\n",
      "368 6440.63\n",
      "369 485.185\n",
      "370 6283.43\n",
      "371 1393.49\n",
      "372 5245.35\n",
      "373 2295.16\n",
      "374 4209.45\n",
      "375 3195.06\n",
      "376 3174.04\n",
      "377 4094.42\n",
      "378 2139.0\n",
      "379 4992.77\n",
      "380 1105.41\n",
      "381 5887.13\n",
      "382 77.3277\n",
      "383 6737.03\n",
      "384 781.283\n",
      "385 5944.88\n",
      "386 1687.43\n",
      "387 4907.49\n",
      "388 2588.54\n",
      "389 3871.71\n",
      "390 3488.33\n",
      "391 2836.36\n",
      "392 4387.53\n",
      "393 1801.55\n",
      "394 5285.18\n",
      "395 768.936\n",
      "396 6177.6\n",
      "397 222.832\n",
      "398 6581.76\n",
      "399 1134.23\n",
      "400 5541.9\n",
      "401 2037.3\n",
      "402 4505.52\n",
      "403 2937.59\n",
      "404 3469.99\n",
      "405 3837.08\n",
      "406 2434.88\n",
      "407 4735.69\n",
      "408 1400.9\n",
      "409 5631.24\n",
      "410 371.165\n",
      "411 6519.21\n",
      "412 563.785\n",
      "413 6193.1\n",
      "414 1471.6\n",
      "415 5154.86\n",
      "416 2373.39\n",
      "417 4118.84\n",
      "418 3273.36\n",
      "419 3083.45\n",
      "420 4172.6\n",
      "421 2048.62\n",
      "422 5070.41\n",
      "423 1015.77\n",
      "424 5963.67\n",
      "425 25.3866\n",
      "426 1078.44\n",
      "427 5909.89\n",
      "428 51.8674\n",
      "429 6229.49\n",
      "430 274.785\n",
      "431 6521.97\n",
      "432 1186.07\n",
      "433 5481.86\n",
      "434 2089.33\n",
      "435 4445.35\n",
      "436 2989.72\n",
      "437 3409.79\n",
      "438 3889.19\n",
      "439 2374.74\n",
      "440 4787.65\n",
      "441 1340.98\n",
      "442 5682.83\n",
      "443 311.719\n",
      "444 6570.55\n",
      "445 615.193\n",
      "446 6133.72\n",
      "447 1523.09\n",
      "448 5095.24\n",
      "449 2425.06\n",
      "450 4059.13\n",
      "451 3325.08\n",
      "452 3023.73\n",
      "453 4224.28\n",
      "454 1989.0\n",
      "455 5121.86\n",
      "456 956.465\n",
      "457 6014.77\n",
      "458 60.5075\n",
      "459 6713.21\n",
      "460 1019.78\n",
      "461 5672.23\n",
      "462 1923.72\n",
      "463 4635.39\n",
      "464 2824.35\n",
      "465 3599.76\n",
      "466 3723.86\n",
      "467 2564.74\n",
      "468 4622.26\n",
      "469 1531.07\n",
      "470 5517.51\n",
      "471 501.681\n",
      "472 6405.81\n",
      "473 450.648\n",
      "474 6322.08\n",
      "475 1359.25\n",
      "476 5283.04\n",
      "477 2261.66\n",
      "478 4246.75\n",
      "479 3161.8\n",
      "480 3211.33\n",
      "481 4060.98\n",
      "482 2176.68\n",
      "483 4958.46\n",
      "484 1144.27\n",
      "485 5851.47\n",
      "486 117.843\n",
      "487 6736.51\n",
      "488 780.995\n",
      "489 5943.97\n",
      "490 1687.54\n",
      "491 4905.84\n",
      "492 2589.2\n",
      "493 3869.79\n",
      "494 3489.09\n",
      "495 2834.53\n",
      "496 4387.96\n",
      "497 1800.3\n",
      "498 5284.49\n",
      "499 769.181\n",
      "500 6175.52\n",
      "501 221.028\n",
      "502 6583.41\n",
      "503 1132.19\n",
      "504 5542.59\n",
      "505 2035.98\n",
      "506 4505.67\n",
      "507 2936.64\n",
      "508 3470.04\n",
      "509 3836.05\n",
      "510 2435.21\n",
      "511 4733.99\n",
      "512 1402.18\n",
      "513 5628.28\n",
      "514 374.053\n",
      "515 6515.53\n",
      "516 560.389\n",
      "517 6195.82\n",
      "518 1468.6\n",
      "519 5156.67\n",
      "520 2371.07\n",
      "521 4120.27\n",
      "522 3271.25\n",
      "523 3084.89\n",
      "524 4170.27\n",
      "525 2050.52\n",
      "526 5067.2\n",
      "527 1018.87\n",
      "528 5959.32\n",
      "529 28.5647\n",
      "530 313.407\n",
      "531 6568.12\n",
      "532 612.998\n",
      "533 6135.31\n",
      "534 1521.18\n",
      "535 5096.1\n",
      "536 2423.69\n",
      "537 4059.67\n",
      "538 3323.89\n",
      "539 3024.28\n",
      "540 4222.89\n",
      "541 1989.95\n",
      "542 5119.72\n",
      "543 958.43\n",
      "544 6011.71\n",
      "545 57.9746\n",
      "546 6446.19\n",
      "547 1251.13\n",
      "548 5405.84\n",
      "549 2154.52\n",
      "550 4369.02\n",
      "551 3055.05\n",
      "552 3333.48\n",
      "553 3954.26\n",
      "554 2298.94\n",
      "555 4851.58\n",
      "556 1266.77\n",
      "557 5744.72\n",
      "558 240.144\n",
      "559 6630.68\n",
      "560 675.489\n",
      "561 6063.79\n",
      "562 1583.07\n",
      "563 5024.74\n",
      "564 2485.44\n",
      "565 3988.35\n",
      "566 3385.55\n",
      "567 2953.06\n",
      "568 4284.33\n",
      "569 1919.05\n",
      "570 5180.62\n",
      "571 888.26\n",
      "572 6071.76\n",
      "573 117.653\n",
      "574 6701.35\n",
      "575 1029.44\n",
      "576 5659.44\n",
      "577 1934.04\n",
      "578 4621.93\n",
      "579 2835.12\n",
      "580 3586.16\n",
      "581 3734.56\n",
      "582 2551.43\n",
      "583 4632.27\n",
      "584 1518.8\n",
      "585 5526.31\n",
      "586 490.97\n",
      "587 6413.86\n",
      "588 459.026\n",
      "589 6311.31\n",
      "590 1367.92\n",
      "591 5271.32\n",
      "592 2271.01\n",
      "593 4234.55\n",
      "594 3171.42\n",
      "595 3199.13\n",
      "596 4070.35\n",
      "597 2165.0\n",
      "598 4966.94\n",
      "599 1133.82\n",
      "600 5858.87\n",
      "601 108.768\n",
      "602 6708.22\n",
      "603 753.056\n",
      "604 5974.43\n",
      "605 1660.38\n",
      "606 4935.29\n",
      "607 2562.77\n",
      "608 3898.84\n",
      "609 3462.87\n",
      "610 2863.64\n",
      "611 4361.39\n",
      "612 1830.02\n",
      "613 5257.1\n",
      "614 800.009\n",
      "615 6147.5\n",
      "616 193.43\n",
      "617 6614.23\n",
      "618 1104.82\n",
      "619 5572.24\n",
      "620 2009.45\n",
      "621 4534.6\n",
      "622 2910.59\n",
      "623 3498.82\n",
      "624 3809.95\n",
      "625 2464.28\n",
      "626 4707.28\n",
      "627 1432.16\n",
      "628 5600.74\n",
      "629 405.099\n",
      "630 6487.8\n",
      "631 533.035\n",
      "632 6225.91\n",
      "633 1441.81\n",
      "634 5185.73\n",
      "635 2345.01\n",
      "636 4148.83\n",
      "637 3245.49\n",
      "638 3113.44\n",
      "639 4144.26\n",
      "640 2079.57\n",
      "641 5040.46\n",
      "642 1048.94\n",
      "643 5931.88\n",
      "644 39.6055\n",
      "645 2889.36\n",
      "646 3522.9\n",
      "647 3789.09\n",
      "648 2488.05\n",
      "649 4686.9\n",
      "650 1455.35\n",
      "651 5581.26\n",
      "652 427.133\n",
      "653 6469.71\n",
      "654 515.311\n",
      "655 6245.08\n",
      "656 1425.24\n",
      "657 5203.96\n",
      "658 2329.16\n",
      "659 4166.61\n",
      "660 3230.01\n",
      "661 3131.0\n",
      "662 4129.04\n",
      "663 2096.88\n",
      "664 5025.65\n",
      "665 1065.76\n",
      "666 5917.8\n",
      "667 48.0305\n",
      "668 4476.69\n",
      "669 1696.93\n",
      "670 5372.52\n",
      "671 666.801\n",
      "672 6263.35\n",
      "673 309.644\n",
      "674 6479.67\n",
      "675 1221.66\n",
      "676 5436.79\n",
      "677 2126.95\n",
      "678 4398.6\n",
      "679 3028.5\n",
      "680 3362.59\n",
      "681 3928.0\n",
      "682 2328.01\n",
      "683 4825.34\n",
      "684 1295.96\n",
      "685 5718.86\n",
      "686 268.83\n",
      "687 6606.3\n",
      "688 651.8\n",
      "689 6088.46\n",
      "690 1561.1\n",
      "691 5047.63\n",
      "692 2464.77\n",
      "693 4010.36\n",
      "694 3365.5\n",
      "695 2974.87\n",
      "696 4264.3\n",
      "697 1941.06\n",
      "698 5160.41\n",
      "699 910.588\n",
      "700 6051.84\n",
      "701 98.473\n",
      "702 6721.85\n",
      "703 1011.19\n",
      "704 5678.13\n",
      "705 1917.11\n",
      "706 4639.49\n",
      "707 2818.99\n",
      "708 3603.3\n",
      "709 3718.64\n",
      "710 2568.66\n",
      "711 4616.09\n",
      "712 1536.5\n",
      "713 5509.9\n",
      "714 508.996\n",
      "715 6397.94\n",
      "716 443.658\n",
      "717 6327.22\n",
      "718 1353.53\n",
      "719 5285.82\n",
      "720 2257.62\n",
      "721 4248.28\n",
      "722 3158.55\n",
      "723 3212.7\n",
      "724 4057.41\n",
      "725 2178.9\n",
      "726 4953.61\n",
      "727 1148.33\n",
      "728 5845.31\n",
      "729 123.571\n",
      "730 6695.23\n",
      "731 740.519\n",
      "732 5987.1\n",
      "733 1648.73\n",
      "734 4946.83\n",
      "735 2551.91\n",
      "736 3909.82\n",
      "737 3452.34\n",
      "738 2874.61\n",
      "739 4350.62\n",
      "740 1841.48\n",
      "741 5245.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742 812.191\n",
      "743 6136.05\n",
      "744 182.469\n",
      "745 6625.79\n",
      "746 1094.24\n",
      "747 5582.63\n",
      "748 1999.69\n",
      "749 4544.18\n",
      "750 2901.36\n",
      "751 3508.14\n",
      "752 3800.73\n",
      "753 2473.87\n",
      "754 4697.63\n",
      "755 1442.44\n",
      "756 5590.58\n",
      "757 416.055\n",
      "758 6477.71\n",
      "759 523.367\n",
      "760 6235.7\n",
      "761 1432.69\n",
      "762 5194.49\n",
      "763 2336.6\n",
      "764 4157.01\n",
      "765 3237.42\n",
      "766 3121.56\n",
      "767 4135.97\n",
      "768 2088.17\n",
      "769 5031.64\n",
      "770 1058.32\n",
      "771 5922.64\n",
      "772 47.8886\n",
      "773 3469.73\n",
      "774 2854.27\n",
      "775 4368.13\n",
      "776 1821.06\n",
      "777 5263.51\n",
      "778 791.584\n",
      "779 6154.05\n",
      "780 200.722\n",
      "781 6604.28\n",
      "782 1112.88\n",
      "783 5560.57\n",
      "784 2018.74\n",
      "785 4521.76\n",
      "786 2920.68\n",
      "787 3485.54\n",
      "788 3820.18\n",
      "789 2451.22\n",
      "790 4717.14\n",
      "791 1419.75\n",
      "792 5610.21\n",
      "793 393.22\n",
      "794 6497.63\n",
      "795 543.465\n",
      "796 6212.09\n",
      "797 1453.11\n",
      "798 5170.48\n",
      "799 2357.31\n",
      "800 4132.77\n",
      "801 3258.29\n",
      "802 3097.24\n",
      "803 4156.89\n",
      "804 2063.86\n",
      "805 5052.56\n",
      "806 1034.03\n",
      "807 5943.64\n",
      "808 39.3889\n",
      "809 1514.92\n",
      "810 5099.4\n",
      "811 2419.1\n",
      "812 4061.67\n",
      "813 3320.08\n",
      "814 3026.15\n",
      "815 4218.66\n",
      "816 1992.81\n",
      "817 5114.26\n",
      "818 963.06\n",
      "819 6005.27\n",
      "820 56.1423\n",
      "821 5070.08\n",
      "822 2444.52\n",
      "823 4032.43\n",
      "824 3345.41\n",
      "825 2996.99\n",
      "826 4243.86\n",
      "827 1963.81\n",
      "828 5139.28\n",
      "829 934.304\n",
      "830 6030.03\n",
      "831 77.2849\n",
      "832 6463.41\n",
      "833 1234.91\n",
      "834 5420.65\n",
      "835 2139.98\n",
      "836 4382.29\n",
      "837 3041.5\n",
      "838 3346.41\n",
      "839 3940.49\n",
      "840 2312.68\n",
      "841 4836.66\n",
      "842 1282.23\n",
      "843 5728.6\n",
      "844 257.159\n",
      "845 6614.77\n",
      "846 660.45\n",
      "847 6078.04\n",
      "848 1569.3\n",
      "849 5036.9\n",
      "850 2473.08\n",
      "851 3999.41\n",
      "852 3373.79\n",
      "853 2964.15\n",
      "854 4271.95\n",
      "855 1931.35\n",
      "856 5166.91\n",
      "857 902.433\n",
      "858 6057.11\n",
      "859 103.95\n",
      "860 6715.44\n",
      "861 1016.0\n",
      "862 5671.43\n",
      "863 1922.03\n",
      "864 4632.34\n",
      "865 2824.12\n",
      "866 3596.05\n",
      "867 3723.54\n",
      "868 2561.97\n",
      "869 4620.19\n",
      "870 1530.96\n",
      "871 5512.9\n",
      "872 504.912\n",
      "873 6400.16\n",
      "874 446.175\n",
      "875 6323.61\n",
      "876 1355.89\n",
      "877 5281.66\n",
      "878 2260.29\n",
      "879 4243.71\n",
      "880 3161.37\n",
      "881 3208.22\n",
      "882 4059.81\n",
      "883 2175.16\n",
      "884 4955.15\n",
      "885 1145.78\n",
      "886 5845.97\n",
      "887 122.342\n",
      "888 6648.37\n",
      "889 694.055\n",
      "890 6039.29\n",
      "891 1602.65\n",
      "892 4998.19\n",
      "893 2506.36\n",
      "894 3960.72\n",
      "895 3406.96\n",
      "896 2925.61\n",
      "897 4304.88\n",
      "898 1893.16\n",
      "899 5199.45\n",
      "900 864.765\n",
      "901 6089.24\n",
      "902 136.106\n",
      "903 6678.4\n",
      "904 1047.89\n",
      "905 5634.44\n",
      "906 1953.84\n",
      "907 4595.32\n",
      "908 2855.92\n",
      "909 3559.05\n",
      "910 3755.21\n",
      "911 2525.18\n",
      "912 4651.59\n",
      "913 1494.55\n",
      "914 5543.95\n",
      "915 468.955\n",
      "916 6430.94\n",
      "917 477.008\n",
      "918 6287.94\n",
      "919 1386.6\n",
      "920 5245.93\n",
      "921 2291.01\n",
      "922 4207.93\n",
      "923 3192.07\n",
      "924 3172.49\n",
      "925 4090.37\n",
      "926 2139.67\n",
      "927 4985.45\n",
      "928 1110.65\n",
      "929 5876.0\n",
      "930 89.5035\n",
      "931 6006.92\n",
      "932 59.7202\n",
      "933 4852.66\n",
      "934 2632.53\n",
      "935 3815.34\n",
      "936 3532.92\n",
      "937 2780.47\n",
      "938 4430.5\n",
      "939 1748.45\n",
      "940 5324.55\n",
      "941 720.727\n",
      "942 6213.75\n",
      "943 260.549\n",
      "944 6535.39\n",
      "945 1171.9\n",
      "946 5491.69\n",
      "947 2077.63\n",
      "948 4452.67\n",
      "949 2979.57\n",
      "950 3416.52\n",
      "951 3878.66\n",
      "952 2382.92\n",
      "953 4774.71\n",
      "954 1352.74\n",
      "955 5666.61\n",
      "956 327.725\n",
      "957 6553.15\n",
      "958 599.221\n",
      "959 6147.34\n",
      "960 1508.55\n",
      "961 5105.44\n",
      "962 2412.85\n",
      "963 4067.47\n",
      "964 3313.81\n",
      "965 3032.15\n",
      "966 4211.91\n",
      "967 1999.59\n",
      "968 5106.68\n",
      "969 970.999\n",
      "970 5996.87\n",
      "971 53.8685\n",
      "972 4077.9\n",
      "973 3304.83\n",
      "974 3042.41\n",
      "975 4203.12\n",
      "976 2009.66\n",
      "977 5098.13\n",
      "978 980.774\n",
      "979 5988.62\n",
      "980 48.7917\n",
      "981 3328.1\n",
      "982 3955.49\n",
      "983 2294.34\n",
      "984 4851.73\n",
      "985 1263.95\n",
      "986 5743.89\n",
      "987 238.629\n",
      "988 6630.79\n",
      "989 677.008\n",
      "990 6057.51\n",
      "991 1586.63\n",
      "992 5015.28\n",
      "993 2491.19\n",
      "994 3977.09\n",
      "995 3392.33\n",
      "996 2941.62\n",
      "997 4290.55\n",
      "998 1908.96\n",
      "999 5185.44\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(1000):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.564244547406\n",
      "Accuracy on the test set is  0.575045207957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       ..., \n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True]], dtype=bool)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(0, len(A)):\n",
    "    if(A[i] < 0.5):\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        ..., \n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]]),\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  ...]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, 1)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step  (0, 0.69314742)\n",
      "Minibatch accuracy:  0.51\n",
      "Test accuracy:  0.524638336347\n",
      "Minibatch loss at step  (500, 6175.9438)\n",
      "Minibatch accuracy:  0.42\n",
      "Test accuracy:  0.475813743219\n",
      "Minibatch loss at step  (1000, 438.9259)\n",
      "Minibatch accuracy:  0.61\n",
      "Test accuracy:  0.476265822785\n",
      "Minibatch loss at step  (1500, 4737.6914)\n",
      "Minibatch accuracy:  0.47\n",
      "Test accuracy:  0.586347197107\n",
      "Minibatch loss at step  (2000, 13.501985)\n",
      "Minibatch accuracy:  0.43\n",
      "Test accuracy:  0.50452079566\n",
      "Minibatch loss at step  (2500, 16026.245)\n",
      "Minibatch accuracy:  0.53\n",
      "Test accuracy:  0.512432188065\n",
      "Minibatch loss at step  (3000, 9815.2871)\n",
      "Minibatch accuracy:  0.62\n",
      "Test accuracy:  0.486211573237\n",
      "Minibatch loss at step  (3500, 7622.7773)\n",
      "Minibatch accuracy:  0.58\n",
      "Test accuracy:  0.518309222423\n",
      "Minibatch loss at step  (4000, 755.77429)\n",
      "Minibatch accuracy:  0.6\n",
      "Test accuracy:  0.570976491863\n",
      "Minibatch loss at step  (4500, 8056.3623)\n",
      "Minibatch accuracy:  0.52\n",
      "Test accuracy:  0.598101265823\n",
      "Minibatch loss at step  (5000, 1706.1162)\n",
      "Minibatch accuracy:  0.64\n",
      "Test accuracy:  0.570072332731\n",
      "Minibatch loss at step  (5500, 1137.84)\n",
      "Minibatch accuracy:  0.7\n",
      "Test accuracy:  0.521699819168\n",
      "Minibatch loss at step  (6000, 452.31122)\n",
      "Minibatch accuracy:  0.65\n",
      "Test accuracy:  0.596066907776\n",
      "Minibatch loss at step  (6500, 13528.487)\n",
      "Minibatch accuracy:  0.56\n",
      "Test accuracy:  0.558092224231\n",
      "Minibatch loss at step  (7000, 3443.47)\n",
      "Minibatch accuracy:  0.53\n",
      "Test accuracy:  0.577757685353\n",
      "Minibatch loss at step  (7500, 7084.1963)\n",
      "Minibatch accuracy:  0.52\n",
      "Test accuracy:  0.544755877034\n",
      "Minibatch loss at step  (8000, 3955.7712)\n",
      "Minibatch accuracy:  0.58\n",
      "Test accuracy:  0.582956600362\n",
      "Minibatch loss at step  (8500, 8031.4248)\n",
      "Minibatch accuracy:  0.47\n",
      "Test accuracy:  0.572106690778\n",
      "Minibatch loss at step  (9000, 3642.7913)\n",
      "Minibatch accuracy:  0.45\n",
      "Test accuracy:  0.476491862568\n",
      "Minibatch loss at step  (9500, 2890.8735)\n",
      "Minibatch accuracy:  0.57\n",
      "Test accuracy:  0.546338155515\n",
      "Minibatch loss at step  (10000, 7428.6851)\n",
      "Minibatch accuracy:  0.55\n",
      "Test accuracy:  0.556962025316\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "batch_size = 100\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2) )\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 79.6169\n",
      "50 0.679001\n",
      "100 0.676395\n",
      "150 0.674188\n",
      "200 0.672583\n",
      "250 0.671542\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(300):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.534297660753\n",
      "Accuracy on the test set is  0.552667269439\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A2.eval()\n",
    "A_test = tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in range(0, len(A)):\n",
    "    if(A[i] < 0.5):\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        ..., \n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]]),\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " [0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  ...]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensamble\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensamble_pred = []\n",
    "for i in range(0, len(predictions[0])):\n",
    "    sum = predictions[0][i] + predictions[1][i] + predictions[2][i]  \n",
    "    sum = sum/3\n",
    "    if(sum < 0.5):\n",
    "        ensamble_pred.append(0)\n",
    "    else:\n",
    "        ensamble_pred.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "4424\n"
     ]
    }
   ],
   "source": [
    "print(ensamble_pred)\n",
    "print(len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5027124773960217\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0, len(ensamble_pred)):\n",
    "    test = Y_test\n",
    "    if(ensamble_pred[i] == Y_test[i]):\n",
    "        count += 1\n",
    "accuracy = count/len(ensamble_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with more layers and more hidden nodes\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "\n",
    "num_hidden_nodes = 100\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_hidden_nodes]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "tf_w3 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b3 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "tf_Z3 = tf.matmul(tf_A2, tf_w3) + tf_b3\n",
    "tf_A3 = tf.nn.relu(tf_Z3)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z3) )\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)\n",
    "tf_Z3_test = tf.matmul(tf_A2_test, tf_w3) + tf_b3\n",
    "tf_A3_test = tf.nn.relu(tf_Z3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(500):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A3.eval()\n",
    "A_test = tf_A3_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
