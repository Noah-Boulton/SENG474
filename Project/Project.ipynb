{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--\n",
    "In our ever changing and evolving field of technology one of the foremost topics is data analytics. The growth in the amount of digital data that is being collected across many different fields is massive and . That is, taking data in whatever raw form it exists and using technology to transform it into information that has value and context. In most cases data analysis is performed in order to provide class descriptions of data, highlight behaviors, trends, associations in the data or predictive information that prove useful or even vital to key decision-makers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "--\n",
    "Using the NHL API following the documentation found at https://gitlab.com/dword4/nhlapi/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each player in the specified year range (years must be consecutive) collect all avalible stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_skaters(y1, y2):\n",
    "    team_rosters = requests.get('https://statsapi.web.nhl.com/api/v1/teams?expand=team.roster&season=' + y1 + y2)\n",
    "    team_rosters = team_rosters.json()\n",
    "    players= []\n",
    "    for i in range(0, len(team_rosters['teams'])):\n",
    "        for j in range(0, len(team_rosters['teams'][i]['roster']['roster'])):\n",
    "            player = [team_rosters['teams'][i]['roster']['roster'][j]['person']['id'], \n",
    "                      team_rosters['teams'][i]['roster']['roster'][j]['person']['fullName'],\n",
    "                     team_rosters['teams'][i]['name']]\n",
    "            if (team_rosters['teams'][i]['roster']['roster'][j]['position']['code'] != 'G'):\n",
    "                players.append(player)\n",
    "    players_stats = []\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'fullName', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "    for i in range(0, len(players)): \n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            players_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        players_stats.append(stats_array)\n",
    "        \n",
    "    skaters = []\n",
    "    skaters.append(header)\n",
    "    for i in range(0, len(players)):\n",
    "        skaters.append(players[i] + players_stats[i])\n",
    "    return skaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the skaters data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_skaters_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting skaters data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        skaters = get_csv_skaters(str(i), str(i+1))\n",
    "        np.savetxt('data/skaters_' + str(i) + '_' + str(i+1) + '.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_skaters_data(1917,1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each team in the specified year range (years must be consecutive) collect all avalible stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_team(y1, y2):\n",
    "    teams = requests.get('https://statsapi.web.nhl.com/api/v1/teams?season=' + str(y1) + str(y2))\n",
    "    teams = teams.json()\n",
    "    team_id_name = []\n",
    "    for i in range(0, len(teams['teams'])):\n",
    "        team_arr = [teams['teams'][i]['id'], teams['teams'][i]['name']]\n",
    "        team_id_name.append(team_arr)\n",
    "\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                           + str(team_id_name[0][0])\n",
    "                           + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    \n",
    "    header = ['id', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "    header.append('PDO')\n",
    "    team_stats = []\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                             + str(team_id_name[i][0]) \n",
    "                             + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            team_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        team_stats.append(stats_array)\n",
    "        stats_array.append((stats['shootingPctg']/100) + stats['savePctg'])\n",
    "    \n",
    "    teams_stats_final = []\n",
    "    teams_stats_final.append(header)\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        teams_stats_final.append(team_id_name[i] + team_stats[i]) \n",
    "    return teams_stats_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the team data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting team data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_team(str(i), str(i+1))\n",
    "        np.savetxt('team_data/teams_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_team_data(2000, 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the index of the team stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_stats(id, team_data):\n",
    "    for i in range(0, len(team_data)):\n",
    "        if team_data[i][0] == id:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each game in the specified year range (years must be consecutive) return the winner, away team ID, home team ID, and the away and home team stats for that season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_game(y1, y2):\n",
    "    team_data = get_csv_team(y1, y2)\n",
    "    header = team_data[0][3:]\n",
    "    away_header = []\n",
    "    home_header = []\n",
    "    for head in header:\n",
    "        away_header.append('away_'+head)\n",
    "        home_header.append('home_'+head)\n",
    "    games_data = [['winner', 'awayID', 'homeID'] + away_header + home_header]\n",
    "    games = requests.get('https://statsapi.web.nhl.com/api/v1/schedule?startDate=' \n",
    "                         + str(y1) + '-10-01&endDate=' + str(y2) + '-06-30')\n",
    "    games = games.json()\n",
    "    for date in games['dates']:\n",
    "        for game in date['games']:\n",
    "            away_ID = game['teams']['away']['team']['id']\n",
    "            home_ID = game['teams']['home']['team']['id']\n",
    "            if away_ID > 80 or home_ID > 80:\n",
    "                continue\n",
    "            \n",
    "            away_score = game['teams']['away']['score']\n",
    "            home_score = game['teams']['home']['score']\n",
    "            winner = 0\n",
    "            away_stats = team_data[get_team_stats(away_ID, team_data)][3:]\n",
    "            home_stats = team_data[get_team_stats(home_ID, team_data)][3:]\n",
    "            if home_score > away_score:\n",
    "                winner = 1\n",
    "            games_data.append([winner,\n",
    "                          away_ID, \n",
    "                          home_ID] +\n",
    "                          away_stats +\n",
    "                          home_stats)\n",
    "    return games_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the game data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_game_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting game data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_game(i, i+1)\n",
    "        np.savetxt('game_data/game_data_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_game_data(2000,2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup master csv and normalized master csv file of all the games data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "\n",
    "data = pd.concat(frames)\n",
    "\n",
    "data = data.drop(['away_wins', 'away_losses', 'away_ot',\n",
    "       'away_pts', 'away_ptPctg', 'away_powerPlayGoals',\n",
    "       'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities','away_shotsPerGame', 'away_shotsAllowed',\n",
    "       'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "       'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "       'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "       'away_faceOffWinPercentage',\n",
    "       'home_wins', 'home_losses', 'home_ot',\n",
    "       'home_pts', 'home_ptPctg', 'home_powerPlayGoals',\n",
    "       'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities','home_shotsPerGame', 'home_shotsAllowed',\n",
    "       'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "       'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "       'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "       'home_faceOffWinPercentage'], axis=1)\n",
    "header = ['winner', 'awayID', 'homeID', 'away_goalsPerGame',\n",
    "       'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "       'away_powerPlayPercentage', 'away_penaltyKillPercentage',\n",
    "       'away_shootingPctg', 'away_savePctg', 'away_PDO', 'home_goalsPerGame',\n",
    "       'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "       'home_powerPlayPercentage', 'home_penaltyKillPercentage',\n",
    "       'home_shootingPctg', 'home_savePctg', 'home_PDO']\n",
    "header = ','.join(header)\n",
    "\n",
    "np.savetxt('master.csv', data, fmt='%s', delimiter=',', header = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    max_data = np.max(data, axis=0)\n",
    "    min_data = np.min(data, axis=0)\n",
    "    stats = ['away_wins', 'away_losses', 'away_ot',\n",
    "             'away_pts', 'away_ptPctg', 'away_goalsPerGame',\n",
    "             'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "             'away_powerPlayPercentage', 'away_powerPlayGoals',\n",
    "             'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities',\n",
    "             'away_penaltyKillPercentage', 'away_shotsPerGame', 'away_shotsAllowed',\n",
    "             'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "             'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "             'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "             'away_faceOffWinPercentage', 'away_shootingPctg', 'away_savePctg',\n",
    "             'home_wins', 'home_losses', 'home_ot', 'home_pts', 'home_ptPctg',\n",
    "             'home_goalsPerGame', 'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "             'home_powerPlayPercentage', 'home_powerPlayGoals',\n",
    "             'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities',\n",
    "             'home_penaltyKillPercentage', 'home_shotsPerGame', 'home_shotsAllowed',\n",
    "             'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "             'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "             'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "             'home_faceOffWinPercentage', 'home_shootingPctg', 'home_savePctg']\n",
    "    for stat in stats:\n",
    "        data[stat] = (data[stat] - min_data[stat])/(max_data[stat] - min_data[stat])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "\n",
    "data = pd.concat(frames)\n",
    "\n",
    "data = data.drop(['away_wins', 'away_losses', 'away_ot',\n",
    "       'away_pts', 'away_ptPctg', 'away_powerPlayGoals',\n",
    "       'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities','away_shotsPerGame', 'away_shotsAllowed',\n",
    "       'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "       'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "       'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "       'away_faceOffWinPercentage',\n",
    "       'home_wins', 'home_losses', 'home_ot',\n",
    "       'home_pts', 'home_ptPctg', 'home_powerPlayGoals',\n",
    "       'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities','home_shotsPerGame', 'home_shotsAllowed',\n",
    "       'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "       'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "       'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "       'home_faceOffWinPercentage'], axis=1)\n",
    "header = ['winner', 'awayID', 'homeID', 'away_goalsPerGame',\n",
    "       'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "       'away_powerPlayPercentage', 'away_penaltyKillPercentage',\n",
    "       'away_shootingPctg', 'away_savePctg', 'away_PDO', 'home_goalsPerGame',\n",
    "       'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "       'home_powerPlayPercentage', 'home_penaltyKillPercentage',\n",
    "       'home_shootingPctg', 'home_savePctg', 'home_PDO']\n",
    "header = ','.join(header)\n",
    "\n",
    "np.savetxt('master_normalized.csv', data, fmt='%s', delimiter=',', header = header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaylsis\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import classification_report, confusion_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    return X,y\n",
    "\n",
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def accuracy(pred, labels):\n",
    "    count = 0\n",
    "    for i in range(0, len(pred)):\n",
    "        if(pred[i] == labels[i]):\n",
    "            count += 1\n",
    "    return count/len(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM using various kernals\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('master_normalized.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)\n",
    "Y = np.concatenate(Y, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>awayID</th>\n",
       "      <th>homeID</th>\n",
       "      <th>away_goalsPerGame</th>\n",
       "      <th>away_goalsAgainstPerGame</th>\n",
       "      <th>away_evGGARatio</th>\n",
       "      <th>away_powerPlayPercentage</th>\n",
       "      <th>away_penaltyKillPercentage</th>\n",
       "      <th>away_shootingPctg</th>\n",
       "      <th>away_savePctg</th>\n",
       "      <th>away_PDO</th>\n",
       "      <th>home_goalsPerGame</th>\n",
       "      <th>home_goalsAgainstPerGame</th>\n",
       "      <th>home_evGGARatio</th>\n",
       "      <th>home_powerPlayPercentage</th>\n",
       "      <th>home_penaltyKillPercentage</th>\n",
       "      <th>home_shootingPctg</th>\n",
       "      <th>home_savePctg</th>\n",
       "      <th>home_PDO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>0.803099</td>\n",
       "      <td>0.075019</td>\n",
       "      <td>0.789802</td>\n",
       "      <td>0.932331</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.574564</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>0.620057</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.834087</td>\n",
       "      <td>0.197989</td>\n",
       "      <td>0.734781</td>\n",
       "      <td>0.676692</td>\n",
       "      <td>0.841584</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.464170</td>\n",
       "      <td>0.613302</td>\n",
       "      <td>0.795786</td>\n",
       "      <td>0.511278</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>0.330536</td>\n",
       "      <td>0.584687</td>\n",
       "      <td>0.664542</td>\n",
       "      <td>0.248120</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.393802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502732</td>\n",
       "      <td>0.488722</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>0.559070</td>\n",
       "      <td>0.508894</td>\n",
       "      <td>0.767300</td>\n",
       "      <td>0.563910</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.566817</td>\n",
       "      <td>0.216551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.445545</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>0.668819</td>\n",
       "      <td>0.169374</td>\n",
       "      <td>0.849896</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.772277</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.227889</td>\n",
       "      <td>0.490333</td>\n",
       "      <td>0.444459</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.188119</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   winner  awayID  homeID  away_goalsPerGame  away_goalsAgainstPerGame  \\\n",
       "0       0      21      25           0.803099                  0.075019   \n",
       "1       0       9       6           0.834087                  0.197989   \n",
       "2       1      16       7           0.330536                  0.584687   \n",
       "3       1      23       4           0.559070                  0.508894   \n",
       "4       0      17      20           0.668819                  0.169374   \n",
       "\n",
       "   away_evGGARatio  away_powerPlayPercentage  away_penaltyKillPercentage  \\\n",
       "0         0.789802                  0.932331                    0.475248   \n",
       "1         0.734781                  0.676692                    0.841584   \n",
       "2         0.664542                  0.248120                    0.594059   \n",
       "3         0.767300                  0.563910                    0.287129   \n",
       "4         0.849896                  0.939850                    0.772277   \n",
       "\n",
       "   away_shootingPctg  away_savePctg  away_PDO  home_goalsPerGame  \\\n",
       "0           0.885714       0.700000     1.020           0.574564   \n",
       "1           0.828571       0.733333     1.019           0.464170   \n",
       "2           0.428571       0.066667     0.985           0.393802   \n",
       "3           0.457143       0.066667     0.986           0.566817   \n",
       "4           0.371429       0.766667     1.004           0.227889   \n",
       "\n",
       "   home_goalsAgainstPerGame  home_evGGARatio  home_powerPlayPercentage  \\\n",
       "0                  0.027842         0.620057                  0.751880   \n",
       "1                  0.613302         0.795786                  0.511278   \n",
       "2                  0.000000         0.502732                  0.488722   \n",
       "3                  0.216551         1.000000                  0.458647   \n",
       "4                  0.490333         0.444459                  0.398496   \n",
       "\n",
       "   home_penaltyKillPercentage  home_shootingPctg  home_savePctg  home_PDO  \n",
       "0                    0.821782           0.942857       0.733333     1.023  \n",
       "1                    0.475248           0.400000       0.000000     0.982  \n",
       "2                    1.000000           0.485714       1.000000     1.015  \n",
       "3                    0.445545           0.457143       0.566667     1.001  \n",
       "4                    0.188119           0.171429       0.266667     0.982  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_svc = svm.SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_Y_pred = linear_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_acc = accuracy(linear_Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6060126582278481\n"
     ]
    }
   ],
   "source": [
    "print(lin_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 976 1106]\n",
      " [ 637 1705]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.47      0.53      2082\n",
      "          1       0.61      0.73      0.66      2342\n",
      "\n",
      "avg / total       0.61      0.61      0.60      4424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, linear_Y_pred))  \n",
    "print(classification_report(Y_test, linear_Y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial SVC - degree 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_svc = svm.SVC(kernel='poly', degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=2, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_svc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_Y_pred = poly_svc.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_acc = accuracy(poly_Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.594258589511754\n"
     ]
    }
   ],
   "source": [
    "print(poly_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 688 1394]\n",
      " [ 401 1941]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.33      0.43      2082\n",
      "          1       0.58      0.83      0.68      2342\n",
      "\n",
      "avg / total       0.61      0.59      0.57      4424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, poly_Y_pred))  \n",
    "print(classification_report(Y_test, poly_Y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gaussian_svc = svm.SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_svc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gaussian_Y_pred = gaussian_svc.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gaussian_acc = accuracy(gaussian_Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5974231464737794\n"
     ]
    }
   ],
   "source": [
    "print(gaussian_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 688 1394]\n",
      " [ 401 1941]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.33      0.43      2082\n",
      "          1       0.58      0.83      0.68      2342\n",
      "\n",
      "avg / total       0.61      0.59      0.57      4424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, poly_Y_pred))  \n",
    "print(classification_report(Y_test, poly_Y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid_svc = svm.SVC(kernel='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='sigmoid',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_svc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid_Y_pred = sigmoid_svc.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid_acc = accuracy(sigmoid_Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5348101265822784\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1003 1079]\n",
      " [ 979 1363]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.48      0.49      2082\n",
      "          1       0.56      0.58      0.57      2342\n",
      "\n",
      "avg / total       0.53      0.53      0.53      4424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, sigmoid_Y_pred))  \n",
    "print(classification_report(Y_test, sigmoid_Y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensamble Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensamble(pred1, pred2, pred3, pred4):\n",
    "    prediction = []\n",
    "    for i in range(0, len(pred1)):\n",
    "        p = (pred1[i] + pred2[i] + pred3[i] + pred4[i])\n",
    "        p = p/4\n",
    "        if(p < 0.5):\n",
    "            prediction.append(0)\n",
    "        else:\n",
    "            prediction.append(1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensamble_pred = ensamble(linear_Y_pred, poly_Y_pred, gaussian_Y_pred, sigmoid_Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5962929475587704\n"
     ]
    }
   ],
   "source": [
    "ensamble_acc = accuracy(ensamble_pred, Y_test)\n",
    "\n",
    "print(ensamble_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Logistic regression in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('master_normalized.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)\n",
    "Y = np.concatenate(Y, axis=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (17698, 17) (17698, 1)\n",
      "Test dataset shape (4424, 17) (4424, 1)\n"
     ]
    }
   ],
   "source": [
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable(tf.zeros((n_x, 1)))\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 0.693131\n",
      "1 0.763325\n",
      "2 1.8282\n",
      "3 1.05473\n",
      "4 1.5427\n",
      "5 1.27953\n",
      "6 1.28612\n",
      "7 1.4882\n",
      "8 1.04833\n",
      "9 1.68503\n",
      "10 0.823371\n",
      "11 1.86012\n",
      "12 0.690036\n",
      "13 0.995872\n",
      "14 1.72834\n",
      "15 0.768064\n",
      "16 1.85159\n",
      "17 0.688681\n",
      "18 0.934008\n",
      "19 1.77828\n",
      "20 0.714005\n",
      "21 1.60472\n",
      "22 0.897337\n",
      "23 1.80749\n",
      "24 0.691869\n",
      "25 1.14829\n",
      "26 1.41763\n",
      "27 1.37776\n",
      "28 1.15538\n",
      "29 1.58988\n",
      "30 0.91411\n",
      "31 1.78834\n",
      "32 0.70169\n",
      "33 1.46232\n",
      "34 1.05369\n",
      "35 1.67319\n",
      "36 0.814269\n",
      "37 1.85585\n",
      "38 0.688105\n",
      "39 1.15204\n",
      "40 1.58981\n",
      "41 0.906705\n",
      "42 1.79076\n",
      "43 0.695339\n",
      "44 1.36113\n",
      "45 1.16351\n",
      "46 1.57852\n",
      "47 0.916344\n",
      "48 1.7809\n",
      "49 0.699924\n",
      "50 1.46446\n",
      "51 1.04239\n",
      "52 1.67674\n",
      "53 0.801668\n",
      "54 1.8543\n",
      "55 0.686696\n",
      "56 1.18779\n",
      "57 1.55457\n",
      "58 0.939083\n",
      "59 1.75836\n",
      "60 0.714902\n",
      "61 1.65627\n",
      "62 0.820041\n",
      "63 1.84547\n",
      "64 0.683273\n",
      "65 1.0621\n",
      "66 1.65559\n",
      "67 0.820364\n",
      "68 1.84331\n",
      "69 0.681688\n",
      "70 1.00898\n",
      "71 1.69747\n",
      "72 0.772546\n",
      "73 1.84446\n",
      "74 0.681801\n",
      "75 1.04391\n",
      "76 1.66722\n",
      "77 0.804462\n",
      "78 1.84648\n",
      "79 0.681993\n",
      "80 1.08249\n",
      "81 1.6338\n",
      "82 0.840834\n",
      "83 1.82712\n",
      "84 0.677551\n",
      "85 0.707873\n",
      "86 1.60138\n",
      "87 0.874863\n",
      "88 1.80221\n",
      "89 0.680244\n",
      "90 0.986101\n",
      "91 1.58019\n",
      "92 1.22236\n",
      "93 1.30926\n",
      "94 1.43986\n",
      "95 1.06211\n",
      "96 1.64328\n",
      "97 0.83083\n",
      "98 1.82783\n",
      "99 0.675781\n",
      "100 0.685878\n",
      "101 1.25722\n",
      "102 1.2655\n",
      "103 1.47542\n",
      "104 1.01758\n",
      "105 1.67932\n",
      "106 0.786446\n",
      "107 1.8403\n",
      "108 0.67781\n",
      "109 0.962473\n",
      "110 1.72514\n",
      "111 0.734363\n",
      "112 1.76428\n",
      "113 0.697384\n",
      "114 1.46915\n",
      "115 1.01656\n",
      "116 1.68148\n",
      "117 0.776628\n",
      "118 1.83564\n",
      "119 0.678207\n",
      "120 0.968919\n",
      "121 1.71927\n",
      "122 0.736015\n",
      "123 1.76579\n",
      "124 0.693883\n",
      "125 1.40817\n",
      "126 1.08317\n",
      "127 1.62497\n",
      "128 0.837203\n",
      "129 1.82008\n",
      "130 0.67539\n",
      "131 0.697237\n",
      "132 1.47132\n",
      "133 1.01107\n",
      "134 1.68227\n",
      "135 0.772878\n",
      "136 1.82945\n",
      "137 0.676268\n",
      "138 0.866036\n",
      "139 1.79831\n",
      "140 0.677247\n",
      "141 0.937548\n",
      "142 1.62516\n",
      "143 1.17551\n",
      "144 1.35201\n",
      "145 1.39446\n",
      "146 1.10326\n",
      "147 1.59909\n",
      "148 0.870656\n",
      "149 1.79043\n",
      "150 0.680645\n",
      "151 1.12784\n",
      "152 1.40367\n",
      "153 1.35282\n",
      "154 1.14761\n",
      "155 1.56195\n",
      "156 0.910124\n",
      "157 1.75847\n",
      "158 0.699247\n",
      "159 1.4971\n",
      "160 0.978629\n",
      "161 1.70371\n",
      "162 0.746881\n",
      "163 1.78525\n",
      "164 0.680501\n",
      "165 1.09035\n",
      "166 1.4432\n",
      "167 1.32122\n",
      "168 1.17962\n",
      "169 1.5348\n",
      "170 0.937174\n",
      "171 1.73506\n",
      "172 0.71677\n",
      "173 1.65359\n",
      "174 0.798824\n",
      "175 1.82663\n",
      "176 0.675322\n",
      "177 0.831997\n",
      "178 1.8144\n",
      "179 0.674146\n",
      "180 0.67395\n",
      "181 0.758171\n",
      "182 1.80356\n",
      "183 0.674799\n",
      "184 0.830555\n",
      "185 1.74052\n",
      "186 1.07414\n",
      "187 1.46352\n",
      "188 1.29535\n",
      "189 1.21206\n",
      "190 1.50182\n",
      "191 0.977428\n",
      "192 1.69731\n",
      "193 0.756485\n",
      "194 1.79741\n",
      "195 0.676326\n",
      "196 0.952062\n",
      "197 1.60098\n",
      "198 1.18538\n",
      "199 1.33418\n",
      "200 1.40094\n",
      "201 1.08942\n",
      "202 1.60322\n",
      "203 0.859394\n",
      "204 1.79132\n",
      "205 0.678414\n",
      "206 1.04493\n",
      "207 1.49234\n",
      "208 1.27298\n",
      "209 1.2324\n",
      "210 1.48457\n",
      "211 0.992182\n",
      "212 1.68384\n",
      "213 0.766968\n",
      "214 1.80544\n",
      "215 0.674399\n",
      "216 0.794172\n",
      "217 1.76934\n",
      "218 1.04609\n",
      "219 1.49261\n",
      "220 1.26726\n",
      "221 1.24121\n",
      "222 1.47377\n",
      "223 1.00653\n",
      "224 1.66943\n",
      "225 0.784258\n",
      "226 1.81439\n",
      "227 0.673524\n",
      "228 0.698661\n",
      "229 1.52031\n",
      "230 1.24649\n",
      "231 1.26176\n",
      "232 1.45733\n",
      "233 1.02237\n",
      "234 1.65615\n",
      "235 0.796621\n",
      "236 1.81537\n",
      "237 0.673622\n",
      "238 0.679731\n",
      "239 1.12734\n",
      "240 1.57004\n",
      "241 0.890976\n",
      "242 1.76495\n",
      "243 0.690043\n",
      "244 1.31905\n",
      "245 1.17247\n",
      "246 1.53464\n",
      "247 0.927765\n",
      "248 1.73624\n",
      "249 0.709552\n",
      "250 1.56229\n",
      "251 0.893187\n",
      "252 1.76502\n",
      "253 0.687676\n",
      "254 1.25063\n",
      "255 1.24844\n",
      "256 1.47325\n",
      "257 0.99534\n",
      "258 1.68067\n",
      "259 0.762153\n",
      "260 1.78825\n",
      "261 0.677885\n",
      "262 0.946862\n",
      "263 1.60012\n",
      "264 1.18347\n",
      "265 1.3288\n",
      "266 1.40168\n",
      "267 1.08096\n",
      "268 1.606\n",
      "269 0.848902\n",
      "270 1.79276\n",
      "271 0.677327\n",
      "272 0.945759\n",
      "273 1.60145\n",
      "274 1.17897\n",
      "275 1.33477\n",
      "276 1.39461\n",
      "277 1.08992\n",
      "278 1.59707\n",
      "279 0.859759\n",
      "280 1.78439\n",
      "281 0.680497\n",
      "282 1.06558\n",
      "283 1.46214\n",
      "284 1.29223\n",
      "285 1.20395\n",
      "286 1.50292\n",
      "287 0.964763\n",
      "288 1.70147\n",
      "289 0.742915\n",
      "290 1.73461\n",
      "291 0.709715\n",
      "292 1.54117\n",
      "293 0.914485\n",
      "294 1.74535\n",
      "295 0.699704\n",
      "296 1.42402\n",
      "297 1.04652\n",
      "298 1.63761\n",
      "299 0.804889\n",
      "300 1.8085\n",
      "301 0.675568\n",
      "302 0.675534\n",
      "303 0.777265\n",
      "304 1.79823\n",
      "305 0.675983\n",
      "306 0.806969\n",
      "307 1.75092\n",
      "308 1.05691\n",
      "309 1.47316\n",
      "310 1.27877\n",
      "311 1.2209\n",
      "312 1.48593\n",
      "313 0.985546\n",
      "314 1.68208\n",
      "315 0.764158\n",
      "316 1.77993\n",
      "317 0.681945\n",
      "318 1.08713\n",
      "319 1.43516\n",
      "320 1.31337\n",
      "321 1.17747\n",
      "322 1.52379\n",
      "323 0.938598\n",
      "324 1.72182\n",
      "325 0.721635\n",
      "326 1.62454\n",
      "327 0.820242\n",
      "328 1.80344\n",
      "329 0.675825\n",
      "330 0.722782\n",
      "331 1.66717\n",
      "332 1.12386\n",
      "333 1.39468\n",
      "334 1.34284\n",
      "335 1.14588\n",
      "336 1.54787\n",
      "337 0.912878\n",
      "338 1.7413\n",
      "339 0.706176\n",
      "340 1.49306\n",
      "341 0.969319\n",
      "342 1.69741\n",
      "343 0.742089\n",
      "344 1.71668\n",
      "345 0.722053\n",
      "346 1.61508\n",
      "347 0.826923\n",
      "348 1.80088\n",
      "349 0.676628\n",
      "350 0.717012\n",
      "351 1.62519\n",
      "352 1.15887\n",
      "353 1.35272\n",
      "354 1.37783\n",
      "355 1.10394\n",
      "356 1.58285\n",
      "357 0.871062\n",
      "358 1.77283\n",
      "359 0.685377\n",
      "360 1.15275\n",
      "361 1.35772\n",
      "362 1.37549\n",
      "363 1.10438\n",
      "364 1.58332\n",
      "365 0.868499\n",
      "366 1.7748\n",
      "367 0.684103\n",
      "368 1.10858\n",
      "369 1.408\n",
      "370 1.3344\n",
      "371 1.15081\n",
      "372 1.54457\n",
      "373 0.912244\n",
      "374 1.74156\n",
      "375 0.70409\n",
      "376 1.4506\n",
      "377 1.01504\n",
      "378 1.65914\n",
      "379 0.780104\n",
      "380 1.78796\n",
      "381 0.679146\n",
      "382 0.894064\n",
      "383 1.65557\n",
      "384 1.13274\n",
      "385 1.38183\n",
      "386 1.35244\n",
      "387 1.13215\n",
      "388 1.55806\n",
      "389 0.898551\n",
      "390 1.75096\n",
      "391 0.698385\n",
      "392 1.37287\n",
      "393 1.10412\n",
      "394 1.58413\n",
      "395 0.864465\n",
      "396 1.77692\n",
      "397 0.683116\n",
      "398 1.04828\n",
      "399 1.4764\n",
      "400 1.27807\n",
      "401 1.21408\n",
      "402 1.4913\n",
      "403 0.972003\n",
      "404 1.69181\n",
      "405 0.748437\n",
      "406 1.72228\n",
      "407 0.71775\n",
      "408 1.56269\n",
      "409 0.885227\n",
      "410 1.76344\n",
      "411 0.688396\n",
      "412 1.16808\n",
      "413 1.33656\n",
      "414 1.39415\n",
      "415 1.07903\n",
      "416 1.60451\n",
      "417 0.84057\n",
      "418 1.7895\n",
      "419 0.6795\n",
      "420 0.882244\n",
      "421 1.66805\n",
      "422 1.12056\n",
      "423 1.39525\n",
      "424 1.33979\n",
      "425 1.1461\n",
      "426 1.54511\n",
      "427 0.912792\n",
      "428 1.73848\n",
      "429 0.707679\n",
      "430 1.4683\n",
      "431 0.994523\n",
      "432 1.67395\n",
      "433 0.764076\n",
      "434 1.75546\n",
      "435 0.693187\n",
      "436 1.25545\n",
      "437 1.23544\n",
      "438 1.47647\n",
      "439 0.984197\n",
      "440 1.68283\n",
      "441 0.754084\n",
      "442 1.73013\n",
      "443 0.709778\n",
      "444 1.47525\n",
      "445 0.982682\n",
      "446 1.68583\n",
      "447 0.748738\n",
      "448 1.71167\n",
      "449 0.723559\n",
      "450 1.5863\n",
      "451 0.854915\n",
      "452 1.78312\n",
      "453 0.681224\n",
      "454 0.893828\n",
      "455 1.65361\n",
      "456 1.13459\n",
      "457 1.37692\n",
      "458 1.35595\n",
      "459 1.12521\n",
      "460 1.56286\n",
      "461 0.890234\n",
      "462 1.7556\n",
      "463 0.695192\n",
      "464 1.28939\n",
      "465 1.19759\n",
      "466 1.50536\n",
      "467 0.952355\n",
      "468 1.70763\n",
      "469 0.730826\n",
      "470 1.63095\n",
      "471 0.807748\n",
      "472 1.7927\n",
      "473 0.679624\n",
      "474 0.805191\n",
      "475 1.74213\n",
      "476 1.06017\n",
      "477 1.46345\n",
      "478 1.28266\n",
      "479 1.21033\n",
      "480 1.49047\n",
      "481 0.974246\n",
      "482 1.68707\n",
      "483 0.754415\n",
      "484 1.72486\n",
      "485 0.716628\n",
      "486 1.5303\n",
      "487 0.921471\n",
      "488 1.73326\n",
      "489 0.708022\n",
      "490 1.44263\n",
      "491 1.01961\n",
      "492 1.6541\n",
      "493 0.781726\n",
      "494 1.77444\n",
      "495 0.684522\n",
      "496 1.00814\n",
      "497 1.52024\n",
      "498 1.24191\n",
      "499 1.25253\n",
      "500 1.45829\n",
      "501 1.00677\n",
      "502 1.66137\n",
      "503 0.778213\n",
      "504 1.76911\n",
      "505 0.687538\n",
      "506 1.10718\n",
      "507 1.40594\n",
      "508 1.33374\n",
      "509 1.14771\n",
      "510 1.54461\n",
      "511 0.908337\n",
      "512 1.74146\n",
      "513 0.703904\n",
      "514 1.39185\n",
      "515 1.07877\n",
      "516 1.60356\n",
      "517 0.838868\n",
      "518 1.78596\n",
      "519 0.681712\n",
      "520 0.894226\n",
      "521 1.65226\n",
      "522 1.13209\n",
      "523 1.37975\n",
      "524 1.35121\n",
      "525 1.1307\n",
      "526 1.55651\n",
      "527 0.897427\n",
      "528 1.74842\n",
      "529 0.700548\n",
      "530 1.34526\n",
      "531 1.13289\n",
      "532 1.55758\n",
      "533 0.891929\n",
      "534 1.75391\n",
      "535 0.695532\n",
      "536 1.25851\n",
      "537 1.23087\n",
      "538 1.47774\n",
      "539 0.981737\n",
      "540 1.68278\n",
      "541 0.753703\n",
      "542 1.71159\n",
      "543 0.725071\n",
      "544 1.57438\n",
      "545 0.868363\n",
      "546 1.7716\n",
      "547 0.686099\n",
      "548 1.0257\n",
      "549 1.49898\n",
      "550 1.25879\n",
      "551 1.23215\n",
      "552 1.47469\n",
      "553 0.98695\n",
      "554 1.67731\n",
      "555 0.760644\n",
      "556 1.72962\n",
      "557 0.711374\n",
      "558 1.45875\n",
      "559 1.00064\n",
      "560 1.66853\n",
      "561 0.766118\n",
      "562 1.7409\n",
      "563 0.701919\n",
      "564 1.3395\n",
      "565 1.13589\n",
      "566 1.55768\n",
      "567 0.888142\n",
      "568 1.7576\n",
      "569 0.69264\n",
      "570 1.18498\n",
      "571 1.31428\n",
      "572 1.40987\n",
      "573 1.05815\n",
      "574 1.61945\n",
      "575 0.820973\n",
      "576 1.78704\n",
      "577 0.682168\n",
      "578 0.876701\n",
      "579 1.67128\n",
      "580 1.11565\n",
      "581 1.39813\n",
      "582 1.33515\n",
      "583 1.14859\n",
      "584 1.54078\n",
      "585 0.914919\n",
      "586 1.73407\n",
      "587 0.711074\n",
      "588 1.45443\n",
      "589 1.00778\n",
      "590 1.66066\n",
      "591 0.776308\n",
      "592 1.75689\n",
      "593 0.693632\n",
      "594 1.19939\n",
      "595 1.29774\n",
      "596 1.42265\n",
      "597 1.04366\n",
      "598 1.63099\n",
      "599 0.808226\n",
      "600 1.78394\n",
      "601 0.68323\n",
      "602 0.909514\n",
      "603 1.63373\n",
      "604 1.1461\n",
      "605 1.36264\n",
      "606 1.36446\n",
      "607 1.11447\n",
      "608 1.56923\n",
      "609 0.881821\n",
      "610 1.75832\n",
      "611 0.694956\n",
      "612 1.22999\n",
      "613 1.26405\n",
      "614 1.44812\n",
      "615 1.01618\n",
      "616 1.65257\n",
      "617 0.786001\n",
      "618 1.76733\n",
      "619 0.689321\n",
      "620 1.09881\n",
      "621 1.41413\n",
      "622 1.32582\n",
      "623 1.15528\n",
      "624 1.5371\n",
      "625 0.91542\n",
      "626 1.73437\n",
      "627 0.709337\n",
      "628 1.42244\n",
      "629 1.04247\n",
      "630 1.63241\n",
      "631 0.805388\n",
      "632 1.78107\n",
      "633 0.684289\n",
      "634 0.927636\n",
      "635 1.61236\n",
      "636 1.16381\n",
      "637 1.34157\n",
      "638 1.38201\n",
      "639 1.09359\n",
      "640 1.58666\n",
      "641 0.861208\n",
      "642 1.7706\n",
      "643 0.689015\n",
      "644 1.09129\n",
      "645 1.42343\n",
      "646 1.3165\n",
      "647 1.16685\n",
      "648 1.52644\n",
      "649 0.928449\n",
      "650 1.72341\n",
      "651 0.718455\n",
      "652 1.50546\n",
      "653 0.948176\n",
      "654 1.70947\n",
      "655 0.727677\n",
      "656 1.57057\n",
      "657 0.872241\n",
      "658 1.76658\n",
      "659 0.689267\n",
      "660 1.06932\n",
      "661 1.44725\n",
      "662 1.29976\n",
      "663 1.18391\n",
      "664 1.51368\n",
      "665 0.941006\n",
      "666 1.71402\n",
      "667 0.725189\n",
      "668 1.55365\n",
      "669 0.892491\n",
      "670 1.75256\n",
      "671 0.696336\n",
      "672 1.22298\n",
      "673 1.26963\n",
      "674 1.44539\n",
      "675 1.01658\n",
      "676 1.65303\n",
      "677 0.783299\n",
      "678 1.7595\n",
      "679 0.692868\n",
      "680 1.15407\n",
      "681 1.34922\n",
      "682 1.37963\n",
      "683 1.09221\n",
      "684 1.5898\n",
      "685 0.853902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686 1.77445\n",
      "687 0.687136\n",
      "688 1.01351\n",
      "689 1.5125\n",
      "690 1.24433\n",
      "691 1.24867\n",
      "692 1.45856\n",
      "693 1.00534\n",
      "694 1.6601\n",
      "695 0.778903\n",
      "696 1.75201\n",
      "697 0.697865\n",
      "698 1.24993\n",
      "699 1.23936\n",
      "700 1.469\n",
      "701 0.99037\n",
      "702 1.67401\n",
      "703 0.762013\n",
      "704 1.71609\n",
      "705 0.721583\n",
      "706 1.51766\n",
      "707 0.931488\n",
      "708 1.72379\n",
      "709 0.714619\n",
      "710 1.45343\n",
      "711 1.00401\n",
      "712 1.66543\n",
      "713 0.767419\n",
      "714 1.72807\n",
      "715 0.710758\n",
      "716 1.41038\n",
      "717 1.05269\n",
      "718 1.62573\n",
      "719 0.809447\n",
      "720 1.77928\n",
      "721 0.685291\n",
      "722 0.912827\n",
      "723 1.62853\n",
      "724 1.15141\n",
      "725 1.35458\n",
      "726 1.37136\n",
      "727 1.10447\n",
      "728 1.57736\n",
      "729 0.87059\n",
      "730 1.76408\n",
      "731 0.692383\n",
      "732 1.14454\n",
      "733 1.36122\n",
      "734 1.36738\n",
      "735 1.10759\n",
      "736 1.57554\n",
      "737 0.871349\n",
      "738 1.76389\n",
      "739 0.692201\n",
      "740 1.13515\n",
      "741 1.37169\n",
      "742 1.35915\n",
      "743 1.11661\n",
      "744 1.56819\n",
      "745 0.879357\n",
      "746 1.75893\n",
      "747 0.694622\n",
      "748 1.18232\n",
      "749 1.31714\n",
      "750 1.40418\n",
      "751 1.06472\n",
      "752 1.61157\n",
      "753 0.829875\n",
      "754 1.77957\n",
      "755 0.686051\n",
      "756 0.955768\n",
      "757 1.57919\n",
      "758 1.18893\n",
      "759 1.3123\n",
      "760 1.40495\n",
      "761 1.06687\n",
      "762 1.608\n",
      "763 0.836535\n",
      "764 1.7776\n",
      "765 0.687221\n",
      "766 1.0005\n",
      "767 1.52769\n",
      "768 1.22995\n",
      "769 1.26563\n",
      "770 1.44318\n",
      "771 1.02343\n",
      "772 1.64407\n",
      "773 0.796659\n",
      "774 1.76783\n",
      "775 0.690896\n",
      "776 1.09451\n",
      "777 1.41854\n",
      "778 1.32006\n",
      "779 1.1615\n",
      "780 1.53031\n",
      "781 0.92276\n",
      "782 1.72696\n",
      "783 0.715707\n",
      "784 1.45857\n",
      "785 1.00049\n",
      "786 1.66586\n",
      "787 0.769132\n",
      "788 1.72631\n",
      "789 0.713445\n",
      "790 1.42914\n",
      "791 1.03172\n",
      "792 1.64172\n",
      "793 0.792575\n",
      "794 1.76353\n",
      "795 0.69133\n",
      "796 1.07974\n",
      "797 1.43414\n",
      "798 1.31003\n",
      "799 1.17097\n",
      "800 1.52387\n",
      "801 0.928152\n",
      "802 1.72343\n",
      "803 0.717454\n",
      "804 1.47116\n",
      "805 0.985232\n",
      "806 1.67887\n",
      "807 0.755287\n",
      "808 1.68649\n",
      "809 0.746638\n",
      "810 1.65258\n",
      "811 0.779564\n",
      "812 1.74538\n",
      "813 0.699791\n",
      "814 1.24269\n",
      "815 1.24492\n",
      "816 1.46659\n",
      "817 0.99\n",
      "818 1.67519\n",
      "819 0.758585\n",
      "820 1.69689\n",
      "821 0.736848\n",
      "822 1.60446\n",
      "823 0.831644\n",
      "824 1.77959\n",
      "825 0.685921\n",
      "826 0.895818\n",
      "827 1.6473\n",
      "828 1.13586\n",
      "829 1.37184\n",
      "830 1.35666\n",
      "831 1.12068\n",
      "832 1.56333\n",
      "833 0.885988\n",
      "834 1.75324\n",
      "835 0.698574\n",
      "836 1.24074\n",
      "837 1.24991\n",
      "838 1.45875\n",
      "839 1.00216\n",
      "840 1.66306\n",
      "841 0.773585\n",
      "842 1.73406\n",
      "843 0.708855\n",
      "844 1.37653\n",
      "845 1.09251\n",
      "846 1.59053\n",
      "847 0.849946\n",
      "848 1.7741\n",
      "849 0.687922\n",
      "850 0.982216\n",
      "851 1.54761\n",
      "852 1.21575\n",
      "853 1.2801\n",
      "854 1.43212\n",
      "855 1.03427\n",
      "856 1.63536\n",
      "857 0.804717\n",
      "858 1.77098\n",
      "859 0.689764\n",
      "860 1.04826\n",
      "861 1.47154\n",
      "862 1.27645\n",
      "863 1.2111\n",
      "864 1.48873\n",
      "865 0.970005\n",
      "866 1.68846\n",
      "867 0.74957\n",
      "868 1.6638\n",
      "869 0.771511\n",
      "870 1.72792\n",
      "871 0.712639\n",
      "872 1.41321\n",
      "873 1.04983\n",
      "874 1.62622\n",
      "875 0.809412\n",
      "876 1.77359\n",
      "877 0.688082\n",
      "878 0.974189\n",
      "879 1.55664\n",
      "880 1.20886\n",
      "881 1.28759\n",
      "882 1.4261\n",
      "883 1.04073\n",
      "884 1.63001\n",
      "885 0.810267\n",
      "886 1.77294\n",
      "887 0.689148\n",
      "888 1.02508\n",
      "889 1.49828\n",
      "890 1.25437\n",
      "891 1.2364\n",
      "892 1.4675\n",
      "893 0.994319\n",
      "894 1.66815\n",
      "895 0.770039\n",
      "896 1.72386\n",
      "897 0.717181\n",
      "898 1.45906\n",
      "899 0.99858\n",
      "900 1.66753\n",
      "901 0.766455\n",
      "902 1.71413\n",
      "903 0.722497\n",
      "904 1.5002\n",
      "905 0.949655\n",
      "906 1.70874\n",
      "907 0.726405\n",
      "908 1.52948\n",
      "909 0.91583\n",
      "910 1.73535\n",
      "911 0.706389\n",
      "912 1.33018\n",
      "913 1.1439\n",
      "914 1.54931\n",
      "915 0.89496\n",
      "916 1.74936\n",
      "917 0.698702\n",
      "918 1.21795\n",
      "919 1.27381\n",
      "920 1.44125\n",
      "921 1.01961\n",
      "922 1.64961\n",
      "923 0.785732\n",
      "924 1.75114\n",
      "925 0.698079\n",
      "926 1.20905\n",
      "927 1.28435\n",
      "928 1.4321\n",
      "929 1.03046\n",
      "930 1.64032\n",
      "931 0.796066\n",
      "932 1.76287\n",
      "933 0.692615\n",
      "934 1.09925\n",
      "935 1.41149\n",
      "936 1.32701\n",
      "937 1.1516\n",
      "938 1.53894\n",
      "939 0.910985\n",
      "940 1.73571\n",
      "941 0.708891\n",
      "942 1.37027\n",
      "943 1.10017\n",
      "944 1.58311\n",
      "945 0.858836\n",
      "946 1.76913\n",
      "947 0.690226\n",
      "948 1.03763\n",
      "949 1.483\n",
      "950 1.26798\n",
      "951 1.21971\n",
      "952 1.48192\n",
      "953 0.976719\n",
      "954 1.68296\n",
      "955 0.75429\n",
      "956 1.6768\n",
      "957 0.75801\n",
      "958 1.68829\n",
      "959 0.745437\n",
      "960 1.63905\n",
      "961 0.794022\n",
      "962 1.76008\n",
      "963 0.69316\n",
      "964 1.09565\n",
      "965 1.41484\n",
      "966 1.32578\n",
      "967 1.15185\n",
      "968 1.53952\n",
      "969 0.90918\n",
      "970 1.73738\n",
      "971 0.707328\n",
      "972 1.34737\n",
      "973 1.12589\n",
      "974 1.56206\n",
      "975 0.882248\n",
      "976 1.75642\n",
      "977 0.695792\n",
      "978 1.16293\n",
      "979 1.33771\n",
      "980 1.38753\n",
      "981 1.08186\n",
      "982 1.59703\n",
      "983 0.844486\n",
      "984 1.7734\n",
      "985 0.689025\n",
      "986 1.0025\n",
      "987 1.5239\n",
      "988 1.23369\n",
      "989 1.25951\n",
      "990 1.44828\n",
      "991 1.01573\n",
      "992 1.65016\n",
      "993 0.788633\n",
      "994 1.75282\n",
      "995 0.698565\n",
      "996 1.2174\n",
      "997 1.27555\n",
      "998 1.4377\n",
      "999 1.02503\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(1000):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.588767092327\n",
      "Accuracy on the test set is  0.585216998192\n"
     ]
    }
   ],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_pred = A_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('master_normalized.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "n_x = X.shape[1]\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2) )\n",
    "\n",
    "# Optimizer.\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(tf_J)\n",
    "\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 3.45627\n",
      "50 0.916226\n",
      "100 0.783304\n",
      "150 0.729717\n",
      "200 0.698526\n",
      "250 0.680674\n",
      "300 0.672604\n",
      "350 0.668805\n",
      "400 0.666762\n",
      "450 0.665453\n",
      "500 0.664554\n",
      "550 0.663988\n",
      "600 0.66335\n",
      "650 0.663677\n",
      "700 0.663214\n",
      "750 0.663078\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(800):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.562040908577\n",
      "Accuracy on the test set is  0.556057866184\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A2.eval()\n",
    "A_test = tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_pred(pred):\n",
    "    for i in range(0, len(pred)):\n",
    "        if(pred[i]<0.5):\n",
    "            pred[i] = 0\n",
    "        else:\n",
    "            pred[i] = 1\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_pred = A_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('master_normalized.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)\n",
    "\n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, 1)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "#optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(tf_J)\n",
    "\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step  (0, 0.69314742)\n",
      "Minibatch accuracy:  0.4\n",
      "Test accuracy:  0.530967450271\n",
      "Minibatch loss at step  (500, 0.70877707)\n",
      "Minibatch accuracy:  0.58\n",
      "Test accuracy:  0.592450271248\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000\n",
    "batch_size = 100\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_pred = A_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Ensamble with all models\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensamble(pred1, pred2, pred3, pred4, pred5, pred6, pred7):\n",
    "    prediction = []\n",
    "    for i in range(0, len(pred1)):\n",
    "        p = (pred1[i] + pred2[i] + pred3[i] + pred4[i] + pred5[i] + pred6[i] + pred7[i])\n",
    "        p = p/7\n",
    "        if(p < 0.5):\n",
    "            prediction.append(0)\n",
    "        else:\n",
    "            prediction.append(1)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ..., 1 1 1] [1 1 0 ..., 1 1 1] [1 1 0 ..., 1 1 1] [0 0 1 ..., 0 0 1] [[ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]] [[ 0.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]] [[ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(linear_Y_pred, poly_Y_pred, gaussian_Y_pred, sigmoid_Y_pred, convert_pred(tensor_pred), convert_pred(nn_pred), convert_pred(sgd_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, labels):\n",
    "    count = 0\n",
    "    for i in range(0, len(pred)):\n",
    "        if(pred[i] == labels[i]):\n",
    "            count += 1\n",
    "    return count/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5981012658227848\n"
     ]
    }
   ],
   "source": [
    "ensamble_pred = ensamble(linear_Y_pred, poly_Y_pred, gaussian_Y_pred, sigmoid_Y_pred, convert_pred(tensor_pred), convert_pred(nn_pred), convert_pred(sgd_pred))\n",
    "\n",
    "ensamble_acc = accuracy(ensamble_pred, Y_test)\n",
    "\n",
    "print(ensamble_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
