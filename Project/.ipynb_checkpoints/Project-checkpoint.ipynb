{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--\n",
    "In our ever changing and evolving field of technology one of the foremost topics is data analytics. The growth in the amount of digital data that is being collected across many different fields is massive and . That is, taking data in whatever raw form it exists and using technology to transform it into information that has value and context. In most cases data analysis is performed in order to provide class descriptions of data, highlight behaviors, trends, associations in the data or predictive information that prove useful or even vital to key decision-makers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "--\n",
    "Using the NHL API following the documentation found at https://gitlab.com/dword4/nhlapi/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each player in the specified year range (years must be consecutive) collect all avalible stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_skaters(y1, y2):\n",
    "    team_rosters = requests.get('https://statsapi.web.nhl.com/api/v1/teams?expand=team.roster&season=' + y1 + y2)\n",
    "    team_rosters = team_rosters.json()\n",
    "    players= []\n",
    "    for i in range(0, len(team_rosters['teams'])):\n",
    "        for j in range(0, len(team_rosters['teams'][i]['roster']['roster'])):\n",
    "            player = [team_rosters['teams'][i]['roster']['roster'][j]['person']['id'], \n",
    "                      team_rosters['teams'][i]['roster']['roster'][j]['person']['fullName'],\n",
    "                     team_rosters['teams'][i]['name']]\n",
    "            if (team_rosters['teams'][i]['roster']['roster'][j]['position']['code'] != 'G'):\n",
    "                players.append(player)\n",
    "    players_stats = []\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'fullName', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "    for i in range(0, len(players)): \n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            players_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        players_stats.append(stats_array)\n",
    "        \n",
    "    skaters = []\n",
    "    skaters.append(header)\n",
    "    for i in range(0, len(players)):\n",
    "        skaters.append(players[i] + players_stats[i])\n",
    "    return skaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the skaters data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_skaters_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting skaters data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        skaters = get_csv_skaters(str(i), str(i+1))\n",
    "        np.savetxt('data/skaters_' + str(i) + '_' + str(i+1) + '.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting skaters data for 1917-1918 season.\n"
     ]
    }
   ],
   "source": [
    "get_skaters_data(1917,1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For each team in the specified year range (years must be consecutive) collect all avalible stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_team(y1, y2):\n",
    "    teams = requests.get('https://statsapi.web.nhl.com/api/v1/teams?season=' + str(y1) + str(y2))\n",
    "    teams = teams.json()\n",
    "    team_id_name = []\n",
    "    for i in range(0, len(teams['teams'])):\n",
    "        team_arr = [teams['teams'][i]['id'], teams['teams'][i]['name']]\n",
    "        team_id_name.append(team_arr)\n",
    "\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                           + str(team_id_name[0][0])\n",
    "                           + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "\n",
    "    team_stats = []\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                             + str(team_id_name[i][0]) \n",
    "                             + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            team_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        team_stats.append(stats_array)\n",
    "\n",
    "    \n",
    "    teams_stats_final = []\n",
    "    teams_stats_final.append(header)\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        teams_stats_final.append(team_id_name[i] + team_stats[i]) \n",
    "    return teams_stats_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the team data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting team data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_team(str(i), str(i+1))\n",
    "        np.savetxt('team_data/teams_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting team data for 1917-1918 season.\n"
     ]
    }
   ],
   "source": [
    "get_team_data(1917, 1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the index of the team stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_stats(id, team_data):\n",
    "    for i in range(0, len(team_data)):\n",
    "        if team_data[i][0] == id:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each game in the specified year range (years must be consecutive) return the winner, away team ID, home team ID, and the away and home team stats for that season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_game(y1, y2):\n",
    "    team_data = get_csv_team(y1, y2)\n",
    "    header = team_data[0][3:]\n",
    "    away_header = []\n",
    "    home_header = []\n",
    "    for head in header:\n",
    "        away_header.append('away_'+head)\n",
    "        home_header.append('home_'+head)\n",
    "    games_data = [['winner', 'awayID', 'homeID'] + away_header + home_header]\n",
    "    games = requests.get('https://statsapi.web.nhl.com/api/v1/schedule?startDate=' \n",
    "                         + str(y1) + '-10-01&endDate=' + str(y2) + '-06-30')\n",
    "    games = games.json()\n",
    "    for date in games['dates']:\n",
    "        for game in date['games']:\n",
    "            away_ID = game['teams']['away']['team']['id']\n",
    "            home_ID = game['teams']['home']['team']['id']\n",
    "            if away_ID > 80 or home_ID > 80:\n",
    "                continue\n",
    "            \n",
    "            away_score = game['teams']['away']['score']\n",
    "            home_score = game['teams']['home']['score']\n",
    "            winner = 0\n",
    "            away_stats = team_data[get_team_stats(away_ID, team_data)][3:]\n",
    "            home_stats = team_data[get_team_stats(home_ID, team_data)][3:]\n",
    "            if home_score > away_score:\n",
    "                winner = 1\n",
    "            games_data.append([winner,\n",
    "                          away_ID, \n",
    "                          home_ID] +\n",
    "                          away_stats +\n",
    "                          home_stats)\n",
    "    return games_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the game data for a year range and saves the result as a csv file.\n",
    "Years must be in the range [1917, 2019], note that the 2004-2005 season is skipped as this was a lockout year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_game_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting game data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_game(i, i+1)\n",
    "        np.savetxt('game_data/game_data_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting game data for 1917-1918 season.\n"
     ]
    }
   ],
   "source": [
    "get_game_data(1917,1918)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aanaylsis\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "header for each dataset is \n",
    "winner, awayID, homeID, wins, losses, ot, pts, ptPctg, goalsPerGame, goalsAgainstPerGame, evGGARatio, powerPlayPercentage, powerPlayGoals, powerPlayGoalsAgainst, powerPlayOpportunities, penaltyKillPercentage, shotsPerGame, shotsAllowed, winScoreFirst, winOppScoreFirst, winLeadFirstPer, winLeadSecondPer, winOutshootOpp, win OutshotByOpp, faceOffstaken, faceOffsWon, faceOffsLost, faceOffWinPercentage, shootingPctg, savePctg\n",
    "\n",
    "Index(['winner', 'awayID', 'homeID', 'wins', 'losses', 'ot', 'pts', 'ptPctg',\n",
    "       'goalsPerGame', 'goalsAgainstPerGame', 'evGGARatio',\n",
    "       'powerPlayPercentage', 'powerPlayGoals', 'powerPlayGoalsAgainst',\n",
    "       'powerPlayOpportunities', 'penaltyKillPercentage', 'shotsPerGame',\n",
    "       'shotsAllowed', 'winScoreFirst', 'winOppScoreFirst', 'winLeadFirstPer',\n",
    "       'winLeadSecondPer', 'winOutshootOpp', 'winOutshotByOpp',\n",
    "       'faceOffsTaken', 'faceOffsWon', 'faceOffsLost', 'faceOffWinPercentage',\n",
    "       'shootingPctg', 'savePctg', 'wins.1', 'losses.1', 'ot.1', 'pts.1',\n",
    "       'ptPctg.1', 'goalsPerGame.1', 'goalsAgainstPerGame.1', 'evGGARatio.1',\n",
    "       'powerPlayPercentage.1', 'powerPlayGoals.1', 'powerPlayGoalsAgainst.1',\n",
    "       'powerPlayOpportunities.1', 'penaltyKillPercentage.1', 'shotsPerGame.1',\n",
    "       'shotsAllowed.1', 'winScoreFirst.1', 'winOppScoreFirst.1',\n",
    "       'winLeadFirstPer.1', 'winLeadSecondPer.1', 'winOutshootOpp.1',\n",
    "       'winOutshotByOpp.1', 'faceOffsTaken.1', 'faceOffsWon.1',\n",
    "       'faceOffsLost.1', 'faceOffWinPercentage.1', 'shootingPctg.1',\n",
    "       'savePctg.1'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def normalize(data):  \n",
    "#     cols = data.columns\n",
    "#     precentage_stats = ['ptPctg', 'powerPlayPercentage', 'penaltyKillPercentage', 'faceOffWinPercentage', 'shootingPctg', \n",
    "#                         'ptPctg.1', 'powerPlayPercentage.1', 'penaltyKillPercentage.1', 'faceOffWinPercentage.1', 'shootingPctg.1']\n",
    "#     for stat in precentage_stats:\n",
    "#         data[stat] = data[stat]/100\n",
    "\n",
    "#     normalized_stats = ['wins', 'losses', 'ot', 'pts', 'goalsPerGame', 'goalsAgainstPerGame', 'evGGARatio', \n",
    "#                         'powerPlayGoals', 'powerPlayGoalsAgainst', 'powerPlayOpportunities', 'shotsPerGame',\n",
    "#                         'shotsAllowed', 'faceOffsTaken', 'faceOffsWon', 'faceOffsLost',\n",
    "#                         'wins.1', 'losses.1', 'ot.1', 'pts.1', 'goalsPerGame.1', 'goalsAgainstPerGame.1', 'evGGARatio.1', \n",
    "#                         'powerPlayGoals.1', 'powerPlayGoalsAgainst.1', 'powerPlayOpportunities.1', 'shotsPerGame.1',\n",
    "#                         'shotsAllowed.1', 'faceOffsTaken.1', 'faceOffsWon.1', 'faceOffsLost.1']\n",
    "#     maxX = np.max(data, axis=0)\n",
    "#     minX = np.min(data, axis=0)\n",
    "#     for stat in normalized_stats:\n",
    "# #         print(stat)\n",
    "#         data[stat] = (data[stat]-minX[stat])/(maxX[stat]-minX[stat])\n",
    "#     return data\n",
    "def normalize(data):\n",
    "    max_data = np.max(data, axis=0)\n",
    "    min_data = np.min(data, axis=0)\n",
    "    stats = ['away_wins', 'away_losses', 'away_ot',\n",
    "             'away_pts', 'away_ptPctg', 'away_goalsPerGame',\n",
    "             'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "             'away_powerPlayPercentage', 'away_powerPlayGoals',\n",
    "             'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities',\n",
    "             'away_penaltyKillPercentage', 'away_shotsPerGame', 'away_shotsAllowed',\n",
    "             'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "             'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "             'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "             'away_faceOffWinPercentage', 'away_shootingPctg', 'away_savePctg',\n",
    "             'home_wins', 'home_losses', 'home_ot', 'home_pts', 'home_ptPctg',\n",
    "             'home_goalsPerGame', 'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "             'home_powerPlayPercentage', 'home_powerPlayGoals',\n",
    "             'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities',\n",
    "             'home_penaltyKillPercentage', 'home_shotsPerGame', 'home_shotsAllowed',\n",
    "             'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "             'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "             'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "             'home_faceOffWinPercentage', 'home_shootingPctg', 'home_savePctg']\n",
    "    for stat in stats:\n",
    "        data[stat] = (data[stat] - min_data[stat])/(max_data[stat] - min_data[stat])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "\n",
    "# header = ['winner', 'awayID', 'homeID', 'away_wins', 'away_losses', 'away_ot',\n",
    "#        'away_pts', 'away_ptPctg', 'away_goalsPerGame',\n",
    "#        'away_goalsAgainstPerGame', 'away_evGGARatio',\n",
    "#        'away_powerPlayPercentage', 'away_powerPlayGoals',\n",
    "#        'away_powerPlayGoalsAgainst', 'away_powerPlayOpportunities',\n",
    "#        'away_penaltyKillPercentage', 'away_shotsPerGame', 'away_shotsAllowed',\n",
    "#        'away_winScoreFirst', 'away_winOppScoreFirst', 'away_winLeadFirstPer',\n",
    "#        'away_winLeadSecondPer', 'away_winOutshootOpp', 'away_winOutshotByOpp',\n",
    "#        'away_faceOffsTaken', 'away_faceOffsWon', 'away_faceOffsLost',\n",
    "#        'away_faceOffWinPercentage', 'away_shootingPctg', 'away_savePctg',\n",
    "#        'home_wins', 'home_losses', 'home_ot', 'home_pts', 'home_ptPctg',\n",
    "#        'home_goalsPerGame', 'home_goalsAgainstPerGame', 'home_evGGARatio',\n",
    "#        'home_powerPlayPercentage', 'home_powerPlayGoals',\n",
    "#        'home_powerPlayGoalsAgainst', 'home_powerPlayOpportunities',\n",
    "#        'home_penaltyKillPercentage', 'home_shotsPerGame', 'home_shotsAllowed',\n",
    "#        'home_winScoreFirst', 'home_winOppScoreFirst', 'home_winLeadFirstPer',\n",
    "#        'home_winLeadSecondPer', 'home_winOutshootOpp', 'home_winOutshotByOpp',\n",
    "#        'home_faceOffsTaken', 'home_faceOffsWon', 'home_faceOffsLost',\n",
    "#        'home_faceOffWinPercentage', 'home_shootingPctg', 'home_savePctg']\n",
    "# header = ', '.join(header)\n",
    "#each one of these data sets needs to be normalized \n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)\n",
    "header\n",
    "# np.savetxt('master.csv', data, fmt='%s', delimiter=',', header = header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "\n",
    "    # we normalize X\n",
    "#     maxX = np.max(X, axis=0)\n",
    "#     minX = np.min(X, axis=0)\n",
    "#     X = (X-minX)/(maxX-minX)\n",
    "\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    \n",
    "    # we normalize y\n",
    "#     maxy = np.max(y, axis=0)\n",
    "#     miny = np.min(y, axis=0)\n",
    "#     y = (y-miny)/(maxy-miny)\n",
    "\n",
    "    where_are_zeros = (y==0)\n",
    "    y[where_are_zeros] = -1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def error(x,y,w):\n",
    "    #print(-y*x@w.T)\n",
    "    return np.log(1+np.exp(-y*x@w.T))\n",
    "\n",
    "#TODO\n",
    "def error_mean(X,y,w):\n",
    "    return sum(error(X,y,w))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def grad(x,y,w):\n",
    "    #print(y*x@w.T)\n",
    "    return (y*x)/(1+np.exp(y*x@w.T))\n",
    "\n",
    "#TODO\n",
    "def grad_mean(X,y,w):\n",
    "    return -1/len(y)*sum(grad(X,y,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X,y,kappa,iter):\n",
    "    w = np.zeros((1,X.shape[1]))\n",
    "    E = []\n",
    "\n",
    "    #TODO\n",
    "    for i in range(0,iter):\n",
    "        E.append(error_mean(X,y,w))\n",
    "        w = w - kappa*grad_mean(X,y,w)\n",
    "    return w,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,E = fit(X,y,0.000001,100)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, X):\n",
    "    pred = 1/(1+np.exp(X@-w.T))\n",
    "    for n in range(0, len(pred)):\n",
    "        if(pred[n] < 0.5):\n",
    "            pred[n] = -1\n",
    "        else:\n",
    "            pred[n] = 1\n",
    "    return pred\n",
    "#TODO\n",
    "def accuracy(y,y_pred):\n",
    "    acc = 0\n",
    "    for n in range(0,len(y)):\n",
    "        if(y[n] + y_pred[n] == 0):\n",
    "            acc = acc +1\n",
    "    return 1-((acc)/len(y))\n",
    "\n",
    "y_pred = predict(w,X)\n",
    "#print(y_pred)\n",
    "print( accuracy(y,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(X,y,pct=80)\n",
    "w,E = fit(X_train,y_train,0.000001,300)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()\n",
    "y_pred = predict(w,X_test)\n",
    "print( accuracy(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression binary classifier in Tensorflow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A>.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_tensorFlow(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "#each one of these data sets needs to be normalized \n",
    "data_2000_2001 = normalize(data_2000_2001)\n",
    "# #np.savetxt('test.csv', data_2000_2001, fmt='%s', delimiter=',')\n",
    "data_2001_2002 = normalize(data_2001_2002)\n",
    "data_2002_2003 = normalize(data_2002_2003)\n",
    "data_2003_2004 = normalize(data_2003_2004)\n",
    "data_2005_2006 = normalize(data_2005_2006)\n",
    "data_2006_2007 = normalize(data_2006_2007)\n",
    "data_2007_2008 = normalize(data_2007_2008)\n",
    "data_2008_2009 = normalize(data_2008_2009)\n",
    "data_2009_2010 = normalize(data_2009_2010)\n",
    "data_2010_2011 = normalize(data_2010_2011)\n",
    "data_2011_2012 = normalize(data_2011_2012)\n",
    "data_2012_2013 = normalize(data_2012_2013)\n",
    "data_2013_2014 = normalize(data_2013_2014)\n",
    "data_2014_2015 = normalize(data_2014_2015)\n",
    "data_2016_2017 = normalize(data_2016_2017)\n",
    "data_2017_2018 = normalize(data_2017_2018)\n",
    "\n",
    "\n",
    "frames = [data_2000_2001, data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, \n",
    "          data_2006_2007, data_2007_2008, data_2008_2009, data_2009_2010, data_2010_2011, \n",
    "          data_2011_2012, data_2012_2013, data_2013_2014, data_2014_2015, data_2015_2016, \n",
    "          data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)\n",
    "\n",
    "X,y = prepare_tensorFlow(data)\n",
    "\n",
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable(tf.zeros((n_x, 1)))\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(1000):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, 1)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "batch_size = 100\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2) )\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(300):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A2.eval()\n",
    "A_test = tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with more layers and more hidden nodes\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "\n",
    "num_hidden_nodes = 100\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_hidden_nodes]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "tf_w3 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b3 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "tf_Z3 = tf.matmul(tf_A2, tf_w3) + tf_b3\n",
    "tf_A3 = tf.nn.relu(tf_Z3)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z3) )\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)\n",
    "tf_Z3_test = tf.matmul(tf_A2_test, tf_w3) + tf_b3\n",
    "tf_A3_test = tf.nn.relu(tf_Z3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(500):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A3.eval()\n",
    "A_test = tf_A3_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
