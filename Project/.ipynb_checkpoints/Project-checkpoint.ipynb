{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://gitlab.com/dword4/nhlapi/tree/master\n",
    "\n",
    "Import numpy, requests, and json modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "--\n",
    "In our ever changing and evolving field of technology one of the foremost topics is data analytics. The growth in the amount of digital data that is being collected across many different fields is massive and . That is, taking data in whatever raw form it exists and using technology to transform it into information that has value and context. In most cases data analysis is performed in order to provide class descriptions of data, highlight behaviors, trends, associations in the data or predictive information that prove useful or even vital to key decision-makers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the rosters for the 2017-2018 season as a json object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_rosters_2017_2018 = requests.get('https://statsapi.web.nhl.com/api/v1/teams?expand=team.roster&season=20172018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_rosters_2017_2018 = team_rosters_2017_2018.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out all the team names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(team_rosters_2017_2018['teams'])):\n",
    "    print(team_rosters_2017_2018['teams'][i]['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get every player from every team\n",
    "Makes a players array holding [player_id, player_name, team]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_2017_2018 = []\n",
    "for i in range(0, len(team_rosters_2017_2018['teams'])):\n",
    "    #print(team_rosters_2017_2018['teams'][i]['name'])\n",
    "    for j in range(0, len(team_rosters_2017_2018['teams'][i]['roster']['roster'])):\n",
    "        #print(team_rosters_2017_2018['teams'][i]['roster']['roster'][j]['person'])\n",
    "        player = [team_rosters_2017_2018['teams'][i]['roster']['roster'][j]['person']['id'], \n",
    "                  team_rosters_2017_2018['teams'][i]['roster']['roster'][j]['person']['fullName'],\n",
    "                 team_rosters_2017_2018['teams'][i]['name']]\n",
    "        if (team_rosters_2017_2018['teams'][i]['roster']['roster'][j]['position']['code'] != 'G'):\n",
    "            players_2017_2018.append(player)\n",
    "                                                                    \n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(players_2017_2018)):\n",
    "    print(players_2017_2018[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the stats for each player as an array of [timeOnIce, assists, goals, pim, shots, games, hits, powerPlayGoals, powerPlayPoints, powerPlayTimeOnIce, evenTimeOnIce, penaltyMinutes, faceOffPct, shotPct, gameWinningGoals, overTimeGoals, shortHandedGoals, shortHandedPoints, shortHandedTimeOnIce, blocked, plusMinus, points, shifts, timeOnIcePerGame, evenTimeOnIcePerGame, shortHandedTimeOnIcePerGame, powerPlayTimeOnIcePerGame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "players_2017_2018_stats = []\n",
    "for i in range(0, len(players_2017_2018)): \n",
    "    stats = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                       + str(players_2017_2018[i][0]) \n",
    "                       + '/stats?stats=statsSingleSeason&season=20172018').json()\n",
    "    stats = stats['stats'][0]['splits'][0]['stat']\n",
    "    stats_array = []\n",
    "    stats_array.append(stats['timeOnIce'])\n",
    "    stats_array.append(stats['assists'])\n",
    "    stats_array.append(stats['goals'])\n",
    "    stats_array.append(stats['pim'])\n",
    "    stats_array.append(stats['shots'])\n",
    "    stats_array.append(stats['games'])\n",
    "    stats_array.append(stats['hits'])\n",
    "    stats_array.append(stats['powerPlayGoals'])\n",
    "    stats_array.append(stats['powerPlayPoints'])\n",
    "    stats_array.append(stats['powerPlayTimeOnIce'])\n",
    "    stats_array.append(stats['evenTimeOnIce'])\n",
    "    stats_array.append(stats['penaltyMinutes'])\n",
    "    stats_array.append(stats['faceOffPct'])\n",
    "    stats_array.append(stats['shotPct'])\n",
    "    stats_array.append(stats['gameWinningGoals'])\n",
    "    stats_array.append(stats['overTimeGoals'])\n",
    "    stats_array.append(stats['shortHandedGoals'])\n",
    "    stats_array.append(stats['shortHandedPoints'])\n",
    "    stats_array.append(stats['shortHandedTimeOnIce'])\n",
    "    stats_array.append(stats['blocked'])\n",
    "    stats_array.append(stats['plusMinus'])\n",
    "    stats_array.append(stats['points'])\n",
    "    stats_array.append(stats['shifts'])\n",
    "    stats_array.append(stats['timeOnIcePerGame'])\n",
    "    stats_array.append(stats['evenTimeOnIcePerGame'])\n",
    "    stats_array.append(stats['shortHandedTimeOnIcePerGame'])\n",
    "    stats_array.append(stats['powerPlayTimeOnIcePerGame'])\n",
    "    players_2017_2018_stats.append(stats_array)\n",
    "\n",
    "print(len(players_2017_2018_stats))\n",
    "print(len(players_2017_2018_stats[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the stats for each player to their id, name, and team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skaters_2017_2018 = []\n",
    "for i in range(0, len(players_2017_2018)):\n",
    "    skaters_2017_2018.append(players_2017_2018[i] + players_2017_2018_stats[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(skaters_2017_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the skaters array as csv data for anaylsis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('data/skaters_2017_2018.csv', skaters_2017_2018, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv(y1, y2):\n",
    "    team_rosters = requests.get('https://statsapi.web.nhl.com/api/v1/teams?expand=team.roster&season=' + y1 + y2)\n",
    "    #print('https://statsapi.web.nhl.com/api/v1/teams?expand=team.roster&season=' + y1 + y2)\n",
    "    team_rosters = team_rosters.json()\n",
    "    players= []\n",
    "    for i in range(0, len(team_rosters['teams'])):\n",
    "        #print(team_rosters_2017_2018['teams'][i]['name'])\n",
    "        for j in range(0, len(team_rosters['teams'][i]['roster']['roster'])):\n",
    "            #print(team_rosters_2017_2018['teams'][i]['roster']['roster'][j]['person'])\n",
    "            player = [team_rosters['teams'][i]['roster']['roster'][j]['person']['id'], \n",
    "                      team_rosters['teams'][i]['roster']['roster'][j]['person']['fullName'],\n",
    "                     team_rosters['teams'][i]['name']]\n",
    "            if (team_rosters['teams'][i]['roster']['roster'][j]['position']['code'] != 'G'):\n",
    "                players.append(player)\n",
    "    players_stats = []\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'fullName', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "    #print(labels)\n",
    "    #print(header)\n",
    "    for i in range(0, len(players)): \n",
    "        #print(str(players[i][0]))\n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                           + str(players[i][0]) \n",
    "                           + '/stats?stats=statsSingleSeason&season=' + y1 + y2).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            players_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        \n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        players_stats.append(stats_array)\n",
    "        \n",
    "    skaters = []\n",
    "    skaters.append(header)\n",
    "    for i in range(0, len(players)):\n",
    "        skaters.append(players[i] + players_stats[i])\n",
    "    return skaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skaters = get_csv('2016', '2017')\n",
    "np.savetxt('data/skaters_2016_2017.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skaters = get_csv('2015', '2016')\n",
    "np.savetxt('data/skaters_2015_2016.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skaters = get_csv('2014', '2015')\n",
    "np.savetxt('data/skaters_2014_2015.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skaters = get_csv('1970', '1971')\n",
    "np.savetxt('data/skaters_1970_1971.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = requests.get('https://statsapi.web.nhl.com/api/v1/people/' \n",
    "                       + str(8445135) \n",
    "                       + '/stats?stats=statsSingleSeason&season=19171918').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats = stats = stats['stats'][0]['splits'][0]['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_stats = []\n",
    "for x in stats:\n",
    "    player_stats.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        skaters = get_csv(str(i), str(i+1))\n",
    "        np.savetxt('data/skaters_' + str(i) + '_' + str(i+1) + '.csv', skaters, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data(2005,2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Get Team data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = requests.get('https://statsapi.web.nhl.com/api/v1/teams?season=20172018')\n",
    "teams = teams.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_id_name = []\n",
    "for i in range(0, len(teams['teams'])):\n",
    "    team_arr = [teams['teams'][i]['id'], teams['teams'][i]['name']]\n",
    "    team_id_name.append(team_arr)\n",
    "#print(team_id_name)\n",
    "\n",
    "labels = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                           + str(team_id_name[0][0])\n",
    "                           + '/stats?stats=statsSingleSeason&season=19901991').json()\n",
    "labels = labels['stats'][0]['splits'][0]['stat']\n",
    "header = ['id', 'teamName']\n",
    "for label in labels:\n",
    "    header.append(label)\n",
    "#print(header)\n",
    "header.append('PDO')\n",
    "\n",
    "team_stats = []\n",
    "for i in range(0, len(team_id_name)):\n",
    "    stats = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                         + str(team_id_name[i][0]) \n",
    "                         + '/stats?stats=statsSingleSeason&season=19901991').json()\n",
    "    if(stats['stats'][0]['splits'] == []):\n",
    "        team_stats.append([0] * len(labels))\n",
    "        continue\n",
    "    stats = stats['stats'][0]['splits'][0]['stat']\n",
    "    PDO = stats['shootingPctg'] + stats['savePctg']\n",
    "    stats_array = []\n",
    "    for label in labels:\n",
    "        if label in stats:\n",
    "            stats_array.append(stats[label])\n",
    "        elif label == 'PDO':\n",
    "            stats_array.append(PDO)\n",
    "        else:\n",
    "            stats_array.append(0)\n",
    "    team_stats.append(stats_array)\n",
    "\n",
    "    \n",
    "teams_stats_final = []\n",
    "teams_stats_final.append(header)\n",
    "for i in range(0, len(team_id_name)):\n",
    "    teams_stats_final.append(team_id_name[i] + team_stats[i])    \n",
    "\n",
    "np.savetxt('team_data/teams_19901991.csv', teams_stats_final, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_team(y1, y2):\n",
    "    teams = requests.get('https://statsapi.web.nhl.com/api/v1/teams?season=' + str(y1) + str(y2))\n",
    "    teams = teams.json()\n",
    "    team_id_name = []\n",
    "    for i in range(0, len(teams['teams'])):\n",
    "        team_arr = [teams['teams'][i]['id'], teams['teams'][i]['name']]\n",
    "        team_id_name.append(team_arr)\n",
    "\n",
    "    labels = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                           + str(team_id_name[0][0])\n",
    "                           + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "    labels = labels['stats'][0]['splits'][0]['stat']\n",
    "    header = ['id', 'teamName']\n",
    "    for label in labels:\n",
    "        header.append(label)\n",
    "\n",
    "    team_stats = []\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        stats = requests.get('https://statsapi.web.nhl.com/api/v1/teams/' \n",
    "                             + str(team_id_name[i][0]) \n",
    "                             + '/stats?stats=statsSingleSeason&season=' + str(y1) + str(y2)).json()\n",
    "        if(stats['stats'][0]['splits'] == []):\n",
    "            team_stats.append([0] * len(labels))\n",
    "            continue\n",
    "        stats = stats['stats'][0]['splits'][0]['stat']\n",
    "        stats_array = []\n",
    "        for label in labels:\n",
    "            if label in stats:\n",
    "                stats_array.append(stats[label])\n",
    "            else:\n",
    "                stats_array.append(0)\n",
    "        team_stats.append(stats_array)\n",
    "\n",
    "    \n",
    "    teams_stats_final = []\n",
    "    teams_stats_final.append(header)\n",
    "    for i in range(0, len(team_id_name)):\n",
    "        teams_stats_final.append(team_id_name[i] + team_stats[i]) \n",
    "    return teams_stats_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_team(str(i), str(i+1))\n",
    "        np.savetxt('team_data/teams_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "games = requests.get('https://statsapi.web.nhl.com/api/v1/schedule?startDate=2017-10-04&endDate=2018-06-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "games = games.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_data_list = []\n",
    "for i in range(2000,2017):\n",
    "    if(i == 2004):\n",
    "        continue\n",
    "    print(\"getting \" + str(i))\n",
    "    team_data_list.append(get_csv_team(i, i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_team_stats(id, team_data):\n",
    "    for i in range(0, len(team_data)):\n",
    "        if team_data[i][0] == id:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_game(y1, y2):\n",
    "    team_data = get_csv_team(y1, y2)\n",
    "    header = team_data[0][3:]\n",
    "    header = header *2\n",
    "    # print(header)\n",
    "    games_data = [['winner', 'awayID', 'homeID'] + header]\n",
    "    games = requests.get('https://statsapi.web.nhl.com/api/v1/schedule?startDate=' \n",
    "                         + str(y1) + '-10-01&endDate=' + str(y2) + '-06-30')\n",
    "    games = games.json()\n",
    "    for date in games['dates']:\n",
    "        for game in date['games']:\n",
    "            away_ID = game['teams']['away']['team']['id']\n",
    "            home_ID = game['teams']['home']['team']['id']\n",
    "            if away_ID > 80 or home_ID > 80:\n",
    "                continue\n",
    "            \n",
    "            away_score = game['teams']['away']['score']\n",
    "            home_score = game['teams']['home']['score']\n",
    "            winner = 0\n",
    "            away_stats = team_data[get_team_stats(away_ID, team_data)][3:]\n",
    "            home_stats = team_data[get_team_stats(home_ID, team_data)][3:]\n",
    "            if home_score > away_score:\n",
    "                winner = 1\n",
    "            games_data.append([winner,\n",
    "                          away_ID, \n",
    "                          home_ID] +\n",
    "                          away_stats +\n",
    "                          home_stats)\n",
    "    return games_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_csv_current_game(date1, date2, y1, y2):\n",
    "    team_data = get_csv_team(y1, y2)\n",
    "    header = team_data[0][3:]\n",
    "    header = header *2\n",
    "    # print(header)\n",
    "    games_data = [['winner', 'awayID', 'homeID'] + header]\n",
    "    games = requests.get('https://statsapi.web.nhl.com/api/v1/schedule?startDate=' \n",
    "                         +date1 + '&endDate=' + date2)\n",
    "    games = games.json()\n",
    "    for date in games['dates']:\n",
    "        for game in date['games']:\n",
    "            away_ID = game['teams']['away']['team']['id']\n",
    "            home_ID = game['teams']['home']['team']['id']\n",
    "            if away_ID > 80 or home_ID > 80:\n",
    "                continue\n",
    "            \n",
    "            away_score = game['teams']['away']['score']\n",
    "            home_score = game['teams']['home']['score']\n",
    "            winner = 0\n",
    "            away_stats = team_data[get_team_stats(away_ID, team_data)][3:]\n",
    "            home_stats = team_data[get_team_stats(home_ID, team_data)][3:]\n",
    "            if home_score > away_score:\n",
    "                winner = 1\n",
    "            games_data.append([winner,\n",
    "                          away_ID, \n",
    "                          home_ID] +\n",
    "                          away_stats +\n",
    "                          home_stats)\n",
    "    return games_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_data = get_csv_game(2017, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_game_data(start, end):\n",
    "    for i in range(start, end):\n",
    "        if(i == 2004):\n",
    "            continue\n",
    "        print(\"Getting data for \" + str(i) + \"-\" + str(i+1) + \" season.\")\n",
    "        data = get_csv_game(i, i+1)\n",
    "        np.savetxt('game_data/game_data_' + str(i) + '_' + str(i+1) + '.csv', data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for 2017-2018 season.\n"
     ]
    }
   ],
   "source": [
    "get_game_data(2017,2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = get_csv_current_game('2018-11-19', '2018-11-19', 2018,2019)\n",
    "np.savetxt('current_data_2018_2019.csv', current_data, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2000_2001 = pd.read_csv('game_data/game_data_2000_2001.csv', header=0)\n",
    "data_2000_2001.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "# data_2004_2005 = pd.read_csv('game_data/game_data_2004_2005.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "frames = [data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, data_2006_2007, data_2007_2008,\n",
    "        data_2008_2009, data_2009_2010, data_2010_2011, data_2011_2012, data_2012_2013, data_2013_2014,\n",
    "        data_2014_2015, data_2015_2016, data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>winner</th>\n",
       "      <th>awayID</th>\n",
       "      <th>homeID</th>\n",
       "      <th>wins</th>\n",
       "      <th>losses</th>\n",
       "      <th>ot</th>\n",
       "      <th>pts</th>\n",
       "      <th>ptPctg</th>\n",
       "      <th>goalsPerGame</th>\n",
       "      <th>goalsAgainstPerGame</th>\n",
       "      <th>...</th>\n",
       "      <th>winLeadFirstPer.1</th>\n",
       "      <th>winLeadSecondPer.1</th>\n",
       "      <th>winOutshootOpp.1</th>\n",
       "      <th>winOutshotByOpp.1</th>\n",
       "      <th>faceOffsTaken.1</th>\n",
       "      <th>faceOffsWon.1</th>\n",
       "      <th>faceOffsLost.1</th>\n",
       "      <th>faceOffWinPercentage.1</th>\n",
       "      <th>shootingPctg.1</th>\n",
       "      <th>savePctg.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>94</td>\n",
       "      <td>57.3</td>\n",
       "      <td>2.963</td>\n",
       "      <td>2.537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.478</td>\n",
       "      <td>5281.0</td>\n",
       "      <td>2736.0</td>\n",
       "      <td>2545.0</td>\n",
       "      <td>51.8</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>60.4</td>\n",
       "      <td>2.585</td>\n",
       "      <td>2.061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.289</td>\n",
       "      <td>5484.0</td>\n",
       "      <td>2583.0</td>\n",
       "      <td>2901.0</td>\n",
       "      <td>47.1</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>56.1</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.429</td>\n",
       "      <td>5487.0</td>\n",
       "      <td>2883.0</td>\n",
       "      <td>2604.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>32.9</td>\n",
       "      <td>2.280</td>\n",
       "      <td>3.512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5468.0</td>\n",
       "      <td>2591.0</td>\n",
       "      <td>2877.0</td>\n",
       "      <td>47.4</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>87</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.524</td>\n",
       "      <td>2.549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.500</td>\n",
       "      <td>5342.0</td>\n",
       "      <td>2677.0</td>\n",
       "      <td>2665.0</td>\n",
       "      <td>50.1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   winner  awayID  homeID  wins  losses  ot  pts  ptPctg  goalsPerGame  \\\n",
       "0       0       9      10    39      27   7   94    57.3         2.963   \n",
       "1       0      21       5    45      28   1   99    60.4         2.585   \n",
       "2       1      22      20    38      28   4   92    56.1         2.500   \n",
       "3       0      11       7    19      47   5   54    32.9         2.280   \n",
       "4       0       8       9    36      31   3   87    53.0         2.524   \n",
       "\n",
       "   goalsAgainstPerGame     ...      winLeadFirstPer.1  winLeadSecondPer.1  \\\n",
       "0                2.537     ...                  0.727               0.846   \n",
       "1                2.061     ...                  0.579               0.800   \n",
       "2                2.220     ...                  0.667               0.778   \n",
       "3                3.512     ...                  0.657               0.829   \n",
       "4                2.549     ...                  0.815               0.909   \n",
       "\n",
       "   winOutshootOpp.1  winOutshotByOpp.1  faceOffsTaken.1  faceOffsWon.1  \\\n",
       "0             0.544              0.478           5281.0         2736.0   \n",
       "1             0.364              0.289           5484.0         2583.0   \n",
       "2             0.314              0.429           5487.0         2883.0   \n",
       "3             0.372              0.500           5468.0         2591.0   \n",
       "4             0.444              0.500           5342.0         2677.0   \n",
       "\n",
       "   faceOffsLost.1  faceOffWinPercentage.1  shootingPctg.1  savePctg.1  \n",
       "0          2545.0                    51.8            10.6       0.899  \n",
       "1          2901.0                    47.1             8.9       0.896  \n",
       "2          2604.0                    52.5             9.1       0.902  \n",
       "3          2877.0                    47.4             9.3       0.906  \n",
       "4          2665.0                    50.1             9.8       0.902  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_regression = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "data_regression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare(data):\n",
    "    X = data.iloc[:,3:].values\n",
    "\n",
    "    # we normalize X\n",
    "    maxX = np.max(X, axis=0)\n",
    "    minX = np.min(X, axis=0)\n",
    "    X = (X-minX)/(maxX-minX)\n",
    "\n",
    "    # we insert an all-ones column at index 0\n",
    "    X = np.insert(X, 0, 1, axis=1)\n",
    "    \n",
    "    # get the first column of the data\n",
    "    y = data.iloc[:,0:1].values\n",
    "    \n",
    "    # we normalize y\n",
    "#     maxy = np.max(y, axis=0)\n",
    "#     miny = np.min(y, axis=0)\n",
    "#     y = (y-miny)/(maxy-miny)\n",
    "\n",
    "    #where_are_zeros = (y==0)\n",
    "    #y[where_are_zeros] = -1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.55813953  0.40816327 ...,  0.62295082  0.68518519\n",
      "   0.30612245]\n",
      " [ 1.          0.69767442  0.42857143 ...,  0.23770492  0.37037037\n",
      "   0.24489796]\n",
      " [ 1.          0.53488372  0.42857143 ...,  0.68032787  0.40740741\n",
      "   0.36734694]\n",
      " ..., \n",
      " [ 1.          0.8372093   0.34693878 ...,  0.50819672  0.72222222\n",
      "   0.51020408]\n",
      " [ 1.          0.8372093   0.34693878 ...,  0.50819672  0.72222222\n",
      "   0.51020408]\n",
      " [ 1.          0.79069767  0.3877551  ...,  0.3852459   0.57407407\n",
      "   0.55102041]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "X,y = prepare(data)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def error(x,y,w):\n",
    "    return np.log(1+np.exp(-y*x@w.T))\n",
    "\n",
    "#TODO\n",
    "def error_mean(X,y,w):\n",
    "    return sum(error(X,y,w))/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "def grad(x,y,w):\n",
    "    return (y*x)/(1+np.exp(y*x@w.T))\n",
    "\n",
    "#TODO\n",
    "def grad_mean(X,y,w):\n",
    "    return -1/len(y)*sum(grad(X,y,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X,y,kappa,iter):\n",
    "    w = np.zeros((1,X.shape[1]))\n",
    "    E = []\n",
    "\n",
    "    #TODO\n",
    "    for i in range(0,iter):\n",
    "        E.append(error_mean(X,y,w))\n",
    "        w = w - kappa*grad_mean(X,y,w)\n",
    "    return w,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44179127  0.21724907  0.22187973  0.2090081   0.23465511  0.20487313\n",
      "   0.18484386  0.20935072  0.17728012  0.1984212   0.16704599  0.16171045\n",
      "   0.18372647  0.23014475  0.19971897  0.24038998  0.25724557  0.17094096\n",
      "   0.26463646  0.28425597  0.22787552  0.19490114  0.29949662  0.25959253\n",
      "   0.28536631  0.20480852  0.1781172   0.20285203  0.24129449  0.20336457\n",
      "   0.20172251  0.25671415  0.22934756  0.19965455  0.19089188  0.19935604\n",
      "   0.2079896   0.17299799  0.15517589  0.18401925  0.24111194  0.20900174\n",
      "   0.22873578  0.27602867  0.18923749  0.28035407  0.29832085  0.24695442\n",
      "   0.21868296  0.29915548  0.26106782  0.28322412  0.21065112  0.19002832\n",
      "   0.21670327]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111754e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w,E = fit(X,y,0.1,1000)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46088214027476504\n"
     ]
    }
   ],
   "source": [
    "def predict(w, X):\n",
    "    pred = 1/(1+np.exp(X@-w.T))\n",
    "    for n in range(0, len(pred)):\n",
    "        if(pred[n] < 0.5):\n",
    "            pred[n] = 0\n",
    "        else:\n",
    "            pred[n] = 1\n",
    "    return pred\n",
    "#TODO\n",
    "def accuracy(y,y_pred):\n",
    "    acc = 0\n",
    "    for n in range(0,len(y)):\n",
    "        if(y[n] == y_pred[n]):\n",
    "            acc = acc +1\n",
    "    return 1-((acc)/len(y))\n",
    "\n",
    "y_pred = predict(w,X)\n",
    "#print(y_pred)\n",
    "print( accuracy(y,y_pred) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_train_test(X,y,pct=80):\n",
    "    n = X.shape[0]\n",
    "    s = round(n * pct / 100)\n",
    "    \n",
    "    indices = np.random.permutation(n)\n",
    "    train_idx, test_idx = indices[:s], indices[s:]\n",
    "    \n",
    "    X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "    y_train, y_test = y[train_idx,:], y[test_idx,:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28126102  0.14575102  0.1413503   0.1349559   0.15682372  0.1341228\n",
      "   0.12101843  0.13192448  0.11591613  0.12811646  0.11063836  0.10549427\n",
      "   0.12090564  0.14749145  0.13004945  0.15305692  0.16621748  0.11207552\n",
      "   0.17070664  0.18358641  0.14768175  0.12803449  0.19728034  0.17098928\n",
      "   0.18678845  0.13186317  0.11548497  0.13091466  0.16091538  0.12963908\n",
      "   0.1304425   0.1707546   0.14935317  0.13063432  0.12081911  0.129715\n",
      "   0.13474308  0.11482949  0.10131884  0.12109188  0.1542392   0.13578774\n",
      "   0.14600008  0.17807992  0.12324371  0.1805001   0.19249329  0.15963847\n",
      "   0.14300728  0.19703328  0.1716425   0.18567748  0.13469403  0.12335148\n",
      "   0.13923878]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1114a8438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4683056158110388\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = split_train_test(X,y,pct=80)\n",
    "w,E = fit(X_train,y_train,0.1,100)\n",
    "print(w)\n",
    "plt.plot(E)\n",
    "plt.show()\n",
    "y_pred = predict(w,X_test)\n",
    "print( accuracy(y_test,y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression binary classifier in Tensorflow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(A, Y):\n",
    "    P = A<.5      #prediction\n",
    "    num_agreements = np.sum(P==Y)\n",
    "    return num_agreements / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,Y,X_test,Y_test = split_train_test(X,y,pct=80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (13277, 55) (13277, 1)\n",
      "Test dataset shape (3319, 55) (3319, 1)\n",
      "Y = [[1]\n",
      " [1]\n",
      " [1]\n",
      " ..., \n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# We will reshape the Y arrays so that they are not rank 1 arrays but rank 2 arrays. \n",
    "# They should be rank 2 arrays.\n",
    "\n",
    "Y = Y.reshape((Y.shape[0],1))\n",
    "Y_test = Y_test.reshape((Y_test.shape[0],1))\n",
    "\n",
    "print(\"Train dataset shape\", X.shape, Y.shape)\n",
    "print(\"Test dataset shape\", X_test.shape, Y_test.shape)\n",
    "\n",
    "print(\"Y =\", Y)\n",
    "\n",
    "m   = X.shape[0] \n",
    "n_x = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable(tf.zeros((n_x, 1)))\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 0.693132\n",
      "1 0.690645\n",
      "2 0.689561\n",
      "3 0.688933\n",
      "4 0.688454\n",
      "5 0.688025\n",
      "6 0.687617\n",
      "7 0.687221\n",
      "8 0.686831\n",
      "9 0.686448\n",
      "10 0.68607\n",
      "11 0.685698\n",
      "12 0.685333\n",
      "13 0.684971\n",
      "14 0.684615\n",
      "15 0.684265\n",
      "16 0.68392\n",
      "17 0.68358\n",
      "18 0.683246\n",
      "19 0.682914\n",
      "20 0.68259\n",
      "21 0.682269\n",
      "22 0.681953\n",
      "23 0.681643\n",
      "24 0.681336\n",
      "25 0.681033\n",
      "26 0.680736\n",
      "27 0.680443\n",
      "28 0.680154\n",
      "29 0.67987\n",
      "30 0.679589\n",
      "31 0.679313\n",
      "32 0.67904\n",
      "33 0.678773\n",
      "34 0.678508\n",
      "35 0.678247\n",
      "36 0.677992\n",
      "37 0.677739\n",
      "38 0.677489\n",
      "39 0.677244\n",
      "40 0.677001\n",
      "41 0.676763\n",
      "42 0.676528\n",
      "43 0.676296\n",
      "44 0.676068\n",
      "45 0.675843\n",
      "46 0.675621\n",
      "47 0.675404\n",
      "48 0.675188\n",
      "49 0.674977\n",
      "50 0.674768\n",
      "51 0.674561\n",
      "52 0.674359\n",
      "53 0.674158\n",
      "54 0.673961\n",
      "55 0.673767\n",
      "56 0.673576\n",
      "57 0.673387\n",
      "58 0.673201\n",
      "59 0.673017\n",
      "60 0.672837\n",
      "61 0.672659\n",
      "62 0.672484\n",
      "63 0.672311\n",
      "64 0.67214\n",
      "65 0.671972\n",
      "66 0.671806\n",
      "67 0.671644\n",
      "68 0.671482\n",
      "69 0.671324\n",
      "70 0.671166\n",
      "71 0.671013\n",
      "72 0.670861\n",
      "73 0.670711\n",
      "74 0.670563\n",
      "75 0.670418\n",
      "76 0.670275\n",
      "77 0.670133\n",
      "78 0.669994\n",
      "79 0.669856\n",
      "80 0.669721\n",
      "81 0.669587\n",
      "82 0.669455\n",
      "83 0.669325\n",
      "84 0.669198\n",
      "85 0.669071\n",
      "86 0.668947\n",
      "87 0.668824\n",
      "88 0.668704\n",
      "89 0.668584\n",
      "90 0.668466\n",
      "91 0.668351\n",
      "92 0.668236\n",
      "93 0.668124\n",
      "94 0.668013\n",
      "95 0.667903\n",
      "96 0.667795\n",
      "97 0.667689\n",
      "98 0.667584\n",
      "99 0.667481\n",
      "100 0.667378\n",
      "101 0.667277\n",
      "102 0.667178\n",
      "103 0.66708\n",
      "104 0.666984\n",
      "105 0.666888\n",
      "106 0.666795\n",
      "107 0.666702\n",
      "108 0.666611\n",
      "109 0.66652\n",
      "110 0.666431\n",
      "111 0.666344\n",
      "112 0.666258\n",
      "113 0.666173\n",
      "114 0.666089\n",
      "115 0.666006\n",
      "116 0.665924\n",
      "117 0.665844\n",
      "118 0.665764\n",
      "119 0.665686\n",
      "120 0.665608\n",
      "121 0.665532\n",
      "122 0.665457\n",
      "123 0.665382\n",
      "124 0.665309\n",
      "125 0.665238\n",
      "126 0.665166\n",
      "127 0.665095\n",
      "128 0.665026\n",
      "129 0.664958\n",
      "130 0.664891\n",
      "131 0.664824\n",
      "132 0.664759\n",
      "133 0.664694\n",
      "134 0.66463\n",
      "135 0.664568\n",
      "136 0.664505\n",
      "137 0.664443\n",
      "138 0.664383\n",
      "139 0.664323\n",
      "140 0.664265\n",
      "141 0.664207\n",
      "142 0.664149\n",
      "143 0.664093\n",
      "144 0.664037\n",
      "145 0.663982\n",
      "146 0.663927\n",
      "147 0.663874\n",
      "148 0.663821\n",
      "149 0.663769\n",
      "150 0.663718\n",
      "151 0.663667\n",
      "152 0.663616\n",
      "153 0.663567\n",
      "154 0.663519\n",
      "155 0.66347\n",
      "156 0.663423\n",
      "157 0.663375\n",
      "158 0.66333\n",
      "159 0.663284\n",
      "160 0.663239\n",
      "161 0.663195\n",
      "162 0.663151\n",
      "163 0.663107\n",
      "164 0.663065\n",
      "165 0.663023\n",
      "166 0.662981\n",
      "167 0.662939\n",
      "168 0.662899\n",
      "169 0.662859\n",
      "170 0.66282\n",
      "171 0.66278\n",
      "172 0.662743\n",
      "173 0.662704\n",
      "174 0.662667\n",
      "175 0.662629\n",
      "176 0.662594\n",
      "177 0.662557\n",
      "178 0.662522\n",
      "179 0.662487\n",
      "180 0.662452\n",
      "181 0.662419\n",
      "182 0.662384\n",
      "183 0.662351\n",
      "184 0.662318\n",
      "185 0.662286\n",
      "186 0.662254\n",
      "187 0.662222\n",
      "188 0.66219\n",
      "189 0.662159\n",
      "190 0.662129\n",
      "191 0.662099\n",
      "192 0.66207\n",
      "193 0.66204\n",
      "194 0.662011\n",
      "195 0.661983\n",
      "196 0.661954\n",
      "197 0.661927\n",
      "198 0.661898\n",
      "199 0.661872\n",
      "200 0.661845\n",
      "201 0.661818\n",
      "202 0.661793\n",
      "203 0.661766\n",
      "204 0.661742\n",
      "205 0.661716\n",
      "206 0.661691\n",
      "207 0.661667\n",
      "208 0.661643\n",
      "209 0.661618\n",
      "210 0.661594\n",
      "211 0.661571\n",
      "212 0.661548\n",
      "213 0.661526\n",
      "214 0.661503\n",
      "215 0.661481\n",
      "216 0.661459\n",
      "217 0.661438\n",
      "218 0.661416\n",
      "219 0.661395\n",
      "220 0.661374\n",
      "221 0.661353\n",
      "222 0.661333\n",
      "223 0.661313\n",
      "224 0.661293\n",
      "225 0.661274\n",
      "226 0.661254\n",
      "227 0.661235\n",
      "228 0.661216\n",
      "229 0.661197\n",
      "230 0.661179\n",
      "231 0.661161\n",
      "232 0.661143\n",
      "233 0.661125\n",
      "234 0.661108\n",
      "235 0.66109\n",
      "236 0.661073\n",
      "237 0.661056\n",
      "238 0.661039\n",
      "239 0.661023\n",
      "240 0.661006\n",
      "241 0.66099\n",
      "242 0.660974\n",
      "243 0.660958\n",
      "244 0.660943\n",
      "245 0.660928\n",
      "246 0.660913\n",
      "247 0.660897\n",
      "248 0.660882\n",
      "249 0.660868\n",
      "250 0.660853\n",
      "251 0.660839\n",
      "252 0.660825\n",
      "253 0.660811\n",
      "254 0.660797\n",
      "255 0.660783\n",
      "256 0.66077\n",
      "257 0.660757\n",
      "258 0.660743\n",
      "259 0.660731\n",
      "260 0.660718\n",
      "261 0.660705\n",
      "262 0.660693\n",
      "263 0.66068\n",
      "264 0.660667\n",
      "265 0.660656\n",
      "266 0.660644\n",
      "267 0.660631\n",
      "268 0.66062\n",
      "269 0.660608\n",
      "270 0.660597\n",
      "271 0.660586\n",
      "272 0.660575\n",
      "273 0.660564\n",
      "274 0.660553\n",
      "275 0.660542\n",
      "276 0.660532\n",
      "277 0.660521\n",
      "278 0.66051\n",
      "279 0.6605\n",
      "280 0.660489\n",
      "281 0.66048\n",
      "282 0.660469\n",
      "283 0.66046\n",
      "284 0.660451\n",
      "285 0.66044\n",
      "286 0.660431\n",
      "287 0.660421\n",
      "288 0.660412\n",
      "289 0.660403\n",
      "290 0.660394\n",
      "291 0.660385\n",
      "292 0.660376\n",
      "293 0.660367\n",
      "294 0.660358\n",
      "295 0.66035\n",
      "296 0.660341\n",
      "297 0.660333\n",
      "298 0.660324\n",
      "299 0.660316\n",
      "300 0.660308\n",
      "301 0.6603\n",
      "302 0.660292\n",
      "303 0.660284\n",
      "304 0.660276\n",
      "305 0.660268\n",
      "306 0.660261\n",
      "307 0.660253\n",
      "308 0.660245\n",
      "309 0.660238\n",
      "310 0.660231\n",
      "311 0.660223\n",
      "312 0.660217\n",
      "313 0.66021\n",
      "314 0.660203\n",
      "315 0.660196\n",
      "316 0.66019\n",
      "317 0.660182\n",
      "318 0.660175\n",
      "319 0.660169\n",
      "320 0.660162\n",
      "321 0.660155\n",
      "322 0.66015\n",
      "323 0.660143\n",
      "324 0.660136\n",
      "325 0.66013\n",
      "326 0.660124\n",
      "327 0.660118\n",
      "328 0.660112\n",
      "329 0.660106\n",
      "330 0.6601\n",
      "331 0.660094\n",
      "332 0.660088\n",
      "333 0.660082\n",
      "334 0.660078\n",
      "335 0.660072\n",
      "336 0.660066\n",
      "337 0.66006\n",
      "338 0.660055\n",
      "339 0.660049\n",
      "340 0.660044\n",
      "341 0.660039\n",
      "342 0.660034\n",
      "343 0.660029\n",
      "344 0.660024\n",
      "345 0.660018\n",
      "346 0.660013\n",
      "347 0.660008\n",
      "348 0.660003\n",
      "349 0.659998\n",
      "350 0.659994\n",
      "351 0.659989\n",
      "352 0.659984\n",
      "353 0.65998\n",
      "354 0.659975\n",
      "355 0.65997\n",
      "356 0.659966\n",
      "357 0.659961\n",
      "358 0.659957\n",
      "359 0.659952\n",
      "360 0.659948\n",
      "361 0.659943\n",
      "362 0.65994\n",
      "363 0.659935\n",
      "364 0.65993\n",
      "365 0.659927\n",
      "366 0.659922\n",
      "367 0.659919\n",
      "368 0.659914\n",
      "369 0.65991\n",
      "370 0.659906\n",
      "371 0.659902\n",
      "372 0.659898\n",
      "373 0.659895\n",
      "374 0.659891\n",
      "375 0.659886\n",
      "376 0.659883\n",
      "377 0.65988\n",
      "378 0.659876\n",
      "379 0.659872\n",
      "380 0.659868\n",
      "381 0.659866\n",
      "382 0.659861\n",
      "383 0.659858\n",
      "384 0.659855\n",
      "385 0.659851\n",
      "386 0.659847\n",
      "387 0.659844\n",
      "388 0.659841\n",
      "389 0.659837\n",
      "390 0.659834\n",
      "391 0.659831\n",
      "392 0.659828\n",
      "393 0.659824\n",
      "394 0.659821\n",
      "395 0.659818\n",
      "396 0.659815\n",
      "397 0.659812\n",
      "398 0.659809\n",
      "399 0.659806\n",
      "400 0.659803\n",
      "401 0.6598\n",
      "402 0.659797\n",
      "403 0.659794\n",
      "404 0.659791\n",
      "405 0.659788\n",
      "406 0.659785\n",
      "407 0.659783\n",
      "408 0.659779\n",
      "409 0.659777\n",
      "410 0.659773\n",
      "411 0.659771\n",
      "412 0.659769\n",
      "413 0.659765\n",
      "414 0.659763\n",
      "415 0.659759\n",
      "416 0.659758\n",
      "417 0.659755\n",
      "418 0.659752\n",
      "419 0.65975\n",
      "420 0.659747\n",
      "421 0.659745\n",
      "422 0.659742\n",
      "423 0.659739\n",
      "424 0.659737\n",
      "425 0.659734\n",
      "426 0.659732\n",
      "427 0.659729\n",
      "428 0.659727\n",
      "429 0.659725\n",
      "430 0.659722\n",
      "431 0.65972\n",
      "432 0.659718\n",
      "433 0.659715\n",
      "434 0.659714\n",
      "435 0.659711\n",
      "436 0.659709\n",
      "437 0.659707\n",
      "438 0.659704\n",
      "439 0.659701\n",
      "440 0.6597\n",
      "441 0.659697\n",
      "442 0.659695\n",
      "443 0.659693\n",
      "444 0.659692\n",
      "445 0.659689\n",
      "446 0.659687\n",
      "447 0.659685\n",
      "448 0.659682\n",
      "449 0.659681\n",
      "450 0.659679\n",
      "451 0.659676\n",
      "452 0.659675\n",
      "453 0.659673\n",
      "454 0.659671\n",
      "455 0.659668\n",
      "456 0.659666\n",
      "457 0.659665\n",
      "458 0.659663\n",
      "459 0.659661\n",
      "460 0.659659\n",
      "461 0.659658\n",
      "462 0.659655\n",
      "463 0.659653\n",
      "464 0.659651\n",
      "465 0.65965\n",
      "466 0.659647\n",
      "467 0.659646\n",
      "468 0.659645\n",
      "469 0.659642\n",
      "470 0.659641\n",
      "471 0.659639\n",
      "472 0.659637\n",
      "473 0.659636\n",
      "474 0.659633\n",
      "475 0.659632\n",
      "476 0.659631\n",
      "477 0.659628\n",
      "478 0.659628\n",
      "479 0.659626\n",
      "480 0.659624\n",
      "481 0.659623\n",
      "482 0.65962\n",
      "483 0.659619\n",
      "484 0.659617\n",
      "485 0.659616\n",
      "486 0.659614\n",
      "487 0.659612\n",
      "488 0.659612\n",
      "489 0.659609\n",
      "490 0.659608\n",
      "491 0.659606\n",
      "492 0.659605\n",
      "493 0.659603\n",
      "494 0.659602\n",
      "495 0.6596\n",
      "496 0.659599\n",
      "497 0.659598\n",
      "498 0.659596\n",
      "499 0.659594\n",
      "500 0.659592\n",
      "501 0.659592\n",
      "502 0.659591\n",
      "503 0.659588\n",
      "504 0.659588\n",
      "505 0.659586\n",
      "506 0.659584\n",
      "507 0.659583\n",
      "508 0.659582\n",
      "509 0.65958\n",
      "510 0.659578\n",
      "511 0.659577\n",
      "512 0.659576\n",
      "513 0.659574\n",
      "514 0.659573\n",
      "515 0.659571\n",
      "516 0.659571\n",
      "517 0.659569\n",
      "518 0.659568\n",
      "519 0.659567\n",
      "520 0.659565\n",
      "521 0.659564\n",
      "522 0.659563\n",
      "523 0.659561\n",
      "524 0.659561\n",
      "525 0.659559\n",
      "526 0.659557\n",
      "527 0.659556\n",
      "528 0.659555\n",
      "529 0.659554\n",
      "530 0.659553\n",
      "531 0.659551\n",
      "532 0.65955\n",
      "533 0.659549\n",
      "534 0.659547\n",
      "535 0.659546\n",
      "536 0.659545\n",
      "537 0.659544\n",
      "538 0.659543\n",
      "539 0.659541\n",
      "540 0.65954\n",
      "541 0.659539\n",
      "542 0.659538\n",
      "543 0.659537\n",
      "544 0.659536\n",
      "545 0.659534\n",
      "546 0.659533\n",
      "547 0.659532\n",
      "548 0.659532\n",
      "549 0.659531\n",
      "550 0.659528\n",
      "551 0.659528\n",
      "552 0.659527\n",
      "553 0.659526\n",
      "554 0.659525\n",
      "555 0.659523\n",
      "556 0.659522\n",
      "557 0.659521\n",
      "558 0.65952\n",
      "559 0.659519\n",
      "560 0.659518\n",
      "561 0.659517\n",
      "562 0.659515\n",
      "563 0.659514\n",
      "564 0.659513\n",
      "565 0.659513\n",
      "566 0.659511\n",
      "567 0.659511\n",
      "568 0.659509\n",
      "569 0.659509\n",
      "570 0.659508\n",
      "571 0.659507\n",
      "572 0.659506\n",
      "573 0.659505\n",
      "574 0.659503\n",
      "575 0.659502\n",
      "576 0.659501\n",
      "577 0.6595\n",
      "578 0.659499\n",
      "579 0.659499\n",
      "580 0.659498\n",
      "581 0.659496\n",
      "582 0.659496\n",
      "583 0.659495\n",
      "584 0.659494\n",
      "585 0.659493\n",
      "586 0.659492\n",
      "587 0.659491\n",
      "588 0.65949\n",
      "589 0.659489\n",
      "590 0.659488\n",
      "591 0.659487\n",
      "592 0.659486\n",
      "593 0.659485\n",
      "594 0.659484\n",
      "595 0.659483\n",
      "596 0.659483\n",
      "597 0.659481\n",
      "598 0.65948\n",
      "599 0.659479\n",
      "600 0.659478\n",
      "601 0.659478\n",
      "602 0.659477\n",
      "603 0.659477\n",
      "604 0.659476\n",
      "605 0.659475\n",
      "606 0.659473\n",
      "607 0.659473\n",
      "608 0.659472\n",
      "609 0.659471\n",
      "610 0.65947\n",
      "611 0.659469\n",
      "612 0.659468\n",
      "613 0.659467\n",
      "614 0.659466\n",
      "615 0.659465\n",
      "616 0.659464\n",
      "617 0.659464\n",
      "618 0.659463\n",
      "619 0.659462\n",
      "620 0.659461\n",
      "621 0.659461\n",
      "622 0.65946\n",
      "623 0.659458\n",
      "624 0.659457\n",
      "625 0.659456\n",
      "626 0.659456\n",
      "627 0.659455\n",
      "628 0.659454\n",
      "629 0.659453\n",
      "630 0.659452\n",
      "631 0.659452\n",
      "632 0.659451\n",
      "633 0.659451\n",
      "634 0.65945\n",
      "635 0.659448\n",
      "636 0.659447\n",
      "637 0.659447\n",
      "638 0.659447\n",
      "639 0.659445\n",
      "640 0.659444\n",
      "641 0.659444\n",
      "642 0.659443\n",
      "643 0.659442\n",
      "644 0.659441\n",
      "645 0.65944\n",
      "646 0.65944\n",
      "647 0.659439\n",
      "648 0.659438\n",
      "649 0.659438\n",
      "650 0.659437\n",
      "651 0.659436\n",
      "652 0.659435\n",
      "653 0.659435\n",
      "654 0.659433\n",
      "655 0.659433\n",
      "656 0.659432\n",
      "657 0.659431\n",
      "658 0.659431\n",
      "659 0.65943\n",
      "660 0.659429\n",
      "661 0.659429\n",
      "662 0.659428\n",
      "663 0.659428\n",
      "664 0.659427\n",
      "665 0.659425\n",
      "666 0.659425\n",
      "667 0.659425\n",
      "668 0.659424\n",
      "669 0.659423\n",
      "670 0.659422\n",
      "671 0.659421\n",
      "672 0.65942\n",
      "673 0.65942\n",
      "674 0.659419\n",
      "675 0.659419\n",
      "676 0.659418\n",
      "677 0.659417\n",
      "678 0.659417\n",
      "679 0.659415\n",
      "680 0.659414\n",
      "681 0.659414\n",
      "682 0.659413\n",
      "683 0.659412\n",
      "684 0.659412\n",
      "685 0.659411\n",
      "686 0.659411\n",
      "687 0.65941\n",
      "688 0.659409\n",
      "689 0.659408\n",
      "690 0.659408\n",
      "691 0.659407\n",
      "692 0.659407\n",
      "693 0.659406\n",
      "694 0.659405\n",
      "695 0.659404\n",
      "696 0.659403\n",
      "697 0.659402\n",
      "698 0.659402\n",
      "699 0.659401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 0.6594\n",
      "701 0.6594\n",
      "702 0.6594\n",
      "703 0.659399\n",
      "704 0.659398\n",
      "705 0.659398\n",
      "706 0.659397\n",
      "707 0.659396\n",
      "708 0.659395\n",
      "709 0.659395\n",
      "710 0.659394\n",
      "711 0.659393\n",
      "712 0.659393\n",
      "713 0.659392\n",
      "714 0.659392\n",
      "715 0.659391\n",
      "716 0.65939\n",
      "717 0.65939\n",
      "718 0.659389\n",
      "719 0.659388\n",
      "720 0.659388\n",
      "721 0.659387\n",
      "722 0.659387\n",
      "723 0.659387\n",
      "724 0.659386\n",
      "725 0.659386\n",
      "726 0.659384\n",
      "727 0.659383\n",
      "728 0.659383\n",
      "729 0.659382\n",
      "730 0.659381\n",
      "731 0.659381\n",
      "732 0.65938\n",
      "733 0.65938\n",
      "734 0.659379\n",
      "735 0.659379\n",
      "736 0.659378\n",
      "737 0.659377\n",
      "738 0.659377\n",
      "739 0.659376\n",
      "740 0.659375\n",
      "741 0.659375\n",
      "742 0.659374\n",
      "743 0.659373\n",
      "744 0.659373\n",
      "745 0.659372\n",
      "746 0.659371\n",
      "747 0.659371\n",
      "748 0.65937\n",
      "749 0.659369\n",
      "750 0.659368\n",
      "751 0.659368\n",
      "752 0.659368\n",
      "753 0.659368\n",
      "754 0.659366\n",
      "755 0.659366\n",
      "756 0.659366\n",
      "757 0.659365\n",
      "758 0.659364\n",
      "759 0.659364\n",
      "760 0.659363\n",
      "761 0.659363\n",
      "762 0.659362\n",
      "763 0.659361\n",
      "764 0.659361\n",
      "765 0.65936\n",
      "766 0.65936\n",
      "767 0.659359\n",
      "768 0.659358\n",
      "769 0.659358\n",
      "770 0.659357\n",
      "771 0.659357\n",
      "772 0.659356\n",
      "773 0.659356\n",
      "774 0.659355\n",
      "775 0.659355\n",
      "776 0.659354\n",
      "777 0.659353\n",
      "778 0.659353\n",
      "779 0.659352\n",
      "780 0.659352\n",
      "781 0.659351\n",
      "782 0.65935\n",
      "783 0.65935\n",
      "784 0.659349\n",
      "785 0.659348\n",
      "786 0.659348\n",
      "787 0.659348\n",
      "788 0.659347\n",
      "789 0.659346\n",
      "790 0.659346\n",
      "791 0.659346\n",
      "792 0.659345\n",
      "793 0.659344\n",
      "794 0.659344\n",
      "795 0.659343\n",
      "796 0.659342\n",
      "797 0.659342\n",
      "798 0.659342\n",
      "799 0.659341\n",
      "800 0.65934\n",
      "801 0.65934\n",
      "802 0.659339\n",
      "803 0.659338\n",
      "804 0.659338\n",
      "805 0.659337\n",
      "806 0.659337\n",
      "807 0.659337\n",
      "808 0.659336\n",
      "809 0.659335\n",
      "810 0.659335\n",
      "811 0.659335\n",
      "812 0.659334\n",
      "813 0.659334\n",
      "814 0.659334\n",
      "815 0.659332\n",
      "816 0.659332\n",
      "817 0.659331\n",
      "818 0.65933\n",
      "819 0.659329\n",
      "820 0.659329\n",
      "821 0.659329\n",
      "822 0.659329\n",
      "823 0.659328\n",
      "824 0.659328\n",
      "825 0.659327\n",
      "826 0.659326\n",
      "827 0.659326\n",
      "828 0.659325\n",
      "829 0.659325\n",
      "830 0.659324\n",
      "831 0.659324\n",
      "832 0.659323\n",
      "833 0.659323\n",
      "834 0.659322\n",
      "835 0.659322\n",
      "836 0.659321\n",
      "837 0.659321\n",
      "838 0.65932\n",
      "839 0.659319\n",
      "840 0.659318\n",
      "841 0.659318\n",
      "842 0.659317\n",
      "843 0.659317\n",
      "844 0.659317\n",
      "845 0.659316\n",
      "846 0.659316\n",
      "847 0.659315\n",
      "848 0.659315\n",
      "849 0.659314\n",
      "850 0.659313\n",
      "851 0.659313\n",
      "852 0.659312\n",
      "853 0.659312\n",
      "854 0.659312\n",
      "855 0.659312\n",
      "856 0.659311\n",
      "857 0.659311\n",
      "858 0.65931\n",
      "859 0.659309\n",
      "860 0.659308\n",
      "861 0.659308\n",
      "862 0.659308\n",
      "863 0.659307\n",
      "864 0.659307\n",
      "865 0.659306\n",
      "866 0.659306\n",
      "867 0.659305\n",
      "868 0.659305\n",
      "869 0.659304\n",
      "870 0.659304\n",
      "871 0.659303\n",
      "872 0.659302\n",
      "873 0.659302\n",
      "874 0.659302\n",
      "875 0.659301\n",
      "876 0.659301\n",
      "877 0.6593\n",
      "878 0.6593\n",
      "879 0.659299\n",
      "880 0.659298\n",
      "881 0.659298\n",
      "882 0.659297\n",
      "883 0.659297\n",
      "884 0.659296\n",
      "885 0.659295\n",
      "886 0.659295\n",
      "887 0.659295\n",
      "888 0.659295\n",
      "889 0.659294\n",
      "890 0.659294\n",
      "891 0.659293\n",
      "892 0.659293\n",
      "893 0.659293\n",
      "894 0.659292\n",
      "895 0.659292\n",
      "896 0.659292\n",
      "897 0.659292\n",
      "898 0.659291\n",
      "899 0.65929\n",
      "900 0.659289\n",
      "901 0.659289\n",
      "902 0.659288\n",
      "903 0.659288\n",
      "904 0.659288\n",
      "905 0.659287\n",
      "906 0.659287\n",
      "907 0.659286\n",
      "908 0.659286\n",
      "909 0.659285\n",
      "910 0.659285\n",
      "911 0.659284\n",
      "912 0.659284\n",
      "913 0.659283\n",
      "914 0.659283\n",
      "915 0.659282\n",
      "916 0.659282\n",
      "917 0.659281\n",
      "918 0.659281\n",
      "919 0.65928\n",
      "920 0.65928\n",
      "921 0.65928\n",
      "922 0.659279\n",
      "923 0.659279\n",
      "924 0.659278\n",
      "925 0.659278\n",
      "926 0.659277\n",
      "927 0.659276\n",
      "928 0.659276\n",
      "929 0.659275\n",
      "930 0.659275\n",
      "931 0.659275\n",
      "932 0.659275\n",
      "933 0.659275\n",
      "934 0.659274\n",
      "935 0.659273\n",
      "936 0.659273\n",
      "937 0.659272\n",
      "938 0.659272\n",
      "939 0.659271\n",
      "940 0.65927\n",
      "941 0.65927\n",
      "942 0.659269\n",
      "943 0.659269\n",
      "944 0.65927\n",
      "945 0.659269\n",
      "946 0.659268\n",
      "947 0.659268\n",
      "948 0.659267\n",
      "949 0.659267\n",
      "950 0.659266\n",
      "951 0.659266\n",
      "952 0.659265\n",
      "953 0.659265\n",
      "954 0.659265\n",
      "955 0.659264\n",
      "956 0.659264\n",
      "957 0.659263\n",
      "958 0.659263\n",
      "959 0.659262\n",
      "960 0.659262\n",
      "961 0.659262\n",
      "962 0.659262\n",
      "963 0.65926\n",
      "964 0.65926\n",
      "965 0.65926\n",
      "966 0.659259\n",
      "967 0.659259\n",
      "968 0.659258\n",
      "969 0.659258\n",
      "970 0.659258\n",
      "971 0.659257\n",
      "972 0.659257\n",
      "973 0.659256\n",
      "974 0.659256\n",
      "975 0.659256\n",
      "976 0.659255\n",
      "977 0.659255\n",
      "978 0.659254\n",
      "979 0.659254\n",
      "980 0.659253\n",
      "981 0.659253\n",
      "982 0.659252\n",
      "983 0.659252\n",
      "984 0.659251\n",
      "985 0.659251\n",
      "986 0.65925\n",
      "987 0.65925\n",
      "988 0.65925\n",
      "989 0.659249\n",
      "990 0.659249\n",
      "991 0.659248\n",
      "992 0.659248\n",
      "993 0.659247\n",
      "994 0.659247\n",
      "995 0.659246\n",
      "996 0.659246\n",
      "997 0.659246\n",
      "998 0.659245\n",
      "999 0.659245\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for iter in range(1000):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A])\n",
    "    \n",
    "    print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.60219329959\n",
      "Accuracy on the test set is  0.621836587129\n"
     ]
    }
   ],
   "source": [
    "# Calling .eval() is basically like calling run(), but\n",
    "# just to get that one numpy array. \n",
    "# Note that it recomputes all its computation graph dependencies.\n",
    "A = tf_A.eval()\n",
    "A_test = tf_A_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "# Let's use placeholders for the training data. \n",
    "# This is so that we can suply batches of tranining examples each iteration.\n",
    "tf_X = tf.placeholder(tf.float32)\n",
    "tf_Y = tf.placeholder(tf.float32)\n",
    "\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w = tf.Variable( tf.zeros((n_x, 1)) )\n",
    "tf_b = tf.Variable(tf.zeros((1,1)))\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_Z = tf.matmul(tf_X, tf_w) + tf_b\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z) )\n",
    "\n",
    "# Optimizer.\n",
    "# We are going to find the minimum of this loss using gradient descent.\n",
    "# We pass alpha=0.1 as input parameter.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(tf_J)\n",
    "\n",
    "# Predictions for the train and test data.\n",
    "# These are not part of training, but merely here so that we can report\n",
    "# accuracy figures as we train.\n",
    "tf_A = tf.nn.sigmoid(tf_Z)\n",
    "tf_A_test = tf.nn.sigmoid(tf.matmul(tf_X_test, tf_w) + tf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step  (0, 0.69314742)\n",
      "Minibatch accuracy:  0.51\n",
      "Test accuracy:  0.4762593396\n",
      "Minibatch loss at step  (500, 0.66541976)\n",
      "Minibatch accuracy:  0.62\n",
      "Test accuracy:  0.625210894191\n",
      "Minibatch loss at step  (1000, 0.64409852)\n",
      "Minibatch accuracy:  0.61\n",
      "Test accuracy:  0.620631477464\n",
      "Minibatch loss at step  (1500, 0.67027396)\n",
      "Minibatch accuracy:  0.59\n",
      "Test accuracy:  0.621836587129\n",
      "Minibatch loss at step  (2000, 0.63774502)\n",
      "Minibatch accuracy:  0.68\n",
      "Test accuracy:  0.613159797542\n",
      "Minibatch loss at step  (2500, 0.64743912)\n",
      "Minibatch accuracy:  0.62\n",
      "Test accuracy:  0.623764762593\n",
      "Minibatch loss at step  (3000, 0.64893675)\n",
      "Minibatch accuracy:  0.6\n",
      "Test accuracy:  0.622559652928\n",
      "Minibatch loss at step  (3500, 0.6965217)\n",
      "Minibatch accuracy:  0.6\n",
      "Test accuracy:  0.622318630995\n",
      "Minibatch loss at step  (4000, 0.70876968)\n",
      "Minibatch accuracy:  0.49\n",
      "Test accuracy:  0.613400819475\n",
      "Minibatch loss at step  (4500, 0.64193666)\n",
      "Minibatch accuracy:  0.65\n",
      "Test accuracy:  0.616293082671\n",
      "Minibatch loss at step  (5000, 0.61940145)\n",
      "Minibatch accuracy:  0.6\n",
      "Test accuracy:  0.623282718727\n",
      "Minibatch loss at step  (5500, 0.66696215)\n",
      "Minibatch accuracy:  0.59\n",
      "Test accuracy:  0.619426367799\n",
      "Minibatch loss at step  (6000, 0.63962263)\n",
      "Minibatch accuracy:  0.6\n",
      "Test accuracy:  0.613641841408\n",
      "Minibatch loss at step  (6500, 0.66872138)\n",
      "Minibatch accuracy:  0.6\n",
      "Test accuracy:  0.616534104604\n",
      "Minibatch loss at step  (7000, 0.62475568)\n",
      "Minibatch accuracy:  0.61\n",
      "Test accuracy:  0.616293082671\n",
      "Minibatch loss at step  (7500, 0.64868021)\n",
      "Minibatch accuracy:  0.58\n",
      "Test accuracy:  0.621595565196\n",
      "Minibatch loss at step  (8000, 0.64023292)\n",
      "Minibatch accuracy:  0.59\n",
      "Test accuracy:  0.614364907207\n",
      "Minibatch loss at step  (8500, 0.6498192)\n",
      "Minibatch accuracy:  0.63\n",
      "Test accuracy:  0.612918775609\n",
      "Minibatch loss at step  (9000, 0.69496232)\n",
      "Minibatch accuracy:  0.56\n",
      "Test accuracy:  0.616534104604\n",
      "Minibatch loss at step  (9500, 0.69718254)\n",
      "Minibatch accuracy:  0.58\n",
      "Test accuracy:  0.620872499397\n",
      "Minibatch loss at step  (10000, 0.68451387)\n",
      "Minibatch accuracy:  0.61\n",
      "Test accuracy:  0.619908411665\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "batch_size = 100\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Pick an offset within the training data.\n",
    "    offset = (step * batch_size) % (X.shape[0] - batch_size)\n",
    "    \n",
    "    # Generate a minibatch.\n",
    "    X_batch = X[offset:(offset + batch_size), :]\n",
    "    Y_batch = Y[offset:(offset + batch_size), :]\n",
    "    \n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A], feed_dict={tf_X : X_batch, tf_Y : Y_batch})\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "        print(\"Minibatch loss at step \", (step, J))\n",
    "        print(\"Minibatch accuracy: \", accuracy(A, Y_batch))\n",
    "        A_test = tf_A_test.eval()\n",
    "        print(\"Test accuracy: \", accuracy(A_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network in TensorFlow\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "\n",
    "num_hidden_nodes = 15\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z2) )\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 46.6312\n",
      "50 0.690632\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(100):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  53.5361904045\n",
      "Accuracy on the test set is  54.1729436577\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A2.eval()\n",
    "A_test = tf_A2_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with more layers and more hidden nodes\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "\n",
    "num_hidden_nodes = 100\n",
    "\n",
    "C = 1\n",
    "\n",
    "# Load the training and test data into constants\n",
    "tf_X = tf.constant(X.astype(np.float32))\n",
    "tf_Y = tf.constant(Y.astype(np.float32))\n",
    "tf_X_test = tf.constant(X_test.astype(np.float32))\n",
    "tf_Y_test = tf.constant(Y_test.astype(np.float32))\n",
    "\n",
    "# Variables.\n",
    "# These are the parameters that we are going to be training.\n",
    "tf_w1 = tf.Variable(tf.truncated_normal((n_x, num_hidden_nodes)))\n",
    "tf_b1 = tf.Variable(tf.zeros((1, num_hidden_nodes)))\n",
    "tf_w2 = tf.Variable(tf.truncated_normal([num_hidden_nodes, num_hidden_nodes]))\n",
    "tf_b2 = tf.Variable(tf.zeros((1, C)))\n",
    "tf_w3 = tf.Variable(tf.truncated_normal([num_hidden_nodes, C]))\n",
    "tf_b3 = tf.Variable(tf.zeros((1, C)))\n",
    "\n",
    "\n",
    "\n",
    "tf_Z1 = tf.matmul(tf_X, tf_w1) + tf_b1\n",
    "tf_A1 = tf.nn.relu(tf_Z1)    #tf.nn.relu(tf_Z1)\n",
    "tf_Z2 = tf.matmul(tf_A1, tf_w2) + tf_b2\n",
    "tf_A2 = tf.nn.relu(tf_Z2)\n",
    "tf_Z3 = tf.matmul(tf_A2, tf_w3) + tf_b3\n",
    "tf_A3 = tf.nn.relu(tf_Z3)\n",
    "\n",
    "# Training computation.\n",
    "# We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "# the sigmoid and cross-entropy (it's one operation in TensorFlow, because\n",
    "# it's very common, and it can be optimized). We take the average of this\n",
    "# cross-entropy across all training examples: that's our cost.\n",
    "tf_J = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_Y, logits=tf_Z3) )\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(tf_J)\n",
    "\n",
    "# Predictions for the test data.\n",
    "tf_Z1_test = tf.matmul(tf_X_test, tf_w1) + tf_b1\n",
    "tf_A1_test = tf.nn.relu(tf_Z1_test)\n",
    "tf_Z2_test = tf.matmul(tf_A1_test, tf_w2) + tf_b2\n",
    "tf_A2_test = tf.nn.relu(tf_Z2_test)\n",
    "tf_Z3_test = tf.matmul(tf_A2_test, tf_w3) + tf_b3\n",
    "tf_A3_test = tf.nn.relu(tf_Z3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0 147.8\n",
      "50 0.690634\n",
      "100 0.690629\n",
      "150 0.690629\n",
      "200 0.690629\n",
      "250 0.690629\n",
      "300 0.690629\n",
      "350 0.690629\n",
      "400 0.690629\n",
      "450 0.690629\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "\n",
    "# This is a one-time operation which ensures the parameters get initialized as\n",
    "# we described in the graph: random weights for the matrix, zeros for the biases. \n",
    "tf.global_variables_initializer().run()\n",
    "print(\"Initialized\")\n",
    "\n",
    "\n",
    "# Replace None with your code.\n",
    "\n",
    "for iter in range(500):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the cost value and the training predictions returned as numpy arrays.\n",
    "    # Print out the iteration number and cost every 50 iterations.\n",
    "    _, J, A = session.run([optimizer, tf_J, tf_A2])\n",
    "    \n",
    "    if iter%50 ==0:\n",
    "        print(iter, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the train set is  0.535361904045\n",
      "Accuracy on the test set is  0.541729436577\n"
     ]
    }
   ],
   "source": [
    "# Print out the accuracy for the training set and test set.\n",
    "A = tf_A3.eval()\n",
    "A_test = tf_A3_test.eval()\n",
    "\n",
    "print(\"Accuracy on the train set is \", accuracy(A,Y))\n",
    "print(\"Accuracy on the test set is \", accuracy(A_test,Y_test))\n",
    "# Put your code here.\n",
    "\n",
    "# Call .eval() on tf_A2 and tf_A2_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying stuff in sklearn\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2001_2002 = pd.read_csv('game_data/game_data_2001_2002.csv', header=0)\n",
    "data_2002_2003 = pd.read_csv('game_data/game_data_2002_2003.csv', header=0)\n",
    "data_2003_2004 = pd.read_csv('game_data/game_data_2003_2004.csv', header=0)\n",
    "# data_2004_2005 = pd.read_csv('game_data/game_data_2004_2005.csv', header=0)\n",
    "data_2005_2006 = pd.read_csv('game_data/game_data_2005_2006.csv', header=0)\n",
    "data_2006_2007 = pd.read_csv('game_data/game_data_2006_2007.csv', header=0)\n",
    "data_2007_2008 = pd.read_csv('game_data/game_data_2007_2008.csv', header=0)\n",
    "data_2008_2009 = pd.read_csv('game_data/game_data_2008_2009.csv', header=0)\n",
    "data_2009_2010 = pd.read_csv('game_data/game_data_2009_2010.csv', header=0)\n",
    "data_2010_2011 = pd.read_csv('game_data/game_data_2010_2011.csv', header=0)\n",
    "data_2011_2012 = pd.read_csv('game_data/game_data_2011_2012.csv', header=0)\n",
    "data_2012_2013 = pd.read_csv('game_data/game_data_2012_2013.csv', header=0)\n",
    "data_2013_2014 = pd.read_csv('game_data/game_data_2013_2014.csv', header=0)\n",
    "data_2014_2015 = pd.read_csv('game_data/game_data_2014_2015.csv', header=0)\n",
    "data_2015_2016 = pd.read_csv('game_data/game_data_2015_2016.csv', header=0)\n",
    "data_2016_2017 = pd.read_csv('game_data/game_data_2016_2017.csv', header=0)\n",
    "data_2017_2018 = pd.read_csv('game_data/game_data_2017_2018.csv', header=0)\n",
    "\n",
    "frames = [data_2001_2002, data_2002_2003, data_2003_2004, data_2005_2006, data_2006_2007, data_2007_2008,\n",
    "        data_2008_2009, data_2009_2010, data_2010_2011, data_2011_2012, data_2012_2013, data_2013_2014,\n",
    "        data_2014_2015, data_2015_2016, data_2016_2017, data_2017_2018]\n",
    "data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = prepare(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'gamma':[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100],\n",
    "'C':[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.concatenate( y, axis=0 )\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
